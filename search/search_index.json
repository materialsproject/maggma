{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#what-is-maggma","title":"What is Maggma","text":"<p>Maggma is a framework to build scientific data processing pipelines from data stored in a variety of formats -- databases, Azure Blobs, files on disk, etc., all the way to a REST API. The rest of this README contains a brief, high-level overview of what <code>maggma</code> can do. For more, please refer to the documentation.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#from-pypi","title":"From PyPI","text":"<p>Maggma is published on the Python Package Index.  The preferred tool for installing packages from PyPi is pip.  This tool is provided with all modern versions of Python.</p> <p>Open your terminal and run the following command:</p> <pre><code>pip install --upgrade maggma\n</code></pre>"},{"location":"#direct-from-git","title":"Direct from <code>git</code>","text":"<p>If you want to install the latest development version, but do not plan to make any changes to it, you can install as follows:</p> <pre><code>pip install git+https://github.com/materialsproject/maggma\n</code></pre>"},{"location":"#local-clone","title":"Local Clone","text":"<p>You can install Maggma directly from a clone of the Git repository.  This can be done either by cloning the repo and installing from the local clone, or simply installing directly via git.</p> <pre><code>git clone https://github.com//materialsproject/maggma\ncd maggma\npython setup.py install\n</code></pre>"},{"location":"#basic-concepts","title":"Basic Concepts","text":"<p><code>maggma</code>'s core classes -- <code>Store</code> and <code>Builder</code> -- provide building blocks for modular data pipelines. Data resides in one or more <code>Store</code> and is processed by a <code>Builder</code>. The results of the processing are saved in another <code>Store</code>, and so on:</p> <pre><code>flowchart\u00a0LR\n\u00a0\u00a0\u00a0\u00a0s1(Store 1)\u00a0--Builder 1--&gt;\u00a0s2(Store 2) --Builder 2--&gt; s3(Store 3)\ns2 -- Builder 3--&gt;s4(Store 4)</code></pre>"},{"location":"#store","title":"Store","text":"<p>A major challenge in building scalable data pipelines is dealing with all the different types of data sources out there. Maggma's <code>Store</code> class provides a consistent, unified interface for querying data from arbitrary data sources. It was originally built around MongoDB, so it's interface closely resembles <code>PyMongo</code> syntax. However, Maggma makes it possible to use that same syntax to query other types of databases, such as Amazon S3, GridFS, or files on disk, and many others. Stores implement methods to <code>connect</code>, <code>query</code>, find <code>distinct</code> values, <code>groupby</code> fields, <code>update</code> documents, and <code>remove</code> documents.</p> <p>The example below demonstrates inserting 4 documents (python <code>dicts</code>) into a <code>MongoStore</code> with <code>update</code>, then accessing the data using <code>count</code>, <code>query</code>, and <code>distinct</code>.</p> <pre><code>&gt;&gt;&gt; turtles = [{\"name\": \"Leonardo\", \"color\": \"blue\", \"tool\": \"sword\"},\n               {\"name\": \"Donatello\",\"color\": \"purple\", \"tool\": \"staff\"},\n               {\"name\": \"Michelangelo\", \"color\": \"orange\", \"tool\": \"nunchuks\"},\n               {\"name\":\"Raphael\", \"color\": \"red\", \"tool\": \"sai\"}\n            ]\n&gt;&gt;&gt; store = MongoStore(database=\"my_db_name\",\n                       collection_name=\"my_collection_name\",\n                       username=\"my_username\",\n                       password=\"my_password\",\n                       host=\"my_hostname\",\n                       port=27017,\n                       key=\"name\",\n                    )\n&gt;&gt;&gt; with store:\n        store.update(turtles)\n&gt;&gt;&gt; store.count()\n4\n&gt;&gt;&gt; store.query_one({})\n{'_id': ObjectId('66746d29a78e8431daa3463a'), 'name': 'Leonardo', 'color': 'blue', 'tool': 'sword'}\n&gt;&gt;&gt; store.distinct('color')\n['purple', 'orange', 'blue', 'red']\n</code></pre>"},{"location":"#builder","title":"Builder","text":"<p>Builders represent a data processing step, analogous to an extract-transform-load (ETL) operation in a data warehouse model. Much like <code>Store</code> provides a consistent interface for accessing data, the <code>Builder</code> classes provide a consistent interface for transforming it. <code>Builder</code> transformation are each broken into 3 phases: <code>get_items</code>, <code>process_item</code>, and <code>update_targets</code>:</p> <ol> <li><code>get_items</code>: Retrieve items from the source Store(s) for processing by the next phase</li> <li><code>process_item</code>: Manipulate the input item and create an output document that is sent to the next phase for storage.</li> <li><code>update_target</code>: Add the processed item to the target Store(s).</li> </ol> <p>Both <code>get_items</code> and <code>update_targets</code> can perform IO (input/output) to the data stores. <code>process_item</code> is expected to not perform any IO so that it can be parallelized by Maggma. Builders can be chained together into an array and then saved as a JSON file to be run on a production system.</p>"},{"location":"#origin-and-maintainers","title":"Origin and Maintainers","text":"<p>Maggma has been developed and is maintained by the Materials Project team at Lawrence Berkeley National Laboratory and the Materials Project Software Foundation.</p> <p>Maggma is written in Python and supports Python 3.9+.</p>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#unreleased","title":"Unreleased","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>[Feature Request]: Document API components #972</li> </ul>"},{"location":"CHANGELOG/#v0720-2025-08-13","title":"v0.72.0 (2025-08-13)","text":"<p>Full Changelog</p> <p>Breaking changes:</p> <ul> <li>Deprecate maggma.api for migration #1048 (esoteric-ephemera)</li> </ul> <p>Closed issues:</p> <ul> <li>Confirm that pymongo, boto3, boto3core updates don't break things #1028</li> </ul> <p>Merged pull requests:</p> <ul> <li>update Code of Conduct to MPSF version #1046 (rkingsbury)</li> <li>Atlas search update #1036 (yang-ruoxi)</li> </ul>"},{"location":"CHANGELOG/#v0715-2025-03-09","title":"v0.71.5 (2025-03-09)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Fix GridFSURIStore and increase testing #1037 (gpetretto)</li> <li>fixed other references to incorrect method name #1033 (rekumar)</li> </ul>"},{"location":"CHANGELOG/#v0714-2025-02-06","title":"v0.71.4 (2025-02-06)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0713-2025-02-06","title":"v0.71.3 (2025-02-06)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>zopen: add explicit encoding to read_json; explicit mode #1030 (rkingsbury)</li> </ul> <p>Fixed bugs:</p> <ul> <li>[Bug]: monty's zopen expects keyword <code>mode</code> to be set #1024</li> </ul> <p>Merged pull requests:</p> <ul> <li>Remove python 3.8 classifier from pyproject.toml #1029 (Andrew-S-Rosen)</li> </ul>"},{"location":"CHANGELOG/#v0712-2025-01-21","title":"v0.71.2 (2025-01-21)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add explicit text mode to zopen calls (monty warning) #1025 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0711-2024-12-29","title":"v0.71.1 (2024-12-29)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Set explicit mode in zopen to address monty FutureWarning #1021 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0710-2024-12-28","title":"v0.71.0 (2024-12-28)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>[Feature] Allow Different Azure Authentication Methods #1020 (jmmshn)</li> </ul> <p>Fixed bugs:</p> <ul> <li>[Bug]: Handling of deprecated <code>np.bool_</code> is not ideal #1006</li> </ul> <p>Closed issues:</p> <ul> <li>[Bug] MongoURIStore not able to access some parent attributes. #684</li> </ul> <p>Merged pull requests:</p> <ul> <li>Minor fix to URI store #1014 (jmmshn)</li> <li>Automated dependency upgrades #1008 (github-actions[bot])</li> <li>Automated dependency upgrades #1003 (github-actions[bot])</li> </ul>"},{"location":"CHANGELOG/#v0700-2024-10-08","title":"v0.70.0 (2024-10-08)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>GroupBuilder: fix query kwarg and add tests #1002 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0694-2024-09-29","title":"v0.69.4 (2024-09-29)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>[Feature Request]: Support numpy 2.0 #990</li> </ul> <p>Merged pull requests:</p> <ul> <li>clarify state of open data stores #997 (kbuma)</li> <li>add python 3.12 to CI tests #992 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0693-2024-08-23","title":"v0.69.3 (2024-08-23)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0692-2024-08-16","title":"v0.69.2 (2024-08-16)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>capability to configure query on request #985 (yang-ruoxi)</li> <li>Automated dependency upgrades #983 (github-actions[bot])</li> </ul>"},{"location":"CHANGELOG/#v0691-2024-07-24","title":"v0.69.1 (2024-07-24)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0690-2024-07-01","title":"v0.69.0 (2024-07-01)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>[Feature Request]: Leverage optional dependency groups to reduce dependency count #928</li> <li>Update README/docs to better reflect the purpose of Maggma #886</li> </ul> <p>Merged pull requests:</p> <ul> <li>Store Documentation update #976 (rkingsbury)</li> <li>Add content to README; documentation fixups #969 (rkingsbury)</li> <li>Automated dependency upgrades #965 (github-actions[bot])</li> </ul>"},{"location":"CHANGELOG/#v0686-2024-06-20","title":"v0.68.6 (2024-06-20)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>move API to optional dependency group; move OpenData to default installation #970 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0685-2024-06-20","title":"v0.68.5 (2024-06-20)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>mv mongogrant to optional dependency group #968 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0684-2024-06-11","title":"v0.68.4 (2024-06-11)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0683-2024-06-11","title":"v0.68.3 (2024-06-11)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Bugfix in sorting query operator #964 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0682-2024-06-05","title":"v0.68.2 (2024-06-05)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0681-2024-05-30","title":"v0.68.1 (2024-05-30)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Handle store error during finalize #958 (jmmshn)</li> </ul>"},{"location":"CHANGELOG/#v0680-2024-05-27","title":"v0.68.0 (2024-05-27)","text":"<p>Full Changelog</p> <p>Breaking changes:</p> <ul> <li>drop python 3.8 support #951 (rkingsbury)</li> </ul> <p>Implemented enhancements:</p> <ul> <li>[Feature Request]: pass keyword arguments to zopen to accommodate non english platforms #932</li> </ul> <p>Merged pull requests:</p> <ul> <li>Add support for python 3.12 to CI #954 (rkingsbury)</li> <li>merge setup.py into pyproject.toml #952 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0670-2024-05-13","title":"v0.67.0 (2024-05-13)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Add character encoding kwarg to JSONStore and FileStore #949 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0660-2024-04-30","title":"v0.66.0 (2024-04-30)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add config option to sort query op #944 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0650-2024-04-18","title":"v0.65.0 (2024-04-18)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>[Feature Request]: support ruamel.yaml 0.18+ #938</li> </ul> <p>Merged pull requests:</p> <ul> <li>Adding store support for tasks stored in open data #943 (kbuma)</li> <li>allow HEAD method for <code>/heartbeat</code> #874 (tschaume)</li> </ul>"},{"location":"CHANGELOG/#v0641-2024-04-16","title":"v0.64.1 (2024-04-16)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0640-2024-03-17","title":"v0.64.0 (2024-03-17)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Enable <code>recursive_msonable</code> in <code>jsanitize</code> calls #930 (Andrew-S-Rosen)</li> </ul>"},{"location":"CHANGELOG/#v0634-2024-02-29","title":"v0.63.4 (2024-02-29)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>write all NaN and NaT Dataframe created values as null #929 (kbuma)</li> </ul>"},{"location":"CHANGELOG/#v0633-2024-02-21","title":"v0.63.3 (2024-02-21)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Tweak docstrings to fix rendered docs #923 (ml-evs)</li> </ul>"},{"location":"CHANGELOG/#v0632-2024-02-16","title":"v0.63.2 (2024-02-16)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>enables using more efficient queries for count, distinct and newer_in #921 (kbuma)</li> </ul>"},{"location":"CHANGELOG/#v0631-2024-02-14","title":"v0.63.1 (2024-02-14)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>fix open data store connect and close and address future warnings for pandas #920 (kbuma)</li> </ul>"},{"location":"CHANGELOG/#v0630-2024-02-13","title":"v0.63.0 (2024-02-13)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>open data refactor for integration with builders #919 (kbuma)</li> </ul>"},{"location":"CHANGELOG/#v0621-2024-02-05","title":"v0.62.1 (2024-02-05)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>chunking for json normalization #914 (kbuma)</li> </ul>"},{"location":"CHANGELOG/#v0620-2024-02-02","title":"v0.62.0 (2024-02-02)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>updating for open data format change #911 (kbuma)</li> </ul>"},{"location":"CHANGELOG/#v0611-2024-01-30","title":"v0.61.1 (2024-01-30)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Make get by key default false #910 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0610-2024-01-19","title":"v0.61.0 (2024-01-19)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li><code>DeprecationWarning</code> associated with <code>pkg_resources</code> #903</li> </ul> <p>Merged pull requests:</p> <ul> <li>creating PandasMemoryStore for use by OpenDataStore #908 (kbuma)</li> </ul>"},{"location":"CHANGELOG/#v0602-2024-01-05","title":"v0.60.2 (2024-01-05)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>rm deprecated pkg_resources #905 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0601-2024-01-05","title":"v0.60.1 (2024-01-05)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Patch s3 kwarg #900 (jmmshn)</li> </ul> <p>Merged pull requests:</p> <ul> <li>special casing for thermo, xas and synth_descriptions collections in OpenData #904 (kbuma)</li> <li>linting fixes #901 (jmmshn)</li> </ul>"},{"location":"CHANGELOG/#v0600-2023-12-15","title":"v0.60.0 (2023-12-15)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>fixing OpenDataStore to pickle correctly #897 (kbuma)</li> </ul>"},{"location":"CHANGELOG/#v0590-2023-12-11","title":"v0.59.0 (2023-12-11)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Enhancement/open data store #893 (kbuma)</li> </ul>"},{"location":"CHANGELOG/#v0580-2023-11-21","title":"v0.58.0 (2023-11-21)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>SSH tunnel support for S3Store #882 (mjwen)</li> </ul> <p>Merged pull requests:</p> <ul> <li>update package metadata in pyproject.toml #892 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v05710-2023-11-17","title":"v0.57.10 (2023-11-17)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Remove key from sorting by default #890 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0579-2023-11-16","title":"v0.57.9 (2023-11-16)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Remove hint in count for S3Store #888 (munrojm)</li> <li>Add missing <code>MontyStore</code> to list of stores #887 (Andrew-S-Rosen)</li> </ul>"},{"location":"CHANGELOG/#v0578-2023-11-09","title":"v0.57.8 (2023-11-09)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Fix aggregation pipeline kwargs #884 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0577-2023-11-07","title":"v0.57.7 (2023-11-07)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Update hint_scheme #883 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0576-2023-11-07","title":"v0.57.6 (2023-11-07)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Ensure sort stage is after match in agg pipeline #881 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0575-2023-11-04","title":"v0.57.5 (2023-11-04)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Store.connect: fix force_reset kwarg implementations #879 (rkingsbury)</li> </ul> <p>Merged pull requests:</p> <ul> <li>chore: fix typos #877 (e-kwsm)</li> <li>Automated dependency upgrades #875 (github-actions[bot])</li> </ul>"},{"location":"CHANGELOG/#v0574-2023-10-13","title":"v0.57.4 (2023-10-13)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Fix header processing with enabled validation #871 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0573-2023-10-12","title":"v0.57.3 (2023-10-12)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Ensure header processor alters the correct object #870 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0572-2023-10-09","title":"v0.57.2 (2023-10-09)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>[Feature Request]: Is there a specific reason why pyzmq is fixed to 24.0.1 rather than supporting more recent versions ?  #867</li> </ul> <p>Merged pull requests:</p> <ul> <li>Remove generic model reference #869 (munrojm)</li> <li>Automated dependency upgrades #868 (github-actions[bot])</li> </ul>"},{"location":"CHANGELOG/#v0571-2023-10-05","title":"v0.57.1 (2023-10-05)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Support for Pydantic 2 #858</li> </ul>"},{"location":"CHANGELOG/#v0570-2023-09-26","title":"v0.57.0 (2023-09-26)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Pydantic 2.0 #865 (munrojm)</li> <li>Revert \"Automated dependency upgrades\" #862 (rkingsbury)</li> <li>CI: add changelog template and prevent duplicate GH Action releases #861 (rkingsbury)</li> <li>Automated dependency upgrades #860 (github-actions[bot])</li> <li>Update @arosen93 to @Andrew-S-Rosen #859 (Andrew-S-Rosen)</li> </ul>"},{"location":"CHANGELOG/#v0560-2023-09-06","title":"v0.56.0 (2023-09-06)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0550-2023-09-06","title":"v0.55.0 (2023-09-06)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Would the maggma docs be a good place to host MongoDB setup instructions?  #845</li> </ul> <p>Merged pull requests:</p> <ul> <li>Automated dependency upgrades #856 (github-actions[bot])</li> <li>migrate dependencies to setup.py and update CI config #855 (rkingsbury)</li> <li>Automated dependency upgrades #854 (github-actions[bot])</li> <li>Fix broken link in README.md #853 (Andrew-S-Rosen)</li> <li>Create dependabot.yml to update GitHub actions packages #852 (Andrew-S-Rosen)</li> <li>Remove tests for Python 3.7 since it reached its end-of-life #851 (Andrew-S-Rosen)</li> <li>Update CI pipeline so repeated commits don't cause concurrent tests #850 (Andrew-S-Rosen)</li> <li>Add a \"Setting up MongoDB\" guide to the docs and update README #849 (Andrew-S-Rosen)</li> <li>CI: use OS-specific requirements in testing #841 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0540-2023-08-29","title":"v0.54.0 (2023-08-29)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Automated dependency upgrades #848 (github-actions[bot])</li> <li>JSONStore: enabled reading of MongoDB extended JSON files #847 (rkingsbury)</li> <li>Automated dependency upgrades #844 (github-actions[bot])</li> </ul>"},{"location":"CHANGELOG/#v0531-2023-08-15","title":"v0.53.1 (2023-08-15)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Aws store botocore fix #843 (tsmathis)</li> <li>Automated dependency upgrades #842 (github-actions[bot])</li> <li>CI: small update to auto dependency workflow #840 (rkingsbury)</li> <li>Automated dependency upgrades #839 (github-actions[bot])</li> </ul>"},{"location":"CHANGELOG/#v0530-2023-08-02","title":"v0.53.0 (2023-08-02)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>allow &gt;5GB and turn on multi-part uploads for AWS #829 (kbuma)</li> </ul>"},{"location":"CHANGELOG/#v0521-2023-08-02","title":"v0.52.1 (2023-08-02)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0522-2023-08-02","title":"v0.52.2 (2023-08-02)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Allow maggma to be used without Azure #837 (jmmshn)</li> <li>rm merge-me action #836 (rkingsbury)</li> <li>Automated dependency upgrades #835 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0520-2023-07-31","title":"v0.52.0 (2023-07-31)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v05125-2023-07-27","title":"v0.51.25 (2023-07-27)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Some cleanup: <code>isort</code>, updated classifiers, remove unused kwarg #833 (Andrew-S-Rosen)</li> </ul>"},{"location":"CHANGELOG/#v05124-2023-07-21","title":"v0.51.24 (2023-07-21)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v05123-2023-07-21","title":"v0.51.23 (2023-07-21)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li><code>database_name</code> of <code>MontyStore</code> doesn't seem to update the name #820</li> </ul> <p>Merged pull requests:</p> <ul> <li>FileStore performance enhancements #824 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v05122-2023-07-21","title":"v0.51.22 (2023-07-21)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Instantiating a <code>Store</code> from a dictionary representation #825</li> </ul> <p>Merged pull requests:</p> <ul> <li>misc. MontyStore improvements #827 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v05120-2023-07-11","title":"v0.51.20 (2023-07-11)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v05121-2023-07-11","title":"v0.51.21 (2023-07-11)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Fixe ruamel.yaml dependency  #823 (jmmshn)</li> </ul>"},{"location":"CHANGELOG/#v05119-2023-07-11","title":"v0.51.19 (2023-07-11)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Update setup.py #822 (jmmshn)</li> </ul>"},{"location":"CHANGELOG/#v05118-2023-07-10","title":"v0.51.18 (2023-07-10)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add <code>MontyStore</code> to <code>maggma.stores.__init__</code> #819 (Andrew-S-Rosen)</li> </ul>"},{"location":"CHANGELOG/#v05117-2023-07-07","title":"v0.51.17 (2023-07-07)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Revert store close apart from S3 #818 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v05116-2023-07-07","title":"v0.51.16 (2023-07-07)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Fix s3 store collection close handling in resource classes #817 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v05115-2023-07-06","title":"v0.51.15 (2023-07-06)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Fix collection check #816 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v05114-2023-07-06","title":"v0.51.14 (2023-07-06)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Check for collection before closing in resources #815 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v05113-2023-07-06","title":"v0.51.13 (2023-07-06)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add explicit store close to resources #814 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v05111-2023-06-27","title":"v0.51.11 (2023-06-27)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v05112-2023-06-27","title":"v0.51.12 (2023-06-27)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v05110-2023-06-27","title":"v0.51.10 (2023-06-27)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>fix the response field #811 (yang-ruoxi)</li> </ul>"},{"location":"CHANGELOG/#v0519-2023-06-22","title":"v0.51.9 (2023-06-22)","text":"<p>Full Changelog</p> <p>Fixed bugs:</p> <ul> <li>python 3.11 CI test failure with AzureBlobStore #807</li> </ul> <p>Merged pull requests:</p> <ul> <li>add patch method for submission resource #809 (yang-ruoxi)</li> </ul>"},{"location":"CHANGELOG/#v0518-2023-06-14","title":"v0.51.8 (2023-06-14)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Memray memory profiler support for mrun command line tool #794 (tsmathis)</li> </ul> <p>Closed issues:</p> <ul> <li><code>MontyStore</code> cannot be used with a pre-existing local DB #796</li> </ul> <p>Merged pull requests:</p> <ul> <li>Fixing bug in azure multi worker test #808 (gpetretto)</li> <li>Clarify docstring for <code>MontyDB</code> and add missing <code>self.database</code> property #806 (Andrew-S-Rosen)</li> <li>Add CodeQL workflow for GitHub code scanning #743 (lgtm-com[bot])</li> </ul>"},{"location":"CHANGELOG/#v0517-2023-06-12","title":"v0.51.7 (2023-06-12)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Explicitly close s3 client connections in <code>S3Store</code> #805 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0516-2023-06-08","title":"v0.51.6 (2023-06-08)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Use tqdm.auto #795 (utf)</li> </ul>"},{"location":"CHANGELOG/#v0515-2023-06-06","title":"v0.51.5 (2023-06-06)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Disable worker timeouts by default #793 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0514-2023-06-02","title":"v0.51.4 (2023-06-02)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>modify JSONStore file creation #792 (gpetretto)</li> </ul>"},{"location":"CHANGELOG/#v0513-2023-05-29","title":"v0.51.3 (2023-05-29)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0512-2023-05-29","title":"v0.51.2 (2023-05-29)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add orjson options in JSONStore #791 (gpetretto)</li> <li>Implementation of an AzureBlobStore for Azure blobs #790 (gpetretto)</li> </ul>"},{"location":"CHANGELOG/#v0511-2023-05-22","title":"v0.51.1 (2023-05-22)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add ruamel-yaml as a dependency #789 (sivonxay)</li> </ul>"},{"location":"CHANGELOG/#v0510-2023-05-22","title":"v0.51.0 (2023-05-22)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Create a MultiStore object and a Store-like object to access it #787 (sivonxay)</li> </ul>"},{"location":"CHANGELOG/#v0504-2023-04-28","title":"v0.50.4 (2023-04-28)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Update MongoStore <code>count</code> method #785 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0503-2023-02-17","title":"v0.50.3 (2023-02-17)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Remove extra heartbeats from workers #779 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0502-2023-02-17","title":"v0.50.2 (2023-02-17)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Pydantic CLI settings #778 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0501-2023-02-16","title":"v0.50.1 (2023-02-16)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Remove stray print in worker debug #777 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0500-2023-02-16","title":"v0.50.0 (2023-02-16)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Overhaul distributed framework and add RabbitMQ support #776 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v04918-2023-02-13","title":"v0.49.18 (2023-02-13)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add more heartbeat pings from worker #775 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v04917-2023-01-30","title":"v0.49.17 (2023-01-30)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Remove default sorting from API #770 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v04916-2023-01-23","title":"v0.49.16 (2023-01-23)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Query pipeline out of memory fix #767 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v04915-2023-01-23","title":"v0.49.15 (2023-01-23)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Fix server-side API sorting #766 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v04914-2023-01-18","title":"v0.49.14 (2023-01-18)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Fix S3 store queries in API #761 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v04913-2023-01-10","title":"v0.49.13 (2023-01-10)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Aggregation pipelines in resource classes #759 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v04912-2023-01-09","title":"v0.49.12 (2023-01-09)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add default sort parameter to <code>MongoStore</code> #758 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v04911-2022-12-15","title":"v0.49.11 (2022-12-15)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Async to sync for fastapi funcs #750 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v04910-2022-12-07","title":"v0.49.10 (2022-12-07)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Enable disk use in mongo find #749 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0499-2022-11-01","title":"v0.49.9 (2022-11-01)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Parse datetime with dateutil #741 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0498-2022-10-25","title":"v0.49.8 (2022-10-25)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0497-2022-10-25","title":"v0.49.7 (2022-10-25)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>FileStore: fix metadata overwriting path #736 (rkingsbury)</li> <li>JSONStore: fix last_updated serialization problem #735 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0496-2022-10-21","title":"v0.49.6 (2022-10-21)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Default sort on _id for determinacy #732 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0495-2022-09-30","title":"v0.49.5 (2022-09-30)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Up manager timeout #718 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0494-2022-09-28","title":"v0.49.4 (2022-09-28)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Up worker timeout #717 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0493-2022-09-27","title":"v0.49.3 (2022-09-27)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Update high water mark #716 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0492-2022-09-27","title":"v0.49.2 (2022-09-27)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Fix stalling in distributed code #715 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0491-2022-09-26","title":"v0.49.1 (2022-09-26)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Send proper exit message to workers #714 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0490-2022-09-23","title":"v0.49.0 (2022-09-23)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Enhance distributed builder code #711 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0481-2022-09-02","title":"v0.48.1 (2022-09-02)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add ssh_tunnel option to GridFSStore #707 (utf)</li> </ul>"},{"location":"CHANGELOG/#v0480-2022-08-04","title":"v0.48.0 (2022-08-04)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Proposal: remove Drone class #669 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0476-2022-08-04","title":"v0.47.6 (2022-08-04)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Docs: add mermaid diagram support #677 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0475-2022-07-26","title":"v0.47.5 (2022-07-26)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add pymongo timeout context to queries #691 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0474-2022-07-25","title":"v0.47.4 (2022-07-25)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Ensure all fields are properly sanitized #690 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0473-2022-06-07","title":"v0.47.3 (2022-06-07)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Fix header processing #679 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0472-2022-05-27","title":"v0.47.2 (2022-05-27)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Docs updates: add FileStore and misc edits #668 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0471-2022-05-24","title":"v0.47.1 (2022-05-24)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Fix gridfs URI store #667 (utf)</li> </ul>"},{"location":"CHANGELOG/#v0470-2022-05-23","title":"v0.47.0 (2022-05-23)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>FileStore: a Store for files on disk #619 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0462-2022-05-23","title":"v0.46.2 (2022-05-23)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>allow s3 resource kwargs #665 (jmmshn)</li> </ul>"},{"location":"CHANGELOG/#v0461-2022-04-21","title":"v0.46.1 (2022-04-21)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Prefix <code>fields</code> input for read resource key endpoint #636 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0460-2022-04-19","title":"v0.46.0 (2022-04-19)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>S3 store and resource additions #635 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0451-2022-04-18","title":"v0.45.1 (2022-04-18)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>minor bug fix in remove_docs #626 (jmmshn)</li> </ul>"},{"location":"CHANGELOG/#v0450-2022-04-14","title":"v0.45.0 (2022-04-14)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Changes to core query operators and API #620 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0445-2022-04-12","title":"v0.44.5 (2022-04-12)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>JSONStore: file_writable -&gt; read_only #625 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0444-2022-04-12","title":"v0.44.4 (2022-04-12)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>JSONStore: write file on init, add descriptive KeyError, add tests #624 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0443-2022-04-11","title":"v0.44.3 (2022-04-11)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>MemoryStore: fix groupby ignoring properties #621 (rkingsbury)</li> </ul>"},{"location":"CHANGELOG/#v0442-2022-04-05","title":"v0.44.2 (2022-04-05)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Force post-process method to take in query params #618 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0441-2022-03-08","title":"v0.44.1 (2022-03-08)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>added localhost test for MongoURIStore #595 (jmmshn)</li> </ul>"},{"location":"CHANGELOG/#v0440-2022-03-07","title":"v0.44.0 (2022-03-07)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0430-2022-03-07","title":"v0.43.0 (2022-03-07)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0420-2022-03-07","title":"v0.42.0 (2022-03-07)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Remove python3.6 support and fix tests #579 (munrojm)</li> <li>typo #576 (jmmshn)</li> </ul>"},{"location":"CHANGELOG/#v0411-2022-03-05","title":"v0.41.1 (2022-03-05)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>mongoclient_kwargs #575 (jmmshn)</li> <li>change cleint -&gt; resource in aws tests #574 (jmmshn)</li> </ul>"},{"location":"CHANGELOG/#v0410-2022-02-15","title":"v0.41.0 (2022-02-15)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add header processing abilities to certain <code>Resource</code> classes #569 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0400-2022-02-10","title":"v0.40.0 (2022-02-10)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add authsource option for mongo and gridfs stores #567 (utf)</li> </ul>"},{"location":"CHANGELOG/#v0391-2022-01-27","title":"v0.39.1 (2022-01-27)","text":"<p>Full Changelog</p> <p>Fixed bugs:</p> <ul> <li>Single import-dependence on pynng causes M1 Mac install error #528</li> </ul> <p>Merged pull requests:</p> <ul> <li>Add boto3 to required packages #544 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0390-2022-01-26","title":"v0.39.0 (2022-01-26)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Replace <code>pynng</code> functionality with <code>pyzmq</code> #543 (munrojm)</li> <li>Encode <code>_</code> as <code>--</code> in metadata when using <code>S3Store.write_doc_to_s3</code> #532 (mkhorton)</li> </ul>"},{"location":"CHANGELOG/#v0381-2021-12-10","title":"v0.38.1 (2021-12-10)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add ability to input index hints to count method #524 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0380-2021-12-09","title":"v0.38.0 (2021-12-09)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Fix issue with <code>close</code> and <code>MongoStore</code> and update <code>_collection</code> attribute #523 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0370-2021-12-07","title":"v0.37.0 (2021-12-07)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Revert broken MongoStore auth testing #522 (munrojm)</li> <li>Fix authentication for <code>MongoStore</code> to work with <code>pymongo==4.0</code> #521 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0360-2021-12-06","title":"v0.36.0 (2021-12-06)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Added on-disk MongoDB compatible MontyStore #514 (utf)</li> </ul>"},{"location":"CHANGELOG/#v0340-2021-12-01","title":"v0.34.0 (2021-12-01)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0350-2021-12-01","title":"v0.35.0 (2021-12-01)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Changes to accommodate new pymongo  #517 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0332-2021-12-01","title":"v0.33.2 (2021-12-01)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Patch mongo store connect methods #516 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0331-2021-12-01","title":"v0.33.1 (2021-12-01)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Patch memory store connect method #515 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0330-2021-11-30","title":"v0.33.0 (2021-11-30)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>MongoDB hint support #513 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0323-2021-11-25","title":"v0.32.3 (2021-11-25)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Added option for writable JSONStores (for single JSON files only). #507 (davidwaroquiers)</li> </ul>"},{"location":"CHANGELOG/#v0322-2021-11-23","title":"v0.32.2 (2021-11-23)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Alter sorting query operator to take comma delimited string #510 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0321-2021-11-10","title":"v0.32.1 (2021-11-10)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Default to yaml full loader to fix tests #505 (munrojm)</li> <li>Add GridFSURIStore with support for URI connections #504 (utf)</li> </ul>"},{"location":"CHANGELOG/#v0320-2021-10-11","title":"v0.32.0 (2021-10-11)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Update sorting query operator to take multiple fields #500 (munrojm)</li> <li>Change to S3Store serialization behavior in update() and other Mongolike Store changes #493 (sivonxay)</li> </ul>"},{"location":"CHANGELOG/#v0310-2021-08-14","title":"v0.31.0 (2021-08-14)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add from_launchpad_file classmethod to MongoStore #476 (sivonxay)</li> </ul>"},{"location":"CHANGELOG/#v0304-2021-08-04","title":"v0.30.4 (2021-08-04)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Fix documentation in aggregation and sparse fields #469 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0303-2021-08-04","title":"v0.30.3 (2021-08-04)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Enable enhanced documentation #468 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0302-2021-07-09","title":"v0.30.2 (2021-07-09)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>orjson added to setup.py #465 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0301-2021-07-09","title":"v0.30.1 (2021-07-09)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Switch from monty to orjson for serialization #464 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0300-2021-07-06","title":"v0.30.0 (2021-07-06)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Enable monty encoded direct responses #463 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0294-2021-06-23","title":"v0.29.4 (2021-06-23)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>BugFix: Manual distinct in MongoStore not using Criteria #461 (shyamd)</li> </ul>"},{"location":"CHANGELOG/#v0293-2021-06-21","title":"v0.29.3 (2021-06-21)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Sort query and query operator meta bug fixes #453 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0292-2021-06-18","title":"v0.29.2 (2021-06-18)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Fix API Sanitizing MSONable types in combined types #454 (shyamd)</li> </ul>"},{"location":"CHANGELOG/#v0291-2021-06-15","title":"v0.29.1 (2021-06-15)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Switch from classic bson to pymongo bson #452 (shyamd)</li> </ul>"},{"location":"CHANGELOG/#v0290-2021-06-08","title":"v0.29.0 (2021-06-08)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Maggma API additions #448 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0281-2021-06-08","title":"v0.28.1 (2021-06-08)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Indescriptive error when not specifying any builders in CLI #446</li> <li>Add port auto-negotiation  #445</li> </ul> <p>Merged pull requests:</p> <ul> <li>New release wflow #450 (shyamd)</li> <li>Ensure Store.name is a property #449 (utf)</li> </ul>"},{"location":"CHANGELOG/#v0280-2021-05-26","title":"v0.28.0 (2021-05-26)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Updates the CLI Runner #447 (shyamd)</li> </ul>"},{"location":"CHANGELOG/#v0270-2021-05-12","title":"v0.27.0 (2021-05-12)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Python 3.6 compatability #336</li> </ul> <p>Merged pull requests:</p> <ul> <li>Fix aws module import #435 (utf)</li> <li>coverage #430 (jmmshn)</li> <li>Update AWS Bucket Detection #429 (jmmshn)</li> <li>Add Object Hash to S3Store #427 (jmmshn)</li> <li>Rebuild API module #423 (shyamd)</li> <li>updated documentaion. #419 (jmmshn)</li> <li>Revert \"Bump ipython from 7.16.1 to 7.21.0\" #406 (shyamd)</li> <li>update gridfs store #381 (gpetretto)</li> </ul>"},{"location":"CHANGELOG/#v0260-2021-01-16","title":"v0.26.0 (2021-01-16)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>No Progress bars  #382 (shyamd)</li> </ul>"},{"location":"CHANGELOG/#v0250-2020-12-04","title":"v0.25.0 (2020-12-04)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>FEATURE: Jupyter Commands #276</li> </ul> <p>Merged pull requests:</p> <ul> <li>Python 3.6 Compatibility #352 (shyamd)</li> <li>Automatically parse the dbname from the URI #350 (jmmshn)</li> <li>Setup: msgpack-python was renamed to msgpack #344 (jan-janssen)</li> <li>Ensure MongoStore can safely continue updating when documents are too large #338 (shyamd)</li> </ul>"},{"location":"CHANGELOG/#v0242-2020-11-17","title":"v0.24.2 (2020-11-17)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Fix array unwrapping in distinct #335 (shyamd)</li> </ul>"},{"location":"CHANGELOG/#v0241-2020-11-17","title":"v0.24.1 (2020-11-17)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>mrun failure with 'dict' object has no attribute 'connect' #316</li> <li>FEATURE: Serialized SSH Tunnel #290</li> </ul> <p>Merged pull requests:</p> <ul> <li>Fix Distinct in MongoStore #332 (shyamd)</li> <li>Direct passing of AWS login to S3Store #326 (jmmshn)</li> <li>Wrap SSHTunnelForward and make it MSONable #320 (shyamd)</li> </ul>"},{"location":"CHANGELOG/#v0240-2020-11-02","title":"v0.24.0 (2020-11-02)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Small fix to make sure searchable_fields are updated #303 (jmmshn)</li> </ul>"},{"location":"CHANGELOG/#v0233-2020-09-23","title":"v0.23.3 (2020-09-23)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0232-2020-09-23","title":"v0.23.2 (2020-09-23)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0231-2020-09-21","title":"v0.23.1 (2020-09-21)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>FEATURE: Python file runner #277</li> </ul>"},{"location":"CHANGELOG/#v0230-2020-09-14","title":"v0.23.0 (2020-09-14)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Separate out S3 Object reference keys from search keys #206</li> </ul> <p>Merged pull requests:</p> <ul> <li>Add custom source loading #278 (shyamd)</li> <li>Inject metadata via fields rather than by indicies #265 (shyamd)</li> </ul>"},{"location":"CHANGELOG/#v0223-2020-08-26","title":"v0.22.3 (2020-08-26)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>added context manager for stores #258 (jmmshn)</li> </ul>"},{"location":"CHANGELOG/#v0222-2020-08-21","title":"v0.22.2 (2020-08-21)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Minor bug fixes to S3Store #253 (jmmshn)</li> </ul>"},{"location":"CHANGELOG/#v0221-2020-08-11","title":"v0.22.1 (2020-08-11)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>accept int as sort keys instead of Sort() in .query() and .groupby() #241 (rkingsbury)</li> <li>Update setup.py #225 (jmmshn)</li> </ul>"},{"location":"CHANGELOG/#v0220-2020-07-16","title":"v0.22.0 (2020-07-16)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Ensure Metadata in Documents from GridFS #222 (shyamd)</li> <li>Projection_Builder tests #213 (acrutt)</li> <li>[WIP] Proper multithreading and msgpack fix #205 (jmmshn)</li> <li>Fix projection_builder.update_targets() #179 (acrutt)</li> </ul>"},{"location":"CHANGELOG/#v0210-2020-06-22","title":"v0.21.0 (2020-06-22)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Reconstruct metadata from index in S3 Store #182 (jmmshn)</li> <li>MapBuilder retry_failed Fix #180 (acrutt)</li> <li>MapBuilder retry_failed bug #111 (acrutt)</li> </ul>"},{"location":"CHANGELOG/#v0200-2020-05-02","title":"v0.20.0 (2020-05-02)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Initial Drone Implementation #145 (wuxiaohua1011)</li> <li>parallel s3 store wrting #130 (jmmshn)</li> <li>Make GridFSStore query check files store first. #128 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0191-2020-04-06","title":"v0.19.1 (2020-04-06)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0190-2020-04-06","title":"v0.19.0 (2020-04-06)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>ISSUE: newer_in method incompatible with GridFSStore #113</li> </ul> <p>Merged pull requests:</p> <ul> <li>Fix async #129 (shyamd)</li> <li>small fixes #115 (jmmshn)</li> <li>Store updates #114 (jmmshn)</li> <li>[WIP] Add EndpointCluster and ClusterManager to maggma #66 (wuxiaohua1011)</li> </ul>"},{"location":"CHANGELOG/#v0180-2020-03-23","title":"v0.18.0 (2020-03-23)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Amazon S3 store update #110 (munrojm)</li> </ul>"},{"location":"CHANGELOG/#v0173-2020-03-18","title":"v0.17.3 (2020-03-18)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0172-2020-03-13","title":"v0.17.2 (2020-03-13)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0171-2020-03-12","title":"v0.17.1 (2020-03-12)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Various Bug Fixes #109 (shyamd)</li> <li>Addition of Projection Builder #99 (acrutt)</li> <li>Fix issues with last_updated in MapBuilder #98 (shyamd)</li> <li>autonotebook for tqdm #97 (shyamd)</li> </ul>"},{"location":"CHANGELOG/#v0161-2020-01-28","title":"v0.16.1 (2020-01-28)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0160-2020-01-28","title":"v0.16.0 (2020-01-28)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Onotology generation from builder #59</li> </ul> <p>Merged pull requests:</p> <ul> <li>Add MongoURIStore #93 (shyamd)</li> <li>Update distinct to be more like mongo distinct #92 (shyamd)</li> <li>Add count to maggma store #86 (shyamd)</li> </ul>"},{"location":"CHANGELOG/#v0150-2020-01-23","title":"v0.15.0 (2020-01-23)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Builder Reporting #78</li> <li>ZeroMQ based multi-node processing #76</li> <li>Add time limits to process_item? (Possibly just in MapBuilder?) #45</li> </ul> <p>Merged pull requests:</p> <ul> <li>[WIP] Builder Reporting #80 (shyamd)</li> <li>Updated GroupBuilder #79 (shyamd)</li> <li>New Distributed Processor #77 (shyamd)</li> </ul>"},{"location":"CHANGELOG/#v0141-2020-01-10","title":"v0.14.1 (2020-01-10)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0140-2020-01-10","title":"v0.14.0 (2020-01-10)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Preserve last_updated for MapBuilder #58</li> <li>Move away from mpi4py #51</li> <li>Run serial processor directly from builder #48</li> <li>Update while processing #42</li> <li>Running JSONStore.connect() multiple times leads to undefined behavior #40</li> <li>get_criteria directly invokes mongo commands #38</li> <li>Cursor timeouts common #35</li> <li>Possible solution to \"stalled\" Runner.run ? #29</li> </ul> <p>Merged pull requests:</p> <ul> <li>Release Workflow for Github #75 (shyamd)</li> <li>Documentation #74 (shyamd)</li> <li>Reorg code #69 (shyamd)</li> <li>Updates for new monitoring services #67 (shyamd)</li> <li>fix GridFSStore #64 (gpetretto)</li> <li>Massive refactoring to get ready for v1.0 #62 (shyamd)</li> <li>Bug Fixes #61 (shyamd)</li> <li>GridFSStore bug fix #60 (munrojm)</li> <li>Fix Store serialization with @version #57 (mkhorton)</li> <li>Update builder to work with new monty #56 (mkhorton)</li> </ul>"},{"location":"CHANGELOG/#v0130-2019-03-29","title":"v0.13.0 (2019-03-29)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add timeout to MapBuilder, store process time #54 (mkhorton)</li> <li>Can update pyyaml req? #50 (dwinston)</li> <li>Concat store #47 (shyamd)</li> </ul>"},{"location":"CHANGELOG/#v0120-2018-11-19","title":"v0.12.0 (2018-11-19)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v0110-2018-11-01","title":"v0.11.0 (2018-11-01)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Better printing of validation erorrs #46 (mkhorton)</li> <li>Updates to JointStore and MapBuilder #44 (shyamd)</li> </ul>"},{"location":"CHANGELOG/#v090-2018-10-01","title":"v0.9.0 (2018-10-01)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Non-obvious error message when trying to query a Store that hasn't been connected #41</li> <li>Criteria/properties order of MongoStore.query #37</li> <li>tqdm in Jupyter #33</li> <li>query args order #31</li> </ul> <p>Merged pull requests:</p> <ul> <li>Simplification of Validator class + tests #39 (mkhorton)</li> <li>Fix for Jupyter detection for tqdm #36 (mkhorton)</li> <li>Add tqdm widget inside Jupyter #34 (mkhorton)</li> <li>Change update_targets log level from debug to exception #32 (mkhorton)</li> <li>Jointstore #23 (montoyjh)</li> </ul>"},{"location":"CHANGELOG/#v080-2018-08-22","title":"v0.8.0 (2018-08-22)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>[WIP] Improve/refactor examples and move inside maggma namespace #30 (dwinston)</li> <li>Fix mrun with default num_workers. Add test. #28 (dwinston)</li> </ul>"},{"location":"CHANGELOG/#v065-2018-06-07","title":"v0.6.5 (2018-06-07)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v064-2018-06-07","title":"v0.6.4 (2018-06-07)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v063-2018-06-07","title":"v0.6.3 (2018-06-07)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Add MongograntStore #27 (dwinston)</li> </ul>"},{"location":"CHANGELOG/#v062-2018-06-01","title":"v0.6.2 (2018-06-01)","text":"<p>Full Changelog</p>"},{"location":"CHANGELOG/#v061-2018-06-01","title":"v0.6.1 (2018-06-01)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>Help user if e.g. target store built without lu_field #26 (dwinston)</li> </ul>"},{"location":"CHANGELOG/#v060-2018-05-01","title":"v0.6.0 (2018-05-01)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Progress Bar #21</li> <li>Query Engine equivalent #9</li> </ul> <p>Merged pull requests:</p> <ul> <li>Progress Bars for Multiprocess Runner #24 (shyamd)</li> <li>GridFS Store update: use metadata field, update removes old file(s) #20 (dwinston)</li> </ul>"},{"location":"CHANGELOG/#v050-2018-03-31","title":"v0.5.0 (2018-03-31)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>Need from pymongo collection #18</li> </ul> <p>Merged pull requests:</p> <ul> <li>Useability updates #19 (shyamd)</li> </ul>"},{"location":"CHANGELOG/#040-2018-02-28","title":"0.4.0 (2018-02-28)","text":"<p>Full Changelog</p> <p>Merged pull requests:</p> <ul> <li>New Multiprocessor and MPI Processor #17 (shyamd)</li> <li>groupby change for memory/jsonstore #16 (montoyjh)</li> <li>Rename Schema to Validator #15 (mkhorton)</li> </ul>"},{"location":"CHANGELOG/#030-2018-02-01","title":"0.3.0 (2018-02-01)","text":"<p>Full Changelog</p> <p>Implemented enhancements:</p> <ul> <li>Vault enabled Store #8</li> </ul> <p>Merged pull requests:</p> <ul> <li>PR for generic Schema class #14 (mkhorton)</li> <li>Issue 8 vault store #13 (shreddd)</li> <li>adds grouping function and test to make aggregation-based builds #12 (montoyjh)</li> </ul>"},{"location":"CHANGELOG/#v020-2018-01-01","title":"v0.2.0 (2018-01-01)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>LU translation functions don't serialize #11</li> </ul> <p>Merged pull requests:</p> <ul> <li>Mongolike mixin #10 (montoyjh)</li> </ul>"},{"location":"CHANGELOG/#v010-2017-11-08","title":"v0.1.0 (2017-11-08)","text":"<p>Full Changelog</p> <p>Closed issues:</p> <ul> <li>ditch python 2 and support only 3? #3</li> <li>Seeking clarifications #1</li> </ul> <p>Merged pull requests:</p> <ul> <li>Do not wait until all items are processed to update targets #7 (dwinston)</li> <li>Run builder with either MPI or multiprocessing #6 (matk86)</li> <li>add lava code and tool execution script #5 (gilbertozp)</li> <li>Add eclipse project files to .gitignore #2 (gilbertozp)</li> </ul> <p>* This Changelog was automatically generated by github_changelog_generator</p>"},{"location":"concepts/","title":"Concepts","text":"<p><code>maggma</code>'s core classes -- <code>Store</code> and <code>Builder</code> -- provide building blocks for modular data pipelines. Data resides in one or more <code>Store</code> and is processed by a <code>Builder</code>. The results of the processing are saved in another <code>Store</code>, and so on:</p> <pre><code>flowchart\u00a0LR\n\u00a0\u00a0\u00a0\u00a0s1(Store 1)\u00a0--Builder 1--&gt;\u00a0s2(Store 2) --Builder 2--&gt; s3(Store 3)\ns2 -- Builder 3--&gt;s4(Store 4)</code></pre>"},{"location":"concepts/#store","title":"Store","text":"<p>A major challenge in building scalable data pipelines is dealing with all the different types of data sources out there. Maggma's <code>Store</code> class provides a consistent, unified interface for querying data from arbitrary data sources. It was originally built around MongoDB, so it's interface closely resembles <code>PyMongo</code> syntax. However, Maggma makes it possible to use that same syntax to query other types of databases, such as Amazon S3, GridFS, or even files on disk.</p> <p>Stores are databases containing organized document-based data. They represent either a data source or a data sink. They are modeled around the MongoDB collection although they can represent more complex data sources that auto-alias keys without the user knowing, or even providing concatenation or joining of Stores. Stores implement methods to <code>connect</code>, <code>query</code>, find <code>distinct</code> values, <code>groupby</code> fields, <code>update</code> documents, and <code>remove</code> documents. Stores also implement a number of critical fields for Maggma that help in efficient document processing: the <code>key</code> and the <code>last_updated_field</code>. <code>key</code> is the field that is used to uniquely index the underlying data source. <code>last_updated_field</code> is the timestamp of when that document was last modified.</p>"},{"location":"concepts/#builder","title":"Builder","text":"<p>Builders represent a data processing step, analogous to an extract-transform-load (ETL) operation in a data warehouse model. Much like <code>Store</code>, the <code>Builder</code> class provides a consistent interface for writing data transformations, which are each broken into 3 phases: <code>get_items</code>, <code>process_item</code>, and <code>update_targets</code>:</p> <ol> <li><code>get_items</code>: Retrieve items from the source Store(s) for processing by the next phase</li> <li><code>process_item</code>: Manipulate the input item and create an output document that is sent to the next phase for storage.</li> <li><code>update_target</code>: Add the processed item to the target Store(s).</li> </ol> <p>Both <code>get_items</code> and <code>update_targets</code> can perform IO (input/output) to the data stores. <code>process_item</code> is expected to not perform any IO so that it can be parallelized by Maggma. Builders can be chained together into an array and then saved as a JSON file to be run on a production system.</p>"},{"location":"concepts/#msonable","title":"MSONable","text":"<p>Another challenge in building complex data-transformation codes is keeping track of all the settings necessary to make some output database. One bad solution is to hard-code these settings, but then any modification is difficult to keep track of.</p> <p>Maggma solves this by putting the configuration with the pipeline definition in JSON or YAML files. This is done using the <code>MSONable</code> pattern, which requires that any Maggma object (the databases and transformation steps) can convert itself to a python dictionary with it's configuration parameters in a process called serialization. These dictionaries can then be converted back to the original Maggma object without having to know what class it belonged. <code>MSONable</code> does this by injecting in <code>@class</code> and <code>@module</code> keys that tell it where to find the original python code for that Maggma object.</p>"},{"location":"quickstart/","title":"5-minute <code>maggma</code> quickstart","text":""},{"location":"quickstart/#install","title":"Install","text":"<p>Open your terminal and run the following command.</p> <pre><code>pip install --upgrade maggma\n</code></pre>"},{"location":"quickstart/#format-your-data","title":"Format your data","text":"<p>Structure your data as a <code>list</code> of <code>dict</code> objects, where each <code>dict</code> represents a single record (called a 'document'). Below, we've created some data to represent info about the Teenage Mutant Ninja Turtles.</p> <pre><code>&gt;&gt;&gt; turtles = [{\"name\": \"Leonardo\", \"color\": \"blue\", \"tool\": \"sword\"},\n               {\"name\": \"Donatello\",\"color\": \"purple\", \"tool\": \"staff\"},\n               {\"name\": \"Michelangelo\", \"color\": \"orange\", \"tool\": \"nunchuks\"},\n               {\"name\":\"Raphael\", \"color\": \"red\", \"tool\": \"sai\"}\n            ]\n</code></pre> <p>Structuring your data in this manner enables highly flexible storage options -- you can easily write it to a <code>.json</code> file, place it in a <code>Store</code>, insert it into a Mongo database, etc. <code>maggma</code> is designed to facilitate this.</p> <p>In addition to being structured as a <code>list</code> of <code>dict</code>, every document (<code>dict</code>) must have a key that uniquely identifies it. By default, this key is the <code>task_id</code>, but it can be set to any value you like using the <code>key</code> argument when you instantiate a <code>Store</code>. In the example above, <code>name</code> can serve as a key because all documents have it, and the values are all unique.</p> <p>See Using Stores for more details on structuring data.</p>"},{"location":"quickstart/#create-a-store","title":"Create a <code>Store</code>","text":"<p><code>maggma</code> contains <code>Store</code> classes that connect to MongoDB, Azure, S3 buckets, <code>.json</code> files, system memory, and many more data sources. Regardless of the underlying storage platform, all <code>Store</code> classes implement the same interface for connecting and querying.</p> <p>The simplest store to use is the <code>MemoryStore</code>. It simply loads your data into memory and makes it accessible via <code>Store</code> methods like <code>query</code>, <code>distinct</code>, etc. Note that for this particular store, your data is not saved anywhere - once you close it, the data are lost from RAM! Note that in this example, we've set <code>key='name'</code> when creating the <code>Store</code> because we want to use <code>name</code> as our unique identifier.</p> <pre><code>&gt;&gt;&gt; from maggma.stores import MemoryStore\n&gt;&gt;&gt; store = MemoryStore(key=\"name\")\n</code></pre> <p>See Using Stores for more details on available <code>Store</code> classes.</p>"},{"location":"quickstart/#connect-to-the-store","title":"Connect to the <code>Store</code>","text":"<p>Before you can interact with a store, you have to <code>connect()</code>. This is as simple as</p> <pre><code>store.connect()\n</code></pre> <p>When you are finished, you can close the connection with <code>store.close()</code>.</p> <p>A cleaner (and recommended) way to make sure connections are appropriately closed is to access <code>Store</code> through a context manager (a <code>with</code> statement), like this:</p> <pre><code>with store as s:\n    s.query()\n</code></pre>"},{"location":"quickstart/#add-your-data-to-the-store","title":"Add your data to the <code>Store</code>","text":"<p>To add data to the store, use <code>update()</code>.</p> <pre><code>with store as s:\n    s.update(turtles)\n</code></pre>"},{"location":"quickstart/#query-the-store","title":"Query the <code>Store</code>","text":"<p>Now that you have added your data to a <code>Store</code>, you can leverage <code>maggma</code>'s powerful API to query and analyze it. Here are some examples:</p> <p>See how many documents the <code>Store</code> contains <pre><code>&gt;&gt;&gt; store.count()\n4\n</code></pre></p> <p>Query a single document to see its structure <pre><code>&gt;&gt;&gt; store.query_one({})\n{'_id': ObjectId('66746d29a78e8431daa3463a'), 'name': 'Leonardo', 'color': 'blue', 'tool': 'sword'}\n</code></pre></p> <p>List all the unique values of the <code>color</code> field <pre><code>&gt;&gt;&gt; store.distinct('color')\n['purple', 'orange', 'blue', 'red']\n</code></pre></p> <p>See Understanding Queries for more example queries and the <code>Store</code> interface for more details about available <code>Store</code> methods.</p>"},{"location":"getting_started/advanced_builder/","title":"Advanced Builder Concepts","text":"<p>There are a number of features in <code>maggma</code> designed to assist with advanced features:</p>"},{"location":"getting_started/advanced_builder/#logging","title":"Logging","text":"<p><code>maggma</code> builders have a python <code>logger</code> object that is already setup to output to the correct level. You can directly use it to output <code>info</code>, <code>debug</code>, and <code>error</code> messages.</p> <pre><code>    def get_items(self) -&gt; Iterable:\n        ...\n        self.logger.info(f\"Got {len(to_process_ids)} to process\")\n        ...\n</code></pre>"},{"location":"getting_started/advanced_builder/#querying-for-updated-documents","title":"Querying for Updated Documents","text":"<p>One of the most important features in a builder is incremental building which allows the builder to just process new documents. One of the parameters for a maggma store is the <code>last_updated_field</code> and the <code>last_updated_type</code> which tell <code>maggma</code> how to deal with dates in the source and target documents. This allows us to get the <code>id</code> of any documents that are newer in the target than the newest document in the source:</p> <pre><code>        new_ids = self.target.newer_in(self.source)\n</code></pre>"},{"location":"getting_started/advanced_builder/#speeding-up-data-transfers","title":"Speeding up Data Transfers","text":"<p>Since <code>maggma</code> is designed around Mongo style data sources and sinks, building indexes or in-memory copies of fields you want to search on is critical to get the fastest possible data input/output (IO). Since this is very builder and document style dependent, <code>maggma</code> provides a direct interface to <code>ensure_indexes</code> on a Store. A common paradigm is to do this in the beginning of <code>get_items</code>:</p> <pre><code>    def ensure_indexes(self):\n        self.source.ensure_index(\"some_search_fields\")\n        self.target.ensure_index(self.target.key)\n\n    def get_items(self) -&gt; Iterable:\n        self.ensure_indexes()\n        ...\n</code></pre>"},{"location":"getting_started/advanced_builder/#built-in-templates-for-advanced-builders","title":"Built in Templates for Advanced Builders","text":"<p><code>maggma</code> implements templates for builders that have many of these advanced features listed above:</p> <ul> <li>MapBuilder Creates one-to-one document mapping of items in the source Store to the transformed documents in the target Store.</li> <li>GroupBuilder Creates many-to-one document mapping of items in the source Store to transformed documents in the target Store</li> </ul>"},{"location":"getting_started/advanced_stores/","title":"Configurations and Usage of Advanced <code>store</code>'s","text":""},{"location":"getting_started/advanced_stores/#s3store","title":"S3Store","text":""},{"location":"getting_started/advanced_stores/#configuration","title":"Configuration","text":"<p>The S3Store interfaces with S3 object storage via boto3. For this to work properly, you have to set your basic configuration in <code>~/.aws/config</code> <pre><code>[default]\nsource_profile = default\n</code></pre></p> <p>Then, you have to set up your credentials in <code>~/.aws/credentials</code> <pre><code>[default]\naws_access_key_id = YOUR_KEY\naws_secret_access_key = YOUR_SECRET\n</code></pre></p> <p>For more information on the configuration please see the following documentation. Note that while these configurations are in the <code>~/.aws</code> folder, they are shared by other similar services like the self-hosted minio service.</p>"},{"location":"getting_started/advanced_stores/#basic-usage","title":"Basic Usage","text":"<p>MongoDB is not designed to handle large object storage. As such, we created an abstract object that combines the large object storage capabilities of Amazon S3 and the easy, python-friendly query language of MongoDB. These <code>S3Store</code>s all include an <code>index</code> store that only stores specific queryable data and the object key for retrieving the data from an S3 bucket using the <code>key</code> attribute (called <code>'fs_id'</code> by default).</p> <p>An entry of in the <code>index</code> may look something like this: <pre><code>{\n    fs_id : \"5fc6b87e99071dfdf04ca871\"\n    task_id : \"mp-12345\"\n}\n</code></pre> Please note that since we are giving users the ability to reconstruct the index store using the object metadata, the object size in the <code>index</code> is limited by the metadata and not MongoDB. Different S3 services might have different rules, but the limit is typically smaller: 8 KB for aws</p> <p>The <code>S3Store</code> should be constructed as follows:</p> <pre><code>from maggma.stores import MongoURIStore, S3Store\nstore = MongoURIStore(\n    \"mongodb+srv://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;\",\n    \"atomate_aeccar0_fs_index\",\n    key=\"fs_id\",\n)\ns3store = S3Store(index=index,\n        bucket=\"&lt;&lt;BUCKET_NAME&gt;&gt;\",\n        s3_profile=\"&lt;&lt;S3_PROFILE_NAME&gt;&gt;\",\n        compress= True,\n        endpoint_url= \"&lt;&lt;S3_URL&gt;&gt;\",\n        sub_dir= \"atomate_aeccar0_fs\",\n        s3_workers=4\n       )\n</code></pre> <p>The <code>subdir</code> field creates subdirectories in the bucket to help the user organize their data.</p>"},{"location":"getting_started/advanced_stores/#parallelism","title":"Parallelism","text":"<p>Once you start working with large quantities of data, the speed at which you process this data will often be limited by database I/O. For the most time-consuming upload part of the process, we have implemented thread-level parallelism in the <code>update</code> member function. The <code>update</code> function received an entire chunk of processed data as defined by <code>chunk_size</code>, however since <code>Store.update</code> is typically called in the <code>update_targets</code> part of a builder, where builder execution is not longer multi-threaded. As such, we multithread the execution inside of <code>update</code> using <code>s3_workers</code> threads to perform the database write operation. As a general rule of thumb, if you notice that your update step is taking too long, you should change the <code>s3_worker</code> field which is optimized differently based on server-side resources.</p>"},{"location":"getting_started/group_builder/","title":"Group Builder","text":"<p>Another advanced template in <code>maggma</code> is the <code>GroupBuilder</code>, which groups documents together before applying your function on the group of items. Just like <code>MapBuilder</code>, <code>GroupBuilder</code> also handles incremental building, keeping track of errors, getting only the data you need, and managing timeouts. GroupBuilder won't delete orphaned documents since that reverse relationship isn't valid.</p> <p>Let's create a simple <code>ResupplyBuilder</code>, which will look at the inventory of items and determine what items need resupply. The source document will look something like this:</p> <pre><code>{\n    \"name\": \"Banana\",\n    \"type\": \"fruit\",\n    \"quantity\": 20,\n    \"minimum\": 10,\n    \"last_updated\": \"2019-11-3T19:09:45\"\n}\n</code></pre> <p>Our builder should give us documents that look like this:</p> <pre><code>{\n    \"names\": [\"Grapes\", \"Apples\", \"Bananas\"],\n    \"type\": \"fruit\",\n    \"resupply\": {\n        \"Apples\": 10,\n        \"Bananes\": 0,\n        \"Grapes\": 5\n    },\n    \"last_updated\": \"2019-11-3T19:09:45\"\n}\n</code></pre> <p>To begin, we define our <code>GroupBuilder</code>:</p> <pre><code>from maggma.builders import GroupBuilder\nfrom maggma.core import Store\n\nclass ResupplyBuilder(GroupBuilder):\n    \"\"\"\n    Simple builder that determines which items to resupply\n    \"\"\"\n\n    def __init__(inventory: Store, resupply: Store,resupply_percent : int = 100, **kwargs):\n        \"\"\"\n        Arguments:\n            inventory: current inventory information\n            resupply: target resupply information\n            resupply_percent: the percent of the minimum to include in the resupply\n        \"\"\"\n        self.inventory = inventory\n        self.resupply = resupply\n        self.resupply_percent = resupply_percent\n        self.kwargs = kwargs\n\n        super().__init__(source=inventory, target=resupply, grouping_properties=[\"type\"], **kwargs)\n</code></pre> <p>Note that unlike the previous <code>MapBuilder</code> example, we didn't call the source and target stores as such. Providing more useful names is a good idea in writing builders to make it clearer what the underlying data should look like.</p> <p><code>GroupBuilder</code> inherits from <code>MapBuilder</code> so it has the same configurational parameters.</p> <ul> <li>query: A query to apply to items in the source Store.</li> <li>projection: list of the fields you want to project. This can reduce the data transfer load if you only need certain fields or sub-documents from the source documents</li> <li>timeout: optional timeout on the process function</li> <li>store_process_timeout: adds the process time into the target document for profiling</li> <li>retry_failed: retries running the process function on previously failed documents</li> </ul> <p>One parameter that doesn't work in <code>GroupBuilder</code> is <code>delete_orphans</code>, since the Many-to-One relationship makes determining orphaned documents very difficult.</p> <p>Finally let's get to the hard part which is running our function. We do this by defining <code>unary_function</code></p> <pre><code>    def unary_function(self, items: List[Dict]) -&gt; Dict:\n        resupply = {}\n\n        for item in items:\n            if item[\"quantity\"] &gt; item[\"minimum\"]:\n                resupply[item[\"name\"]] = int(item[\"minimum\"] * self.resupply_percent )\n            else:\n                resupply[item[\"name\"]] = 0\n        return {\"resupply\": resupply}\n</code></pre> <p>Just as in <code>MapBuilder</code>, we're not returning all the extra information typically kept in the originally item. Normally, we would have to write code that copies over the source <code>key</code> and convert it to the target <code>key</code>. Same goes for the <code>last_updated_field</code>. <code>GroupBuilder</code> takes care of this, while also recording errors, processing time, and the Builder version.<code>GroupBuilder</code> also keeps a plural version of the <code>source.key</code> field, so in this example, all the <code>name</code> values will be put together and kept in <code>names</code></p>"},{"location":"getting_started/map_builder/","title":"Map Builder","text":"<p><code>maggma</code> has a built in builder called the <code>MapBuilder</code> which handles a number of tedious tasks in writing a builder. This class is designed to be used similar to a map operator in any other framework in even the map function in python. <code>MapBuilder</code> will take each document in the source store, apply the function you give it, and then store that in the target store. It handles incremental building, keeping track of errors, getting only the data you need, managing timeouts, and deleting orphaned documents through configurational options.</p> <p>Let's create the same <code>MultiplierBuilder</code> we wrote earlier using <code>MapBuilder</code>:</p> <pre><code>from maggma.builders import MapBuilder\nfrom maggma.core import Store\n\nclass MultiplyBuilder(MapBuilder):\n    \"\"\"\n    Simple builder that multiplies the \"a\" sub-document by pre-set value\n    \"\"\"\n</code></pre> <p>Just like before we define a new class, but this time it should inherit from <code>MapBuilder</code>.</p> <pre><code>    def __init__(self, source: Store, target: Store, multiplier: int = 2, **kwargs):\n        \"\"\"\n        Arguments:\n            source: the source store\n            target: the target store\n            multiplier: the multiplier to apply to \"a\" sub-document\n        \"\"\"\n        self.source = source\n        self.target = target\n        self.multiplier = multiplier\n        self.kwargs = kwargs\n\n        kwargs = {k,v in kwargs.items() if k not in [\"projection\",\"delete_orphans\",\"timeout\",\"store_process_time\",\"retry_failed\"]}\n\n        super().__init__(source=source,\n                         target=target,\n                         projection=[\"a\"],\n                         delete_orphans=False,\n                         timeout=10,\n                         store_process_time=True,\n                         retry_failed=True,\n                         **kwargs)\n</code></pre> <p>MapBuilder has a number of configurational options that you can hardcode as above or expose as properties for the user through **kwargs:</p> <ul> <li>projection: list of the fields you want to project. This can reduce the data transfer load if you only need certain fields or sub-documents from the source documents</li> <li>delete_orphans: this will delete documents in the target which don't have a corresponding document in the source</li> <li>timeout: optional timeout on the process function</li> <li>store_process_timeout: adds the process time into the target document for profiling</li> <li>retry_failed: retries running the process function on previously failed documents</li> </ul> <p>Finally let's get to the hard part which is running our function. We do this by defining <code>unary_function</code></p> <pre><code>    def unary_function(self,item):\n        return {\"a\": item[\"a\"] * self.multiplier}\n</code></pre> <p>Note that we're not returning all the extra information typically kept in the originally item. Normally, we would have to write code that copies over the source <code>key</code> and convert it to the target <code>key</code>. Same goes for the <code>last_updated_field</code>. <code>MapBuilder</code> takes care of this, while also recording errors, processing time, and the Builder version.</p>"},{"location":"getting_started/mongodb/","title":"Setting up MongoDB","text":"<p>Many users find MongoDB to best suit their data storage needs. While MongoDB can be installed locally, the easiest route is often to create a Mongo database via a cloud storage solution called MongoDB Atlas, which has a free tier. The setup instructions for using Maggma with MongoDB Atlas are described below:</p> <ol> <li>Sign up for a free account on MongoDB Atlas.</li> <li>Once logged in, select the \"Create a Project\" option and give your project a name (e.g. \"MyProject\"). Add your email address as the Project Owner.</li> <li>Click the \"Build a Database\" button under the \"Deployment &gt; Database\" section and choose the free (i.e. M0) option. Give your cluster a unique name (e.g. \"MyCluster\").</li> <li>Select \"Create\" and enter your desired login credentials that you will use to access your database. You are probably best off not using special characters here since it will be URL-encoded. You should also use different credentials than your usual, since it's not uncommon to share credentials with trusted colleagues. Select \"Finish and Close\" when done.</li> <li>Go to the \"Collections\" tab of your cluster, which is where you will create a database (e.g. \"my_database\") and corresponding data collection (e.g. \"my_collection\") by clicking the \"Add My Own Data\" button.</li> <li>Under the \"Security &gt; Network Access\" section, edit the IP Access List to allow access from anywhere for maximum flexibility.</li> <li>Finally, retrieve your MongoDB URI, which is the address of your MongoDB cluster. You can find your database's URI by clicking the \"Database\" section in the sidebar and then selecting \"Connect &gt; Compass\" and copying the link of the form <code>mongodb+srv://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;</code>.</li> </ol> <p>To test that you can connect to your database, run the following code:</p> <pre><code>from maggma.stores import MongoURIStore\n\n# Define your database credentials\nstore = MongoURIStore(\n    \"mongodb+srv://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;\",\n    \"my_collection\",\n    database=\"my_database\",\n)\n\n# Query the database\nwith store:\n    print(store.count())\n</code></pre> <p>Note</p> <p>If you are using a self-hosted Mongo database, you will probably want to use a <code>MongoStore</code> instead of the <code>MongoURIStore</code>, which takes slightly different arguments.</p>"},{"location":"getting_started/query_101/","title":"Understanding Queries","text":"<p>Putting your data into a <code>maggma</code> <code>Store</code> gives you powerful search, summary, and analytical capabilities. All are based on \"queries\", which specify how you want to search your data, and which parts of it you want to get in return.</p> <p><code>maggma</code> query syntax closely follows MongoDB Query syntax. In this tutorial, we'll cover the syntax of the most common query operations. You can refer to the MongoDB or  pymongo (python interface to MongoDB) documentation for examples of more advanced use cases.</p> <p>Let's create an example dataset describing the Teenage Mutant Ninja Turtles.</p> <pre><code>&gt;&gt;&gt; turtles = [{\"name\": \"Leonardo\",\n                \"color\": \"blue\",\n                \"tool\": \"sword\",\n                \"occupation\": \"ninja\"\n                },\n               {\"name\": \"Donatello\",\n                \"color\": \"purple\",\n                \"tool\": \"staff\",\n                \"occupation\": \"ninja\"\n                },\n               {\"name\": \"Michelangelo\",\n                \"color\": \"orange\",\n                \"tool\": \"nunchuks\",\n                \"occupation\": \"ninja\"\n                },\n               {\"name\":\"Raphael\",\n               \"color\": \"red\",\n               \"tool\": \"sai\",\n               \"occupation\": \"ninja\"\n                },\n               {\"name\":\"Splinter\",\n               \"occupation\": \"sensei\"\n                }\n            ]\n</code></pre> <p>Notice how this data follows the principles described in Structuring <code>Store</code> data: - every document (<code>dict</code>) has a <code>name</code> key with a unique value - every document has a common set of keys (<code>name</code>, <code>occupation</code>). - Note that SOME documents also share the keys <code>tool</code> and <code>color</code>, but not all. This is OK.</p> <p>For the rest of this tutorial, we will assume that this data has already been added to a <code>Store</code> called <code>tmnt_store</code>, which we are going to query.</p>"},{"location":"getting_started/query_101/#the-query-method","title":"The <code>query</code> method","text":"<p><code>Store.query()</code> is the primary method you will use to search your data.</p> <ul> <li><code>query</code> always returns a generator yielding any and all documents that match the query you provide.</li> <li>There are no mandatory arguments. If you run <code>query()</code> you will get a generator containing all documents in the <code>Store</code></li> <li>The first (optional) argument is <code>criteria</code>, which is a query formatted as a <code>dict</code> as described in the next section.</li> <li>You can also specify <code>properties</code>, which is a list of fields from the documents you want to return. This is useful when working with large documents because then you only have to download the data you need rather than the entire document.</li> <li>You can also <code>skip</code> every N documents, <code>limit</code> the number of documents returned, and <code>sort</code> the result by some field.</li> </ul> <p>Since <code>query</code> returns a generator, you will typically want to turn the results into a list, or use them in a <code>for</code> loop.</p> <p>Turn into a list <pre><code>results = [d for d in store.query()]\n</code></pre></p> <p>Use in a <code>for</code> loop <pre><code>for doc in store.query():\n    print(doc)\n</code></pre></p>"},{"location":"getting_started/query_101/#the-structure-of-a-query","title":"The structure of a query","text":"<p>A query is also a <code>dict</code>. Each key in the dict corresponds to a fjeld in the documents you want to query (such as <code>name</code>, <code>color</code>, etc.), and the value is the value of that key that you want to match. For example, a query to select all documents where <code>occupation</code> is <code>ninja</code>, would look like</p> <pre><code>{\"occupation\": \"ninja\"}\n</code></pre> <p>This query will be passed as an argument to <code>Store</code> methods like <code>query_one</code>, <code>query</code>, and <code>count</code>, as demonstrated next.</p>"},{"location":"getting_started/query_101/#example-queries","title":"Example queries","text":""},{"location":"getting_started/query_101/#match-a-single-value","title":"Match a single value","text":"<p>To select all records where a field matches a single value, set the key to the field you want to match and its value to the value you are looking for.</p> <p>Return all records where 'occupation' is 'ninja' <pre><code>&gt;&gt;&gt; with tmnt_store as store:\n...     results = list(store.query({\"occupation\": \"ninja\"}))\n&gt;&gt;&gt; len(results)\n4\n</code></pre></p> <p>Return all records where 'name' is 'Splinter'</p> <pre><code>&gt;&gt;&gt; with tmnt_store as store:\n...     results = list(store.query({\"name\": \"Splinter\"}))\n&gt;&gt;&gt; len(results)\n1\n</code></pre>"},{"location":"getting_started/query_101/#match-any-value-in-a-list-in","title":"Match any value in a list: <code>$in</code>","text":"<p>To find all documents where a field matches one of several different values, use <code>$in</code> with a list of the value you want to search.</p> <pre><code>&gt;&gt;&gt; with tmnt_store as store:\n...     results = list(store.query({\"color\": {\"$in\": [\"red\", \"blue\"]}}))\n&gt;&gt;&gt; len(results)\n2\n</code></pre> <p><code>$in</code> is an example of a \"query operator\". Others include:</p> <ul> <li><code>$nin</code>: a value is NOT in a list (the inverse of the above example)</li> <li><code>$gt</code>, <code>$gte</code>: greater than, greater than or equal to a value</li> <li><code>$lt</code>, <code>$lte</code>: greater than, greater than or equal to a value</li> <li><code>$ne</code>: not equal to a value</li> <li><code>$not</code>: inverts the effect of a query expression, returning results that     do NOT match.</li> </ul> <p>See the MongoDB docs for a complete list.</p> <p>Note</p> <p>When using query operators like <code>$in</code>, you must include a nested <code>dict</code> in your query, where the operator is the key and the search parameters are the value, e.g., the dictionary <code>{\"$in\": [\"red\", \"blue\"]}</code> is the value associated with the search field (<code>color</code>) in the parent dictionary.</p>"},{"location":"getting_started/query_101/#nested-fields","title":"Nested fields","text":"<p>Suppose that our documents had a nested structure, for example, by having separate fields for first and last name:</p> <pre><code>&gt;&gt;&gt; turtles = [{\"name\":\n                    {\"first\": \"Leonardo\",\n                     \"last\": \"turtle\"\n                     },\n                \"color\": \"blue\",\n                \"tool\": \"sword\",\n                \"occupation\": \"ninja\"\n                },\n                ...\n                ]\n</code></pre> <p>You can query nested fields by placing a period <code>.</code> between each level in the hierarchy. For example:</p> <pre><code>&gt;&gt;&gt; with tmnt_store as store:\n...     results = list(store.query({\"name.first\": \"Splinter\"}))\n&gt;&gt;&gt; len(results)\n1\n</code></pre>"},{"location":"getting_started/query_101/#numerical-values","title":"Numerical Values","text":"<p>You can query numerical values in analogous fashion to the examples given above.</p> <p>Note</p> <p>When querying on numerical values, be mindful of the <code>type</code> of the data. Data stored in <code>json</code> format is often converted entirely to <code>str</code>, so if you use a numerical query operator like <code>$gte</code>, you might not get the results you expect unless you first verify that the numerical data in the <code>Store</code> is a <code>float</code> or <code>int</code> .</p>"},{"location":"getting_started/running_builders/","title":"Running Builders","text":"<p><code>maggma</code> is designed to run build-pipelines in a production environment. Builders can be run directly in a python environment, but this gives you none of the performance features such as multiprocessing. The base <code>Builder</code> class implements a simple <code>run</code> method that can be used to run that builder:</p> <pre><code>class MultiplyBuilder(Builder):\n    \"\"\"\n    Simple builder that multiplies the \"a\" sub-document by pre-set value\n    \"\"\"\n\n    ...\n\n\nmy_builder = MultiplyBuilder(source_store,target_store,multiplier=3)\nmy_builder.run()\n</code></pre> <p>A better way to run this builder would be to use the <code>mrun</code> command line tool. Since everything in <code>maggma</code> is MSONable, we can use <code>monty</code> to dump the builders into a JSON file:</p> <pre><code>from monty.serialization import dumpfn\n\ndumpfn(my_builder,\"my_builder.json\")\n</code></pre> <p>Then we can run the builder using <code>mrun</code>:</p> <pre><code>mrun my_builder.json\n</code></pre> <p><code>mrun</code> has a number of useful options:</p> <pre><code>mrun --help\nUsage: mrun [OPTIONS] [BUILDERS]...\n\nOptions:\n  -v, --verbose                   Controls logging level per number of v's\n  -n, --num-workers INTEGER RANGE\n                                  Number of worker processes. Defaults to\n                                  single processing\n  --help                          Show this message and exit.\n</code></pre> <p>We can use the <code>-n</code> option to control how many workers run <code>process_items</code> in parallel. Similarly, <code>-v</code> controls the logging verbosity from just WARNINGs to INFO to DEBUG output.</p> <p>The result will be something that looks like this:</p> <pre><code>2020-01-08 14:33:17,187 - Builder - INFO - Starting Builder Builder\n2020-01-08 14:33:17,217 - Builder - INFO - Processing 100 items\nGet: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 15366.00it/s]\n2020-01-08 14:33:17,235 - MultiProcessor - INFO - Processing batch of 1000 items\nUpdate Targets: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 584.51it/s]\nProcess Items: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100/100 [00:00&lt;00:00, 567.39it/s]\n</code></pre> <p>There are progress bars for each of the three steps, which lets you understand what the slowest step is and the overall progress of the system.</p>"},{"location":"getting_started/running_builders/#running-distributed","title":"Running Distributed","text":"<p><code>maggma</code> can distribute work across multiple computers. There are two steps to this:</p> <ol> <li>Run a <code>mrun</code> manager by providing it with a <code>--url</code> to listen for workers on and <code>--num-chunks</code>(<code>-N</code>) which tells <code>mrun</code> how many sub-pieces to break up the work into. You can can run fewer workers then chunks. This will cause <code>mrun</code> to call the builder's <code>prechunk</code> to get the distribution of work and run distributed work on all workers</li> <li>Run <code>mrun</code> workers b y providing it with a <code>--url</code> to listen for a manager and <code>--num-workers</code> (<code>-n</code>) to tell it how many processes to run in this worker.</li> </ol> <p>The <code>url</code> argument takes a fully qualified url including protocol. <code>tcp</code> is recommended: Example: <code>tcp://127.0.0.1:8080</code></p>"},{"location":"getting_started/running_builders/#running-scripts","title":"Running Scripts","text":"<p><code>mrun</code> has the ability to run Builders defined in python scripts or in jupyter-notebooks.</p> <p>The only requirements are:</p> <ol> <li>The builder file has to be in a sub-directory from where <code>mrun</code> is called.</li> <li>The builders you want to run are in a variable called <code>__builder__</code> or <code>__builders__</code></li> </ol> <p><code>mrun</code> will run the whole python/jupyter file, grab the builders in these variables and adds these builders to the builder queue.</p> <p>Assuming you have a builder in a python file: <code>my_builder.py</code> <pre><code>class MultiplyBuilder(Builder):\n    \"\"\"\n    Simple builder that multiplies the \"a\" sub-document by pre-set value\n    \"\"\"\n\n    ...\n\n__builder__ = MultiplyBuilder(source_store,target_store,multiplier=3)\n</code></pre></p> <p>You can use <code>mrun</code> to run this builder and parallelize for you: <pre><code>mrun -n 2 -v my_builder.py\n</code></pre></p>"},{"location":"getting_started/running_builders/#running-multiple-builders","title":"Running Multiple Builders","text":"<p><code>mrun</code> can run multiple builders. You can have multiple builders in a single file: <code>json</code>, <code>python</code>, or <code>jupyter-notebook</code>. Or you can chain multiple files in the order you want to run them: <pre><code>mrun -n 32 -vv my_first_builder.json builder_2_and_3.py last_builder.ipynb\n</code></pre></p> <p><code>mrun</code> will then execute the builders in these files in order.</p>"},{"location":"getting_started/running_builders/#reporting-build-state","title":"Reporting Build State","text":"<p><code>mrun</code> has the ability to report the status of the build pipeline to a user-provided <code>Store</code>. To do this, you first have to save the <code>Store</code> as a JSON or YAML file. Then you can use the <code>-r</code> option to give this to <code>mrun</code>. It will then periodically add documents to the <code>Store</code> for one of 3 different events:</p> <ul> <li><code>BUILD_STARTED</code> - This event tells us that a new builder started, the names of the <code>sources</code> and <code>targets</code> as well as the <code>total</code> number of items the builder expects to process</li> <li><code>UPDATE</code> - This event tells us that a batch of items was processed and is going to <code>update_targets</code>. The number of items is stored in <code>items</code>.</li> <li><code>BUILD_ENDED</code> - This event tells us the build process finished this specific builder. It also indicates the total number of <code>errors</code> and <code>warnings</code> that were caught during the process.</li> </ul> <p>These event docs also contain the <code>builder</code>, a <code>build_id</code> which is unique for each time a builder is run and anonymous but unique ID for the machine the builder was run on.</p>"},{"location":"getting_started/running_builders/#profiling-memory-usage-of-builders","title":"Profiling Memory Usage of Builders","text":"<p><code>mrun</code> can optionally profile the memory usage of a running builder by using the Memray Python memory profiling tool (Memray). To get started, Memray should be installed in the same environment as <code>maggma</code> using <code>pip install memray</code> (r <code>pip install maggma[memray]</code>).</p> <p>Setting the <code>--memray</code> (<code>-m</code>) option to <code>on</code>, or <code>True</code>, will signal <code>mrun</code> to profile the memory usage of any builders passed to <code>mrun</code> as the builders are running. The profiler also supports profiling of both single and forked processes. For example, spawning multiple processes in <code>mrun</code> with <code>-n</code> will signal the profiler to track any forked child processes spawned from the parent process.</p> <p>A basic invocation of the memory profiler using the <code>mrun</code> command line tool would look like this: <pre><code>mrun --memray on my_builder.json\n</code></pre></p> <p>The profiler will generate two files after the builder finishes: 1. An output <code>.bin</code> file that is dumped by default into the <code>temp</code> directory, which is platform/OS dependent. For Linux/MacOS this will be <code>/tmp/</code> and for Windows the target directory will be <code>C:\\TEMP\\</code>.The output file will have a generic naming pattern as follows: <code>BUILDER_NAME_PASSED_TO_MRUN + BUILDER_START_DATETIME_ISO.bin</code>, e.g., <code>my_builder.json_2023-06-09T13:57:48.446361.bin</code>. 2. A <code>.html</code> flamegraph file that will be written to the same directory as the <code>.bin</code> dump file. The flamegraph will have a naming pattern similar to the following: <code>memray-flamegraph-my_builder.json_2023-06-09T13:57:48.446361.html</code>. The flamegraph can be viewed using any web browser.</p> <p>Note: Different platforms/operating systems purge their system's <code>temp</code> directory at different intervals. It is recommended to move at least the <code>.bin</code> file to a more stable location. The <code>.bin</code> file can be used to recreate the flamegraph at anytime using the Memray CLI.</p> <p>Using the flag <code>--memray-dir</code> (<code>-md</code>) allows for specifying an output directory for the <code>.bin</code> and <code>.html</code> files created by the profiler. The provided directory will be created if the directory does not exist, mimicking the <code>mkdir -p</code> command.</p> <p>Further data visualization and transform examples can be found in Memray's documentation (Memray reporters).</p>"},{"location":"getting_started/simple_builder/","title":"Writing a Builder","text":""},{"location":"getting_started/simple_builder/#builder-architecture","title":"Builder Architecture","text":"<p>A <code>Builder</code> is a class that inherits from <code>maggma.core.Builder</code> and implement 3 methods:</p> <ul> <li><code>get_items</code>:  This method should return some iterable of items to run through <code>process_item</code></li> <li><code>process_item</code>: This method should take a single item, process it, and return the processed item</li> <li><code>update_targets</code>: This method should take a list of processed items and update the target stores.</li> </ul> <p>To make this less abstract, we will write a builder that multiplies the \"a\" sub-document by a pre-configured <code>multiplier</code>. Let's assume we have some source collection in MongoDB with documents that look like this:</p> <pre><code>{\n    \"id\": 1,\n    \"a\": 3,\n    \"last_updated\": \"2019-11-3\"\n}\n</code></pre>"},{"location":"getting_started/simple_builder/#class-definition-and-__init__","title":"Class definition and <code>__init__</code>","text":"<p>A simple class definition for a Maggma-based builder looks like this:</p> <pre><code>from maggma.core import Builder\nfrom maggma.core import Store\n\nclass MultiplyBuilder(Builder):\n    \"\"\"\n    Simple builder that multiplies the \"a\" sub-document by pre-set value\n    \"\"\"\n</code></pre> <p>The <code>__init__</code> for a builder can have any set of parameters. Generally, you want a source <code>Store</code> and a target <code>Store</code> along with any parameters that configure the builder. Due to the <code>MSONable</code> pattern, any parameters to <code>__init__</code> have to be stored as attributes. A simple <code>__init__</code> would look like this:</p> <pre><code>    def __init__(self, source: Store, target: Store, multiplier: int = 2, **kwargs):\n        \"\"\"\n        Arguments:\n            source: the source store\n            target: the target store\n            multiplier: the multiplier to apply to \"a\" sub-document\n        \"\"\"\n        self.source = source\n        self.target = target\n        self.multiplier = multiplier\n        self.kwargs = kwargs\n\n        super().__init__(sources=source,targets=target,**kwargs)\n</code></pre> <p>Python type annotations provide a really nice way of documenting the types we expect and being able to later type check using <code>mypy</code>. We defined the type for <code>source</code> and <code>target</code> as <code>Store</code> since we only care that implements that pattern. How exactly these <code>Store</code>s operate doesn't concern us here.</p> <p>Note that the <code>__init__</code> arguments: <code>source</code>, <code>target</code>, <code>multiplier</code>, and <code>kwargs</code> get saved as attributes:</p> <pre><code>        self.source = source\n        self.target = target\n        self.multiplier = multiplier\n        self.kwargs = kwargs\n</code></pre> <p>Finally, we want to call the base <code>Builder</code>'s <code>__init__</code> to tell it our sources and targets for this builder. In addition, we pass along any extra parameters that might configured the base builder class.</p> <pre><code>super().__init__(sources=source,targets=target,**kwargs)\n</code></pre> <p>Calling the parent class <code>__init__</code> is a good practice as sub-classing builders is a good way to encapsulate complex logic.</p>"},{"location":"getting_started/simple_builder/#get_items","title":"<code>get_items</code>","text":"<p><code>get_items</code> is conceptually a simple method to implement, but in practice can easily be more code than the rest of the builder. All of the logic for getting data from the sources has to happen here, which requires some planning. <code>get_items</code> should also sort all of the data into individual items to process. This simple builder has a very easy <code>get_items</code>:</p> <pre><code>    def get_items(self) -&gt; Iterator:\n        \"\"\"\n        Gets induvidual documents to multiply\n        \"\"\"\n\n        return self.source.query()\n</code></pre> <p>Here, get items just returns the results of <code>query()</code> from the store. It could also have been written as a generator:</p> <pre><code>    def get_items(self) -&gt; Iterable:\n        \"\"\"\n        Gets induvidual documents to multiply\n        \"\"\"\n        for doc in self.source.query():\n            yield doc\n</code></pre> <p>We could have also returned a list of items:</p> <pre><code>    def get_items(self) -&gt; Iterable:\n        \"\"\"\n        Gets induvidual documents to multiply\n        \"\"\"\n        docs = list(self.source.query())\n</code></pre> <p>One advantage of using the generator approach is it is less memory intensive than the approach where a list of items returned. For large datasets, returning a list of all items for processing may be prohibitive due to memory constraints.</p>"},{"location":"getting_started/simple_builder/#process_item","title":"<code>process_item</code>","text":"<p><code>process_item</code> just has to do the parallelizable work on each item. Since the item is whatever comes out of <code>get_items</code>, you know exactly what it should be. It may be a single document, a list of documents, a mapping, a set, etc.</p> <p>Our simple process item just has to multiply one field by <code>self.multiplier</code>:</p> <pre><code>    def process_item(self, item : Dict) -&gt; Dict:\n        \"\"\"\n        Multiplies the \"a\" sub-document by self.multiplier\n        \"\"\"\n        new_item = dict(**item)\n        new_item[\"a\"] *= self.multiplier\n        return new_item\n</code></pre>"},{"location":"getting_started/simple_builder/#update_targets","title":"<code>update_targets</code>","text":"<p>Finally, we have to put the processed item in to the target store:</p> <pre><code>    def update_targets(self,items: List[Dict]):\n        \"\"\"\n        Adds the processed items into the target store\n        \"\"\"\n        self.target.update(items)\n</code></pre> <p>Note</p> <p>Note that whatever <code>process_item</code> returns, <code>update_targets</code> takes a <code>List</code> of these: For instance, if <code>process_item</code> returns <code>str</code>, then <code>update_targets</code> would look like: <pre><code>    def update_target(self,items: List[str]):\n</code></pre></p> <p>Putting it all together we get:</p> <pre><code>from typing import Dict, Iterable, List\nfrom maggma.core import Builder\nfrom maggma.core import Store\n\nclass MultiplyBuilder(Builder):\n    \"\"\"\n    Simple builder that multiplies the \"a\" sub-document by pre-set value\n    \"\"\"\n\n\n    def __init__(self, source: Store, target: Store, multiplier: int = 2, **kwargs):\n        \"\"\"\n        Arguments:\n            source: the source store\n            target: the target store\n            multiplier: the multiplier to apply to \"a\" sub-document\n        \"\"\"\n        self.source = source\n        self.target = target\n        self.multiplier = multiplier\n        self.kwargs = kwargs\n\n        super().__init__(sources=source,targets=target,**kwargs)\n\n    def get_items(self) -&gt; Iterable:\n        \"\"\"\n        Gets induvidual documents to multiply\n        \"\"\"\n        docs = list(self.source.query())\n\n    def process_item(self, item : Dict) -&gt; Dict:\n        \"\"\"\n        Multiplies the \"a\" sub-document by self.multiplier\n        \"\"\"\n        new_item = dict(**item)\n        new_item[\"a\"] *= self.multiplier\n        return new_item\n\n\n    def update_targets(self,items: List[Dict]):\n        \"\"\"\n        Adds the processed items into the target store\n        \"\"\"\n        self.target.update(items)\n</code></pre>"},{"location":"getting_started/simple_builder/#distributed-processing","title":"Distributed Processing","text":"<p><code>maggma</code> can distribute a builder across multiple computers.</p> <p>The <code>Builder</code> must have a <code>prechunk</code> method defined. <code>prechunk</code> should do a subset of <code>get_items</code> to figure out what needs to be processed and then return dictionaries that modify the <code>Builder</code> in-place to only work on each subset.</p> <p>For example, if in the above example we'd first have to update the builder to be able to work on a subset of keys. One pattern is to define a generic <code>query</code> argument for the builder and use that in get items:</p> <pre><code>    def __init__(self, source: Store, target: Store, multiplier: int = 2, query: Optional[Dict] = None, **kwargs):\n        \"\"\"\n        Arguments:\n            source: the source store\n            target: the target store\n            multiplier: the multiplier to apply to \"a\" sub-document\n        \"\"\"\n        self.source = source\n        self.target = target\n        self.multiplier = multiplier\n        self.query = query\n        self.kwargs = kwargs\n\n        super().__init__(sources=source,targets=target,**kwargs)\n\n    def get_items(self) -&gt; Iterable:\n        \"\"\"\n        Gets induvidual documents to multiply\n        \"\"\"\n        query = self.query or {}\n        docs = list(self.source.query(criteria=query))\n</code></pre> <p>Then we can define a prechunk method that modifies the <code>Builder</code> dict in place to operate on just a subset of the keys:</p> <pre><code>    from maggma.utils import grouper\n    def prechunk(self, number_splits: int) -&gt; Iterable[Dict]:\n        keys  = self.source.distinct(self.source.key)\n        for split in grouper(keys, N):\n            yield {\n                \"query\": {self.source.key: {\"$in\": list(split)}}\n            }\n</code></pre> <p>When distributed processing runs, it will modify the <code>Builder</code> dictionary in place by the prechunk dictionary. In this case, each builder distribute to a worker will get a modified <code>query</code> parameter that only runs on a subset of all possible keys.</p>"},{"location":"getting_started/stores/","title":"Using <code>Store</code>","text":"<p>A <code>Store</code> is just a wrapper to access data from a data source. That data source is typically a MongoDB collection, but it could also be an Amazon S3 bucket, a GridFS collection, or folder of files on disk. <code>maggma</code> makes interacting with all of these data sources feel the same (see the <code>Store</code> interface, below). <code>Store</code> can also perform logic, concatenating two or more <code>Store</code> together to make them look like one data source for instance.</p> <p>The benefit of the <code>Store</code> interface is that you only have to write a <code>Builder</code> once. As your data moves or evolves, you simply point it to different <code>Store</code> without having to change your processing code.</p>"},{"location":"getting_started/stores/#structuring-store-data","title":"Structuring <code>Store</code> data","text":"<p>Because <code>Store</code> is built around a MongoDB-like query syntax, data that goes into <code>Store</code> needs to be structured similarly to MongoDB data. In python terms, that means the data in a <code>Store</code> must be structured as a <code>list</code> of <code>dict</code>, where each <code>dict</code> represents a single record (called a 'document').</p> <pre><code>data = [{\"AM\": \"sunrise\"}, {\"PM\": \"sunset\"} ... ]\n</code></pre> <p>Note that this structure is very similar to the widely-used JSON format. So structuring your data in this manner enables highly flexible storage options -- you can easily write it to a <code>.json</code> file, place it in a <code>Store</code>, insert it into a Mongo database, etc. <code>maggma</code> is designed to facilitate this.</p> <p>In addition to being structured as a <code>list</code> of <code>dict</code>, every document (<code>dict</code>) must have a key that uniquely identifies it. By default, this key is the <code>task_id</code>, but it can be set to any value you like using the <code>key</code> argument when you instantiate a <code>Store</code>.</p> <pre><code>data = [{\"task_id\": 1, \"AM\": \"sunrise\"}, {\"task_id: 2, \"PM\": \"sunset\"} ... ]\n</code></pre> <p>Just to emphasize - every document must have a <code>task_id</code>, and the value of <code>task_id</code> must be unique for every document. The rest of the document structure is up to you, but <code>maggma</code> works best when every document follows a pre-defined schema (i.e., all <code>dict</code> have the same set of keys / same structure).</p>"},{"location":"getting_started/stores/#the-store-interface","title":"The <code>Store</code> interface","text":"<p>All <code>Store</code> provide a number of basic methods that facilitate querying, updating, and removing data:</p> <ul> <li><code>query</code>: Standard mongo style <code>find</code> method that lets you search the store. See Understanding Queries for more details about the query syntax.</li> <li><code>query_one</code>: Same as above but limits returned results to just the first document that matches your query. Very useful for understanding the structure of the returned data.</li> <li><code>count</code>: Counts documents in the <code>Store</code></li> <li><code>distinct</code>: Returns a list of distinct values of a field.</li> <li><code>groupby</code>: Similar to query but performs a grouping operation and returns sets of documents.</li> <li><code>update</code>: Update (insert) documents into the <code>Store</code>. This will overwrite documents if the key field matches.</li> <li><code>remove_docs</code>: Removes documents from the underlying data source.</li> <li><code>newer_in</code>: Finds all documents that are newer in the target collection and returns their <code>key</code>s. This is a very useful way of performing incremental processing.</li> <li><code>ensure_index</code>: Creates an index for the underlying data-source for fast querying.</li> <li><code>last_updated</code>: Finds the most recently updated <code>last_updated_field</code> value and returns that. Useful for knowing how old a data-source is.</li> </ul> <p>Note</p> <p>If you are familiar with <code>pymongo</code>, you may find the comparison table below helpful. This table illustrates how <code>maggma</code> method and argument names map onto <code>pymongo</code> concepts.</p> <code>maggma</code> <code>pymongo</code> equivalent methods <code>query_one</code> <code>find_one</code> <code>query</code> <code>find</code> <code>count</code> <code>count_documents</code> <code>distinct</code> <code>distinct</code> <code>groupby</code> <code>group</code> <code>update</code> <code>insert</code> arguments <code>criteria={}</code> <code>filter={}</code> <code>properties=[]</code> <code>projection=[]</code>"},{"location":"getting_started/stores/#creating-a-store","title":"Creating a Store","text":"<p>All <code>Store</code>s have a few basic arguments that are critical for basic usage. Every <code>Store</code> has two attributes that the user should customize based on the data contained in that store: <code>key</code> and <code>last_updated_field</code>.</p> <p>The <code>key</code> defines how the <code>Store</code> tells documents apart. Typically this is <code>_id</code> in MongoDB, but you could use your own field (be sure all values under the key field can be used to uniquely identify documents).</p> <p><code>last_updated_field</code> tells <code>Store</code> how to order the documents by a date, which is typically in the <code>datetime</code> format, but can also be an ISO 8601-format (ex: <code>2009-05-28T16:15:00</code>) <code>Store</code>s can also take a <code>Validator</code> object to make sure the data going into it obeys some schema.</p> <p>In the example below, we create a <code>MongoStore</code>, which connects to a MongoDB database. To create this store, we have to provide <code>maggma</code> the connection details to the database like the hostname, collection name, and authentication info. Note that we've set <code>key='name'</code> because we want to use that <code>name</code> as our unique identifier.</p> <pre><code>&gt;&gt;&gt; store = MongoStore(database=\"my_db_name\",\n                       collection_name=\"my_collection_name\",\n                       username=\"my_username\",\n                       password=\"my_password\",\n                       host=\"my_hostname\",\n                       port=27017,\n                       key=\"name\",\n                    )\n</code></pre> <p>The specific arguments required to create a <code>Store</code> depend on the underlying format. For example, the <code>MemoryStore</code>, which just loads data into memory, requires no arguments to instantiate. Refer to the list of Stores below (and their associated documentation) for specific details.</p>"},{"location":"getting_started/stores/#connecting-to-a-store","title":"Connecting to a <code>Store</code>","text":"<p>You must connect to a store by running <code>store.connect()</code> before querying or updating the store. If you are operating on the stores inside of another code it is recommended to use the built-in context manager, e.g.:</p> <pre><code>with MongoStore(...) as store:\n    store.query()\n</code></pre> <p>This will take care of the <code>connect()</code> automatically while ensuring that the connection is closed properly after the store tasks are complete.</p>"},{"location":"getting_started/stores/#list-of-stores","title":"List of Stores","text":"<p>Current working and tested <code>Store</code> include the following. Click the name of each store for more detailed documentation.</p> <ul> <li><code>MongoStore</code>: interfaces to a MongoDB Collection using port and hostname.</li> <li><code>MongoURIStore</code>: interfaces to a MongoDB Collection using a \"mongodb+srv://\" URI.</li> <li><code>MemoryStore</code>: just a Store that exists temporarily in memory</li> <li><code>JSONStore</code>: builds a MemoryStore and then populates it with the contents of the given JSON files</li> <li><code>FileStore</code>: query and add metadata to files stored on disk as if they were in a database</li> <li><code>GridFSStore</code>: interfaces to GridFS collection in MongoDB using port and hostname.</li> <li><code>GridFSURIStore</code>: interfaces to GridFS collection in MongoDB using a \"mongodb+srv://\" URI.</li> <li><code>S3Store</code>: provides an interface to an S3 Bucket either on AWS or self-hosted solutions (additional documentation)</li> <li><code>ConcatStore</code>: concatenates several Stores together so they look like one Store</li> <li><code>VaultStore</code>: uses Vault to get credentials for a MongoDB database</li> <li><code>AliasingStore</code>: aliases keys from the underlying store to new names</li> <li><code>SandboxStore: provides permission control to documents via a</code>_sbxn` sandbox key</li> <li><code>JointStore</code>: joins several MongoDB collections together, merging documents with the same <code>key</code>, so they look like one collection</li> <li><code>AzureBlobStore</code>: provides an interface to Azure Blobs for the storage of large amount of data</li> <li><code>MontyStore</code>: provides an interface to montydb for in-memory or filesystem-based storage</li> <li><code>MongograntStore</code>: (DEPRECATED) uses Mongogrant to get credentials for MongoDB database</li> </ul>"},{"location":"getting_started/using_file_store/","title":"Using <code>FileStore</code> for files on disk","text":"<p>The first step in any <code>maggma</code> pipeline is creating a <code>Store</code> so that data can be queried and transformed. Often times your data will originate as files on disk (e.g., calculation output files, files generated by instruments, etc.). <code>FileStore</code> provides a convenient way to access this type of data as if it were in a database, making it possible to <code>query</code>, add metadata, and run <code>Builder</code> on it.</p> <p>Suppose you have some data files organized in the following directory structure:</p> <p></p>"},{"location":"getting_started/using_file_store/#creating-the-filestore","title":"Creating the <code>FileStore</code>","text":"<p>To create a <code>Filestore</code>, simply pass the path to the top-level directory that contains the files.</p> <pre><code>&gt;&gt;&gt; fs = FileStore('/path/to/file_store_test/')\n&gt;&gt;&gt; fs.connect()\n</code></pre> <p>On <code>connect()</code>, <code>FileStore</code> iterates through all files in the base directory and all subdirectories. For each file, it creates dict-like record based on the file's metadata such as name, size, last modification date, etc. These records are kept in memory using an internal <code>MemoryStore</code>. An example record is shown below.</p> <pre><code>{'_id': ObjectId('625e581113cef6275a992abe'),\n 'name': 'input.in',\n 'path': '/test_files/file_store_test/calculation1/input.in',\n 'parent': 'calculation1',\n 'size': 90,\n 'file_id': '2d12e9803fa0c6eaffb065c8dc3cf4fe',\n 'last_updated': datetime.datetime(2022, 4, 19, 5, 23, 54, 109000),\n 'hash': 'd42c9ff24dc2fde99ed831ec767bd3fb',\n 'orphan': False,\n 'contents': 'This is the file named input.in\\nIn directory calculation1\\nin the FileStore test directory.'}\n</code></pre>"},{"location":"getting_started/using_file_store/#choosing-files-to-index","title":"Choosing files to index","text":"<p>To restrict which files are indexed by the Store (which can improve performance), the optional keyword arguments <code>max_depth</code> and <code>file_filters</code> can be used. For example, to index only files ending in \".in\", use</p> <pre><code>&gt;&gt;&gt; fs = FileStore('/path/to/my/data', file_filters=[\"*.in\"])\n</code></pre> <p>You can pass multiple <code>file_filters</code> and use regex-like fnmatch patterns as well. For example, to index all files ending in \".in\" or named \"test-X.txt\" where X is any single letter between a and d, use</p> <pre><code>&gt;&gt;&gt; fs = FileStore('/path/to/my/data', file_filters=[\"*.in\",\"test-[abcd].txt\"])\n</code></pre> <p>If you only want to index the root directory and exclude all subdirectories, use <code>max_depth=0</code>, e.g.</p> <pre><code>&gt;&gt;&gt; fs = FileStore('/path/to/my/data', max_depth=0)\n</code></pre>"},{"location":"getting_started/using_file_store/#write-access","title":"Write access","text":"<p>By default, the <code>FileStore</code> is read-only. However you can set <code>read_only=False</code> if you want to add additional metadata to the data (See \"Adding Metadata\" below). This metadata is stored in a .json file placed in the root directory of the <code>FileStore</code> (the name of the file can be customized with the <code>json_name</code> keyword argument.)</p> <pre><code>&gt;&gt;&gt; fs = FileStore('/path/to/my/data', read_only=False, json_name='my_store.json')\n</code></pre> <p>Several methods that modify the contents of the <code>FileStore</code> such as <code>add_metadata</code>, <code>update</code>, and <code>remove_docs</code> will not work unless the store is writable (i.e., <code>read_only=False</code>).</p>"},{"location":"getting_started/using_file_store/#file-identifiers-file_id","title":"File identifiers (<code>file_id</code>)","text":"<p>Each file is uniquely identified by a <code>file_id</code> key, which is computed from the hash of the file's path relative to the base <code>FileStore</code> directory. Unique identifiers for every file are necessary to enable <code>Builder</code> to work correctly and for associating custom metadata (See \"Adding Metadata\" below). By using the relative path instead of the absolute path makes it possible to move the entire <code>FileStore</code> to a new location on disk without changing <code>file_id</code>  (as long as the relative paths don't change).</p>"},{"location":"getting_started/using_file_store/#connecting-and-querying","title":"Connecting and querying","text":"<p>As with any <code>Store</code>, you have to <code>connect()</code> before you can query any data from a <code>FileStore</code>. After that, you can use <code>query_one()</code> to examine a single document or <code>query()</code> to return an iterator of matching documents. For example, let's print the parent directory of each of the files named \"input.in\" in our example <code>FileStore</code>:</p> <pre><code>&gt;&gt;&gt; fs.connect()\n&gt;&gt;&gt; [d[\"parent\"] for d in fs.query({\"name\":\"input.in\"})]\n['calculation2', 'calculation1']\n</code></pre>"},{"location":"getting_started/using_file_store/#performance","title":"Performance","text":"<p>NOTE <code>FileStore</code> can take a long time to <code>connect()</code> when there are more than a few hundred files in the directory. This is due to limitations of the <code>mongomock</code> package that powers the internal <code>MemoryStore</code>. We hope to identify a more performant alternative in the near future. In the mean time, use <code>file_filters</code> and <code>max_depth</code> to limit the total number of files in the <code>FileStore</code>.</p>"},{"location":"getting_started/using_file_store/#file-contents","title":"File Contents","text":"<p>When you <code>query()</code> data, <code>FileStore</code> attempts to read the contents of each matching file and include them in the <code>contents</code> key of the returned dictionary, as you can see in the example above. There is an optional keyword argument <code>contents_size_limit</code> which specifies the maximum size of file that <code>FileStore</code> will attempt to read.</p> <p>At present, this only works with text files and the entire file contents are returned as a single string. If a file is too large to read, or if <code>FileStore</code> was unable to open the file (because it is a binary file, etc.), then you will see <code>contents</code> populated with a message that beings with <code>\"Unable to read:</code>. This behavior may change in the future.</p>"},{"location":"getting_started/using_file_store/#adding-metadata","title":"Adding metadata","text":"<p>As long as a store is not read-only (see #write-access), you can <code>update()</code> documents in it just like any other <code>Store</code>. This is a great way to associate additional information with raw data files. For example, if you have a store of files generated by an instrument, you can add metadata related to the environmental conditions, the sample that was tested, etc.</p>"},{"location":"getting_started/using_file_store/#update-method","title":"<code>update</code> method","text":"<p>You can use <code>update()</code> to add keys to the <code>FileStore</code> records. For example, to add some tags to the files named \"input.in\", use:</p> <pre><code>docs = [d for d in fs.query({\"name\":\"input.in\"})]\nfor d in docs:\n    d[\"tags\"] = [\"preliminary\"]\nfs.update(docs)\n</code></pre> <p>The above steps will result in the following contents being added to the .json file. This metadata will be automatically read back in next time you connect to the Store.</p> <pre><code>[{\"path\":\".../file_store_test/calculation2/input.in\",\n\"file_id\":\"3c3012f84c162e9ff9bb834c53dd1f58\",\n\"tags\":[\"preliminary\"]},\n{\"path\":\".../file_store_test/calculation1/input.in\",\n\"file_id\":\"fde43ea119034eb8732d6f3f0d9802ce\",\n\"tags\":[\"preliminary\"]}]\n</code></pre> <p>Notice that only the items modified with extra keys are written to the JSON (i.e., if you have 10 items in the store but add metadata to just two, only the two items will be written to the JSON). The purpose of this behavior is to prevent any duplication of data. The <code>file_id</code> and <code>path</code> are retained in the JSON file to make each metadata record manually identifiable.</p>"},{"location":"getting_started/using_file_store/#add_metadata-convenience-method","title":"<code>add_metadata</code> convenience method","text":"<p>A more convenient way to add metadata is via the <code>add_metadata</code> method. To use it, just pass a query to identify the documents you want to update, and a dict to add to the document. Here is what the example above would look like using <code>add_metadata</code></p> <pre><code>fs.add_metadata({\"name\":\"input.in\"}, {\"tags\":[\"preliminary\"]})\n</code></pre>"},{"location":"getting_started/using_file_store/#automatic-metadata","title":"Automatic metadata","text":"<p>You can even define a function to automatically create metadata from file or directory names. For example, if you prefix all your files with datestamps (e.g., '2022-05-07_experiment.csv'), you can write a simple string parsing function to extract information from any key in a <code>FileStore</code> record and pass the function as an argument to <code>add_metadata</code>.</p> <p>For example, to extract the date from files named like '2022-05-07_experiment.csv' and add it to the 'date' field:</p> <pre><code>&gt;&gt;&gt; def get_date_from_filename(d):\n    \"\"\"\n    Args:\n        d: An item returned from the `FileStore`\n    \"\"\"\n    return {\"date\": d[\"name\"].split(\"_\")[0],\n            \"test_name\": d[\"name\"].split(\"_\")[1]\n            }\n\n&gt;&gt;&gt; fs.add_metadata({}, auto_data=get_date_from_filename)\n</code></pre>"},{"location":"getting_started/using_file_store/#protected-keys","title":"Protected Keys","text":"<p>Note that when using any of the above methods, you cannot modify any keys that are populated by default (e.g. <code>name</code>, <code>parent</code>, <code>file_id</code>), because they are derived directly from the files on disk.</p>"},{"location":"getting_started/using_file_store/#orphaned-metadata","title":"Orphaned Metadata","text":"<p>In the course of working with <code>FileStore</code> you may encounter a situation where there are metadata records stored in the JSON file that no longer match files on disk. This can happen if, for example, you init a <code>FileStore</code> and later delete a file, or if you init the store with the default arguments but later restrict the file selection with <code>max_depth</code> or <code>file_filters</code>.</p> <p>These orphaned metadata records will appear in the <code>FileStore</code> with the field <code>{\"orphan\": True}</code>. The goal with this behavior is to preserve all metadata the user may have added and prevent data loss.</p> <p>By default, orphaned metadata is excluded from query results. There is an <code>include_orphans</code> keyword argument you can set on init if you want orphaned metadata to be returned in queries.</p>"},{"location":"getting_started/using_file_store/#deleting-files","title":"Deleting files","text":"<p>For consistency with the <code>Store</code> interface, <code>FileStore</code> provides the <code>remove_docs</code> method whenever <code>read_only=False</code>. This method will delete files on disk, because <code>FileStore</code> documents are simply representations of those files. It has an additional guard argument <code>confirm</code> which must be set to the non-default value <code>True</code> for the method to actually do anything.</p> <pre><code>&gt;&gt;&gt; fs.remove_docs({\"name\":\"input.in\"})\nTraceback (most recent call last):\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\n  File \".../maggma/src/maggma/stores/file_store.py\", line 496, in remove_docs\n    raise StoreError(\nmaggma.core.store.StoreError: (StoreError(...), 'Warning! This command is about '\n    'to delete 2 items from disk! If this is what you want, reissue '\n    'this command with confirm=True.')\n</code></pre>"},{"location":"getting_started/using_file_store/#processing-files-with-a-builder","title":"Processing files with a <code>Builder</code>","text":"<p>Now that you can access your files on disk via a <code>FileStore</code>, it's time to write a <code>Builder</code> to read and process the data (see Writing a Builder). Keep in mind that <code>get_items</code> will return documents like the one shown in (#creating-the-filestore). You can then use <code>process_items</code> to</p> <ul> <li>Create structured data from the <code>contents</code></li> <li>Open the file for reading using a custom piece of code</li> <li>etc.</li> </ul> <p>Once you can process data on your disk with a <code>Builder</code>, you can send that data to any kind of <code>Store</code> you like - another <code>FileStore</code>, a database, etc.</p>"},{"location":"getting_started/using_ssh_tunnel/","title":"Using <code>SSHTunnel</code> to connect to remote database","text":"<p>One of the typical scenarios to use <code>maggma</code> is to connect to a remote database that is behind a firewall and thus cannot be accessed directly from your local computer (as shown below, image credits).</p> <p>In this case, you can use <code>SSHTunnel</code> to first connect to the remote server, and then connect to the database from the server.</p> <pre><code>----------------------------------------------------------------------\n\n                            |\n-------------+              |    +----------+               +---------\n    LOCAL    |              |    |  REMOTE  |               | PRIVATE\n  COMPUTER   | &lt;== SSH ========&gt; |  SERVER  | &lt;== local ==&gt; | SERVER\n-------------+              |    +----------+               +---------\n                            |\n                         FIREWALL (only port 22 is open)\n\n----------------------------------------------------------------------\n\nNote, the `local` indicates that the connection to the PRIVATE SERVER can only be made from the REMOTE SERVER.\n</code></pre>"},{"location":"getting_started/using_ssh_tunnel/#example-usage-with-s3store","title":"Example usage with <code>S3Store</code>","text":"<p>Below is an example of how to use <code>SSHTunnel</code> to connect to an AWS <code>S3Store</code> hosted on a private server.</p> <p>Let's assume that, from you local computer, you can ssh to the remote server using the following command with your credentials (e.g. ): <pre><code>ssh &lt;USERNAME&gt;@&lt;REMOTE_SERVER_ADDRESS&gt;\n</code></pre> <p>and then from the remote server, you can access your database using, e.g., the following information: <pre><code>private_server_address: COMPUTE_NODE_1\nprivate_server_port: 9000\n</code></pre></p> <p>You can create an <code>SSHTunnel</code> object as follows:</p> <p><pre><code>from maggma.stores.ssh_tunnel import SSHTunnel\n\ntunnel = SSHTunnel(\n    tunnel_server_address = \"&lt;REMOTE_SERVER_ADDRESS&gt;:22\",\n    username = \"&lt;USERNAME&gt;\",\n    password= \"&lt;USER_CREDENTIAL&gt;\",\n    remote_server_address = \"COMPUTE_NODE_1:9000\",\n    local_port = 9000,\n)\n</code></pre> and then pass it to the <code>S3Store</code> to connect to the database. The arguments of the <code>SSHTunnel</code> are self-explanatory, but <code>local_port</code> needs more explanation. We assume that on the local computer, we want to connect to the localhost address <code>http://127.0.0.1</code>, so we do not need to provide the address, but only the port number (<code>9000</code> in this case.)</p> <p>In essence, <code>SSHTunnel</code> allows the connection to the database at <code>COMPUTE_NODE_1:9000</code> on the private server from the localhost address <code>http://127.0.0.1:9000</code> on the local computer as if the database is hosted on the local computer.</p>"},{"location":"getting_started/using_ssh_tunnel/#other-use-cases","title":"Other use cases","text":"<p>Alternative to using <code>username</code> and <code>password</code> for authentication with the remote server, <code>SSHTunnel</code> also supports authentication using SSH keys. In this case, you will need to provide your SSH credentials using the <code>private_key</code> argument. Read the docs of the <code>SSHTunnel</code> for more information.</p> <p><code>SSHTunnel</code> can also be used with other stores such as <code>MongoStore</code>, <code>MongoURIStore</code>, and <code>GridFSStore</code>. The usage is similar to the example above, but you might need to adjust the arguments to the <code>SSHTunnel</code> to match the use case.</p>"},{"location":"reference/builders/","title":"Builders","text":"<p>One-to-One Map Builder and a simple CopyBuilder implementation.</p> <p>Many-to-Many GroupBuilder.</p>"},{"location":"reference/builders/#maggma.builders.map_builder.CopyBuilder","title":"<code>CopyBuilder</code>","text":"<p>               Bases: <code>MapBuilder</code></p> <p>Sync a source store with a target store.</p> Source code in <code>src/maggma/builders/map_builder.py</code> <pre><code>class CopyBuilder(MapBuilder):\n    \"\"\"Sync a source store with a target store.\"\"\"\n\n    def unary_function(self, item):\n        \"\"\"\n        Identity function for copy builder map operation.\n        \"\"\"\n        if \"_id\" in item:\n            del item[\"_id\"]\n        return item\n</code></pre>"},{"location":"reference/builders/#maggma.builders.map_builder.CopyBuilder.unary_function","title":"<code>unary_function(item)</code>","text":"<p>Identity function for copy builder map operation.</p> Source code in <code>src/maggma/builders/map_builder.py</code> <pre><code>def unary_function(self, item):\n    \"\"\"\n    Identity function for copy builder map operation.\n    \"\"\"\n    if \"_id\" in item:\n        del item[\"_id\"]\n    return item\n</code></pre>"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder","title":"<code>MapBuilder</code>","text":"<p>               Bases: <code>Builder</code></p> <p>Apply a unary function to yield a target document for each source document.</p> <p>Supports incremental building, where a source document gets built only if it has newer (by last_updated_field) data than the corresponding (by key) target document.</p> Source code in <code>src/maggma/builders/map_builder.py</code> <pre><code>class MapBuilder(Builder, metaclass=ABCMeta):\n    \"\"\"\n    Apply a unary function to yield a target document for each source document.\n\n    Supports incremental building, where a source document gets built only if it\n    has newer (by last_updated_field) data than the corresponding (by key) target\n    document.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        source: Store,\n        target: Store,\n        query: Optional[dict] = None,\n        projection: Optional[list] = None,\n        delete_orphans: bool = False,\n        timeout: int = 0,\n        store_process_time: bool = True,\n        retry_failed: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Apply a unary function to each source document.\n\n        Args:\n            source: source store\n            target: target store\n            query: optional query to filter source store\n            projection: list of keys to project from the source for\n                processing. Limits data transfer to improve efficiency.\n            delete_orphans: Whether to delete documents on target store\n                with key values not present in source store. Deletion happens\n                after all updates, during Builder.finalize.\n            timeout: maximum running time per item in seconds\n            store_process_time: If True, add \"_process_time\" key to\n                document for profiling purposes\n            retry_failed: If True, will retry building documents that\n                previously failed\n        \"\"\"\n        self.source = source\n        self.target = target\n        self.query = query\n        self.projection = projection\n        self.delete_orphans = delete_orphans\n        self.kwargs = kwargs\n        self.timeout = timeout\n        self.store_process_time = store_process_time\n        self.retry_failed = retry_failed\n        super().__init__(sources=[source], targets=[target], **kwargs)\n\n    def ensure_indexes(self):\n        \"\"\"\n        Ensures indices on critical fields for MapBuilder.\n        \"\"\"\n        index_checks = [\n            self.source.ensure_index(self.source.key),\n            self.source.ensure_index(self.source.last_updated_field),\n            self.target.ensure_index(self.target.key),\n            self.target.ensure_index(self.target.last_updated_field),\n            self.target.ensure_index(\"state\"),\n        ]\n\n        if not all(index_checks):\n            self.logger.warning(\n                \"Missing one or more important indices on stores. \"\n                \"Performance for large stores may be severely degraded. \"\n                \"Ensure indices on target.key and \"\n                \"[(store.last_updated_field, -1), (store.key, 1)] \"\n                \"for each of source and target.\"\n            )\n\n    def prechunk(self, number_splits: int) -&gt; Iterator[dict]:\n        \"\"\"\n        Generic prechunk for map builder to perform domain-decomposition\n        by the key field.\n        \"\"\"\n        self.ensure_indexes()\n        keys = self.target.newer_in(self.source, criteria=self.query, exhaustive=True)\n\n        N = ceil(len(keys) / number_splits)\n        for split in grouper(keys, N):\n            yield {\"query\": {self.source.key: {\"$in\": list(split)}}}\n\n    def get_items(self):\n        \"\"\"\n        Generic get items for Map Builder designed to perform\n        incremental building.\n        \"\"\"\n        self.logger.info(f\"Starting {self.__class__.__name__} Builder\")\n\n        self.ensure_indexes()\n\n        keys = self.target.newer_in(self.source, criteria=self.query, exhaustive=True)\n        if self.retry_failed:\n            if isinstance(self.query, (dict)):\n                failed_query = {\"$and\": [self.query, {\"state\": \"failed\"}]}\n            else:\n                failed_query = {\"state\": \"failed\"}\n            failed_keys = self.target.distinct(self.target.key, criteria=failed_query)\n            keys = list(set(keys + failed_keys))\n\n        self.logger.info(f\"Processing {len(keys)} items\")\n\n        if self.projection:\n            projection = list({*self.projection, self.source.key, self.source.last_updated_field})\n        else:\n            projection = None\n\n        self.total = len(keys)\n        for chunked_keys in grouper(keys, self.chunk_size):\n            chunked_keys = list(chunked_keys)\n            yield from list(\n                self.source.query(\n                    criteria={self.source.key: {\"$in\": chunked_keys}},\n                    properties=projection,\n                )\n            )\n\n    def process_item(self, item: dict):\n        \"\"\"\n        Generic process items to process a dictionary using\n        a map function.\n        \"\"\"\n        self.logger.debug(f\"Processing: {item[self.source.key]}\")\n\n        time_start = time()\n\n        try:\n            with Timeout(seconds=self.timeout):\n                processed = dict(self.unary_function(item))\n                processed.update({\"state\": \"successful\"})\n\n            for k in [self.source.key, self.source.last_updated_field]:\n                if k in processed:\n                    del processed[k]\n\n        except Exception as e:\n            self.logger.error(traceback.format_exc())\n            processed = {\"error\": str(e), \"state\": \"failed\"}\n\n        time_end = time()\n\n        key, last_updated_field = self.source.key, self.source.last_updated_field\n\n        out = {\n            self.target.key: item[key],\n            self.target.last_updated_field: self.source._lu_func[0](item.get(last_updated_field, datetime.utcnow())),\n        }\n\n        if self.store_process_time:\n            out[\"_process_time\"] = time_end - time_start\n\n        out.update(processed)\n        return out\n\n    def update_targets(self, items: list[dict]):\n        \"\"\"\n        Generic update targets for Map Builder.\n        \"\"\"\n        target = self.target\n        for item in items:\n            item[\"_bt\"] = datetime.utcnow()\n            if \"_id\" in item:\n                del item[\"_id\"]\n\n        if len(items) &gt; 0:\n            target.update(items)\n\n    def finalize(self):\n        \"\"\"\n        Finalize MapBuilder operations including removing orphaned documents.\n        \"\"\"\n        if self.delete_orphans:\n            source_keyvals = set(self.source.distinct(self.source.key))\n            target_keyvals = set(self.target.distinct(self.target.key))\n            to_delete = list(target_keyvals - source_keyvals)\n            if len(to_delete):\n                self.logger.info(f\"Finalize: Deleting {len(to_delete)} orphans.\")\n            self.target.remove_docs({self.target.key: {\"$in\": to_delete}})\n        super().finalize()\n\n    @abstractmethod\n    def unary_function(self, item):\n        \"\"\"\n        ufn: Unary function to process item\n                You do not need to provide values for\n                source.key and source.last_updated_field in the output.\n                Any uncaught exceptions will be caught by\n                process_item and logged to the \"error\" field\n                in the target document.\n        \"\"\"\n</code></pre>"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.__init__","title":"<code>__init__(source, target, query=None, projection=None, delete_orphans=False, timeout=0, store_process_time=True, retry_failed=False, **kwargs)</code>","text":"<p>Apply a unary function to each source document.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>Store</code> <p>source store</p> required <code>target</code> <code>Store</code> <p>target store</p> required <code>query</code> <code>Optional[dict]</code> <p>optional query to filter source store</p> <code>None</code> <code>projection</code> <code>Optional[list]</code> <p>list of keys to project from the source for processing. Limits data transfer to improve efficiency.</p> <code>None</code> <code>delete_orphans</code> <code>bool</code> <p>Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize.</p> <code>False</code> <code>timeout</code> <code>int</code> <p>maximum running time per item in seconds</p> <code>0</code> <code>store_process_time</code> <code>bool</code> <p>If True, add \"_process_time\" key to document for profiling purposes</p> <code>True</code> <code>retry_failed</code> <code>bool</code> <p>If True, will retry building documents that previously failed</p> <code>False</code> Source code in <code>src/maggma/builders/map_builder.py</code> <pre><code>def __init__(\n    self,\n    source: Store,\n    target: Store,\n    query: Optional[dict] = None,\n    projection: Optional[list] = None,\n    delete_orphans: bool = False,\n    timeout: int = 0,\n    store_process_time: bool = True,\n    retry_failed: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Apply a unary function to each source document.\n\n    Args:\n        source: source store\n        target: target store\n        query: optional query to filter source store\n        projection: list of keys to project from the source for\n            processing. Limits data transfer to improve efficiency.\n        delete_orphans: Whether to delete documents on target store\n            with key values not present in source store. Deletion happens\n            after all updates, during Builder.finalize.\n        timeout: maximum running time per item in seconds\n        store_process_time: If True, add \"_process_time\" key to\n            document for profiling purposes\n        retry_failed: If True, will retry building documents that\n            previously failed\n    \"\"\"\n    self.source = source\n    self.target = target\n    self.query = query\n    self.projection = projection\n    self.delete_orphans = delete_orphans\n    self.kwargs = kwargs\n    self.timeout = timeout\n    self.store_process_time = store_process_time\n    self.retry_failed = retry_failed\n    super().__init__(sources=[source], targets=[target], **kwargs)\n</code></pre>"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.ensure_indexes","title":"<code>ensure_indexes()</code>","text":"<p>Ensures indices on critical fields for MapBuilder.</p> Source code in <code>src/maggma/builders/map_builder.py</code> <pre><code>def ensure_indexes(self):\n    \"\"\"\n    Ensures indices on critical fields for MapBuilder.\n    \"\"\"\n    index_checks = [\n        self.source.ensure_index(self.source.key),\n        self.source.ensure_index(self.source.last_updated_field),\n        self.target.ensure_index(self.target.key),\n        self.target.ensure_index(self.target.last_updated_field),\n        self.target.ensure_index(\"state\"),\n    ]\n\n    if not all(index_checks):\n        self.logger.warning(\n            \"Missing one or more important indices on stores. \"\n            \"Performance for large stores may be severely degraded. \"\n            \"Ensure indices on target.key and \"\n            \"[(store.last_updated_field, -1), (store.key, 1)] \"\n            \"for each of source and target.\"\n        )\n</code></pre>"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.finalize","title":"<code>finalize()</code>","text":"<p>Finalize MapBuilder operations including removing orphaned documents.</p> Source code in <code>src/maggma/builders/map_builder.py</code> <pre><code>def finalize(self):\n    \"\"\"\n    Finalize MapBuilder operations including removing orphaned documents.\n    \"\"\"\n    if self.delete_orphans:\n        source_keyvals = set(self.source.distinct(self.source.key))\n        target_keyvals = set(self.target.distinct(self.target.key))\n        to_delete = list(target_keyvals - source_keyvals)\n        if len(to_delete):\n            self.logger.info(f\"Finalize: Deleting {len(to_delete)} orphans.\")\n        self.target.remove_docs({self.target.key: {\"$in\": to_delete}})\n    super().finalize()\n</code></pre>"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.get_items","title":"<code>get_items()</code>","text":"<p>Generic get items for Map Builder designed to perform incremental building.</p> Source code in <code>src/maggma/builders/map_builder.py</code> <pre><code>def get_items(self):\n    \"\"\"\n    Generic get items for Map Builder designed to perform\n    incremental building.\n    \"\"\"\n    self.logger.info(f\"Starting {self.__class__.__name__} Builder\")\n\n    self.ensure_indexes()\n\n    keys = self.target.newer_in(self.source, criteria=self.query, exhaustive=True)\n    if self.retry_failed:\n        if isinstance(self.query, (dict)):\n            failed_query = {\"$and\": [self.query, {\"state\": \"failed\"}]}\n        else:\n            failed_query = {\"state\": \"failed\"}\n        failed_keys = self.target.distinct(self.target.key, criteria=failed_query)\n        keys = list(set(keys + failed_keys))\n\n    self.logger.info(f\"Processing {len(keys)} items\")\n\n    if self.projection:\n        projection = list({*self.projection, self.source.key, self.source.last_updated_field})\n    else:\n        projection = None\n\n    self.total = len(keys)\n    for chunked_keys in grouper(keys, self.chunk_size):\n        chunked_keys = list(chunked_keys)\n        yield from list(\n            self.source.query(\n                criteria={self.source.key: {\"$in\": chunked_keys}},\n                properties=projection,\n            )\n        )\n</code></pre>"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.prechunk","title":"<code>prechunk(number_splits)</code>","text":"<p>Generic prechunk for map builder to perform domain-decomposition by the key field.</p> Source code in <code>src/maggma/builders/map_builder.py</code> <pre><code>def prechunk(self, number_splits: int) -&gt; Iterator[dict]:\n    \"\"\"\n    Generic prechunk for map builder to perform domain-decomposition\n    by the key field.\n    \"\"\"\n    self.ensure_indexes()\n    keys = self.target.newer_in(self.source, criteria=self.query, exhaustive=True)\n\n    N = ceil(len(keys) / number_splits)\n    for split in grouper(keys, N):\n        yield {\"query\": {self.source.key: {\"$in\": list(split)}}}\n</code></pre>"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.process_item","title":"<code>process_item(item)</code>","text":"<p>Generic process items to process a dictionary using a map function.</p> Source code in <code>src/maggma/builders/map_builder.py</code> <pre><code>def process_item(self, item: dict):\n    \"\"\"\n    Generic process items to process a dictionary using\n    a map function.\n    \"\"\"\n    self.logger.debug(f\"Processing: {item[self.source.key]}\")\n\n    time_start = time()\n\n    try:\n        with Timeout(seconds=self.timeout):\n            processed = dict(self.unary_function(item))\n            processed.update({\"state\": \"successful\"})\n\n        for k in [self.source.key, self.source.last_updated_field]:\n            if k in processed:\n                del processed[k]\n\n    except Exception as e:\n        self.logger.error(traceback.format_exc())\n        processed = {\"error\": str(e), \"state\": \"failed\"}\n\n    time_end = time()\n\n    key, last_updated_field = self.source.key, self.source.last_updated_field\n\n    out = {\n        self.target.key: item[key],\n        self.target.last_updated_field: self.source._lu_func[0](item.get(last_updated_field, datetime.utcnow())),\n    }\n\n    if self.store_process_time:\n        out[\"_process_time\"] = time_end - time_start\n\n    out.update(processed)\n    return out\n</code></pre>"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.unary_function","title":"<code>unary_function(item)</code>  <code>abstractmethod</code>","text":"Unary function to process item <p>You do not need to provide values for source.key and source.last_updated_field in the output. Any uncaught exceptions will be caught by process_item and logged to the \"error\" field in the target document.</p> Source code in <code>src/maggma/builders/map_builder.py</code> <pre><code>@abstractmethod\ndef unary_function(self, item):\n    \"\"\"\n    ufn: Unary function to process item\n            You do not need to provide values for\n            source.key and source.last_updated_field in the output.\n            Any uncaught exceptions will be caught by\n            process_item and logged to the \"error\" field\n            in the target document.\n    \"\"\"\n</code></pre>"},{"location":"reference/builders/#maggma.builders.map_builder.MapBuilder.update_targets","title":"<code>update_targets(items)</code>","text":"<p>Generic update targets for Map Builder.</p> Source code in <code>src/maggma/builders/map_builder.py</code> <pre><code>def update_targets(self, items: list[dict]):\n    \"\"\"\n    Generic update targets for Map Builder.\n    \"\"\"\n    target = self.target\n    for item in items:\n        item[\"_bt\"] = datetime.utcnow()\n        if \"_id\" in item:\n            del item[\"_id\"]\n\n    if len(items) &gt; 0:\n        target.update(items)\n</code></pre>"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder","title":"<code>GroupBuilder</code>","text":"<p>               Bases: <code>Builder</code></p> <p>Group source docs and produces merged documents for each group Supports incremental building, where a source group gets (re)built only if it has a newer (by last_updated_field) doc than the corresponding (by key) target doc.</p> <p>This is a Many-to-One or Many-to-Many Builder. As a result, this builder can't determine when a source document is orphaned.</p> Source code in <code>src/maggma/builders/group_builder.py</code> <pre><code>class GroupBuilder(Builder, metaclass=ABCMeta):\n    \"\"\"\n    Group source docs and produces merged documents for each group\n    Supports incremental building, where a source group gets (re)built only if\n    it has a newer (by last_updated_field) doc than the corresponding (by key) target doc.\n\n    This is a Many-to-One or Many-to-Many Builder. As a result, this builder can't determine when a source document\n    is orphaned.\n    \"\"\"\n\n    def __init__(\n        self,\n        source: Store,\n        target: Store,\n        grouping_keys: list[str],\n        query: Optional[dict] = None,\n        projection: Optional[list] = None,\n        timeout: int = 0,\n        store_process_time: bool = True,\n        retry_failed: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            source: source store\n            target: target store\n            query: optional query to filter items from the source store.\n            projection: list of keys to project from the source for\n                processing. Limits data transfer to improve efficiency.\n            delete_orphans: Whether to delete documents on target store\n                with key values not present in source store. Deletion happens\n                after all updates, during Builder.finalize.\n            timeout: maximum running time per item in seconds\n            store_process_time: If True, add \"_process_time\" key to\n                document for profiling purposes\n            retry_failed: If True, will retry building documents that\n                previously failed.\n        \"\"\"\n        self.source = source\n        self.target = target\n        self.grouping_keys = grouping_keys\n        self.query = query if query else {}\n        self.projection = projection\n        self.kwargs = kwargs\n        self.timeout = timeout\n        self.store_process_time = store_process_time\n        self.retry_failed = retry_failed\n\n        self._target_keys_field = f\"{self.source.key}s\"\n\n        super().__init__(sources=[source], targets=[target], **kwargs)\n\n    def ensure_indexes(self):\n        \"\"\"\n        Ensures indices on critical fields for GroupBuilder\n        which include the plural version of the target's key field.\n        \"\"\"\n        index_checks = [\n            self.source.ensure_index(self.source.key),\n            self.source.ensure_index(self.source.last_updated_field),\n            self.target.ensure_index(self.target.key),\n            self.target.ensure_index(self.target.last_updated_field),\n            self.target.ensure_index(\"state\"),\n            self.target.ensure_index(self._target_keys_field),\n        ]\n\n        if not all(index_checks):\n            self.logger.warning(\n                \"Missing one or more important indices on stores. \"\n                \"Performance for large stores may be severely degraded. \"\n                \"Ensure indices on target.key and \"\n                \"[(store.last_updated_field, -1), (store.key, 1)] \"\n                \"for each of source and target.\"\n            )\n\n    def prechunk(self, number_splits: int) -&gt; Iterator[dict]:\n        \"\"\"\n        Generic prechunk for group builder to perform domain-decomposition\n        by the grouping keys.\n        \"\"\"\n        self.ensure_indexes()\n\n        keys = self.get_ids_to_process()\n        groups = self.get_groups_from_keys(keys)\n\n        N = ceil(len(groups) / number_splits)\n        for split in grouper(keys, N):\n            yield {\"query\": dict(zip(self.grouping_keys, split))}\n\n    def get_items(self):\n        self.logger.info(f\"Starting {self.__class__.__name__} Builder\")\n\n        self.ensure_indexes()\n        keys = self.get_ids_to_process()\n        groups = self.get_groups_from_keys(keys)\n\n        if self.projection:\n            projection = list({*self.projection, self.source.key, self.source.last_updated_field})\n        else:\n            projection = None\n\n        self.total = len(groups)\n        for group in groups:\n            group_criteria = dict(zip(self.grouping_keys, group))\n            group_criteria.update(self.query)\n            yield list(self.source.query(criteria=group_criteria, properties=projection))\n\n    def process_item(self, item: list[dict]) -&gt; dict[tuple, dict]:  # type: ignore\n        keys = [d[self.source.key] for d in item]\n\n        self.logger.debug(f\"Processing: {keys}\")\n\n        time_start = time()\n\n        try:\n            with Timeout(seconds=self.timeout):\n                processed = self.unary_function(item)\n                processed.update({\"state\": \"successful\"})\n        except Exception as e:\n            self.logger.error(traceback.format_exc())\n            processed = {\"error\": str(e), \"state\": \"failed\"}\n\n        time_end = time()\n\n        last_updated = [self.source._lu_func[0](d[self.source.last_updated_field]) for d in item]\n\n        update_doc = {\n            self.target.key: keys[0],\n            f\"{self.source.key}s\": keys,\n            self.target.last_updated_field: max(last_updated),\n            \"_bt\": datetime.utcnow(),\n        }\n        processed.update({k: v for k, v in update_doc.items() if k not in processed})\n\n        if self.store_process_time:\n            processed[\"_process_time\"] = time_end - time_start\n\n        return processed\n\n    def update_targets(self, items: list[dict]):\n        \"\"\"\n        Generic update targets for Group Builder.\n        \"\"\"\n        target = self.target\n        for item in items:\n            if \"_id\" in item:\n                del item[\"_id\"]\n\n        if len(items) &gt; 0:\n            target.update(items)\n\n    @abstractmethod\n    def unary_function(self, items: list[dict]) -&gt; dict:\n        \"\"\"\n        Processing function for GroupBuilder.\n\n        Arguments:\n            items: list of of documents with matching grouping keys\n\n        Returns:\n            Dictionary mapping:\n                tuple of source document keys that are in the grouped document\n                to the grouped and processed document\n        \"\"\"\n\n    def get_ids_to_process(self) -&gt; Iterable:\n        \"\"\"\n        Gets the IDs that need to be processed.\n        \"\"\"\n        distinct_from_target = list(self.target.distinct(self._target_keys_field, criteria=self.query))\n        processed_ids = []\n        # Not always guaranteed that MongoDB will unpack the list so we\n        # have to make sure we do that\n        for d in distinct_from_target:\n            if isinstance(d, list):\n                processed_ids.extend(d)\n            else:\n                processed_ids.append(d)\n\n        all_ids = set(self.source.distinct(self.source.key, criteria=self.query))\n        self.logger.debug(f\"Found {len(all_ids)} total docs in source\")\n\n        if self.retry_failed:\n            failed_keys = self.target.distinct(self._target_keys_field, criteria={\"state\": \"failed\", **self.query})\n            unprocessed_ids = all_ids - (set(processed_ids) - set(failed_keys))\n            self.logger.debug(f\"Found {len(failed_keys)} failed IDs in target\")\n        else:\n            unprocessed_ids = all_ids - set(processed_ids)\n\n        self.logger.info(f\"Found {len(unprocessed_ids)} IDs to process\")\n\n        new_ids = set(self.source.newer_in(self.target, criteria=self.query, exhaustive=False))\n\n        self.logger.info(f\"Found {len(new_ids)} updated IDs to process\")\n        return list(new_ids | unprocessed_ids)\n\n    def get_groups_from_keys(self, keys) -&gt; set[tuple]:\n        \"\"\"\n        Get the groups by grouping_keys for these documents.\n        \"\"\"\n        grouping_keys = self.grouping_keys\n\n        groups: set[tuple] = set()\n\n        for chunked_keys in grouper(keys, self.chunk_size):\n            docs = list(\n                self.source.query(\n                    criteria={self.source.key: {\"$in\": chunked_keys}},\n                    properties=grouping_keys,\n                )\n            )\n\n            sub_groups = {tuple(get(d, prop, None) for prop in grouping_keys) for d in docs}\n            self.logger.debug(f\"Found {len(sub_groups)} subgroups to process\")\n\n            groups |= sub_groups\n\n        self.logger.info(f\"Found {len(groups)} groups to process\")\n        return groups\n</code></pre>"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.__init__","title":"<code>__init__(source, target, grouping_keys, query=None, projection=None, timeout=0, store_process_time=True, retry_failed=False, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>source</code> <code>Store</code> <p>source store</p> required <code>target</code> <code>Store</code> <p>target store</p> required <code>query</code> <code>Optional[dict]</code> <p>optional query to filter items from the source store.</p> <code>None</code> <code>projection</code> <code>Optional[list]</code> <p>list of keys to project from the source for processing. Limits data transfer to improve efficiency.</p> <code>None</code> <code>delete_orphans</code> <p>Whether to delete documents on target store with key values not present in source store. Deletion happens after all updates, during Builder.finalize.</p> required <code>timeout</code> <code>int</code> <p>maximum running time per item in seconds</p> <code>0</code> <code>store_process_time</code> <code>bool</code> <p>If True, add \"_process_time\" key to document for profiling purposes</p> <code>True</code> <code>retry_failed</code> <code>bool</code> <p>If True, will retry building documents that previously failed.</p> <code>False</code> Source code in <code>src/maggma/builders/group_builder.py</code> <pre><code>def __init__(\n    self,\n    source: Store,\n    target: Store,\n    grouping_keys: list[str],\n    query: Optional[dict] = None,\n    projection: Optional[list] = None,\n    timeout: int = 0,\n    store_process_time: bool = True,\n    retry_failed: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        source: source store\n        target: target store\n        query: optional query to filter items from the source store.\n        projection: list of keys to project from the source for\n            processing. Limits data transfer to improve efficiency.\n        delete_orphans: Whether to delete documents on target store\n            with key values not present in source store. Deletion happens\n            after all updates, during Builder.finalize.\n        timeout: maximum running time per item in seconds\n        store_process_time: If True, add \"_process_time\" key to\n            document for profiling purposes\n        retry_failed: If True, will retry building documents that\n            previously failed.\n    \"\"\"\n    self.source = source\n    self.target = target\n    self.grouping_keys = grouping_keys\n    self.query = query if query else {}\n    self.projection = projection\n    self.kwargs = kwargs\n    self.timeout = timeout\n    self.store_process_time = store_process_time\n    self.retry_failed = retry_failed\n\n    self._target_keys_field = f\"{self.source.key}s\"\n\n    super().__init__(sources=[source], targets=[target], **kwargs)\n</code></pre>"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.ensure_indexes","title":"<code>ensure_indexes()</code>","text":"<p>Ensures indices on critical fields for GroupBuilder which include the plural version of the target's key field.</p> Source code in <code>src/maggma/builders/group_builder.py</code> <pre><code>def ensure_indexes(self):\n    \"\"\"\n    Ensures indices on critical fields for GroupBuilder\n    which include the plural version of the target's key field.\n    \"\"\"\n    index_checks = [\n        self.source.ensure_index(self.source.key),\n        self.source.ensure_index(self.source.last_updated_field),\n        self.target.ensure_index(self.target.key),\n        self.target.ensure_index(self.target.last_updated_field),\n        self.target.ensure_index(\"state\"),\n        self.target.ensure_index(self._target_keys_field),\n    ]\n\n    if not all(index_checks):\n        self.logger.warning(\n            \"Missing one or more important indices on stores. \"\n            \"Performance for large stores may be severely degraded. \"\n            \"Ensure indices on target.key and \"\n            \"[(store.last_updated_field, -1), (store.key, 1)] \"\n            \"for each of source and target.\"\n        )\n</code></pre>"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.get_groups_from_keys","title":"<code>get_groups_from_keys(keys)</code>","text":"<p>Get the groups by grouping_keys for these documents.</p> Source code in <code>src/maggma/builders/group_builder.py</code> <pre><code>def get_groups_from_keys(self, keys) -&gt; set[tuple]:\n    \"\"\"\n    Get the groups by grouping_keys for these documents.\n    \"\"\"\n    grouping_keys = self.grouping_keys\n\n    groups: set[tuple] = set()\n\n    for chunked_keys in grouper(keys, self.chunk_size):\n        docs = list(\n            self.source.query(\n                criteria={self.source.key: {\"$in\": chunked_keys}},\n                properties=grouping_keys,\n            )\n        )\n\n        sub_groups = {tuple(get(d, prop, None) for prop in grouping_keys) for d in docs}\n        self.logger.debug(f\"Found {len(sub_groups)} subgroups to process\")\n\n        groups |= sub_groups\n\n    self.logger.info(f\"Found {len(groups)} groups to process\")\n    return groups\n</code></pre>"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.get_ids_to_process","title":"<code>get_ids_to_process()</code>","text":"<p>Gets the IDs that need to be processed.</p> Source code in <code>src/maggma/builders/group_builder.py</code> <pre><code>def get_ids_to_process(self) -&gt; Iterable:\n    \"\"\"\n    Gets the IDs that need to be processed.\n    \"\"\"\n    distinct_from_target = list(self.target.distinct(self._target_keys_field, criteria=self.query))\n    processed_ids = []\n    # Not always guaranteed that MongoDB will unpack the list so we\n    # have to make sure we do that\n    for d in distinct_from_target:\n        if isinstance(d, list):\n            processed_ids.extend(d)\n        else:\n            processed_ids.append(d)\n\n    all_ids = set(self.source.distinct(self.source.key, criteria=self.query))\n    self.logger.debug(f\"Found {len(all_ids)} total docs in source\")\n\n    if self.retry_failed:\n        failed_keys = self.target.distinct(self._target_keys_field, criteria={\"state\": \"failed\", **self.query})\n        unprocessed_ids = all_ids - (set(processed_ids) - set(failed_keys))\n        self.logger.debug(f\"Found {len(failed_keys)} failed IDs in target\")\n    else:\n        unprocessed_ids = all_ids - set(processed_ids)\n\n    self.logger.info(f\"Found {len(unprocessed_ids)} IDs to process\")\n\n    new_ids = set(self.source.newer_in(self.target, criteria=self.query, exhaustive=False))\n\n    self.logger.info(f\"Found {len(new_ids)} updated IDs to process\")\n    return list(new_ids | unprocessed_ids)\n</code></pre>"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.prechunk","title":"<code>prechunk(number_splits)</code>","text":"<p>Generic prechunk for group builder to perform domain-decomposition by the grouping keys.</p> Source code in <code>src/maggma/builders/group_builder.py</code> <pre><code>def prechunk(self, number_splits: int) -&gt; Iterator[dict]:\n    \"\"\"\n    Generic prechunk for group builder to perform domain-decomposition\n    by the grouping keys.\n    \"\"\"\n    self.ensure_indexes()\n\n    keys = self.get_ids_to_process()\n    groups = self.get_groups_from_keys(keys)\n\n    N = ceil(len(groups) / number_splits)\n    for split in grouper(keys, N):\n        yield {\"query\": dict(zip(self.grouping_keys, split))}\n</code></pre>"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.unary_function","title":"<code>unary_function(items)</code>  <code>abstractmethod</code>","text":"<p>Processing function for GroupBuilder.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list[dict]</code> <p>list of of documents with matching grouping keys</p> required <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary mapping: tuple of source document keys that are in the grouped document to the grouped and processed document</p> Source code in <code>src/maggma/builders/group_builder.py</code> <pre><code>@abstractmethod\ndef unary_function(self, items: list[dict]) -&gt; dict:\n    \"\"\"\n    Processing function for GroupBuilder.\n\n    Arguments:\n        items: list of of documents with matching grouping keys\n\n    Returns:\n        Dictionary mapping:\n            tuple of source document keys that are in the grouped document\n            to the grouped and processed document\n    \"\"\"\n</code></pre>"},{"location":"reference/builders/#maggma.builders.group_builder.GroupBuilder.update_targets","title":"<code>update_targets(items)</code>","text":"<p>Generic update targets for Group Builder.</p> Source code in <code>src/maggma/builders/group_builder.py</code> <pre><code>def update_targets(self, items: list[dict]):\n    \"\"\"\n    Generic update targets for Group Builder.\n    \"\"\"\n    target = self.target\n    for item in items:\n        if \"_id\" in item:\n            del item[\"_id\"]\n\n    if len(items) &gt; 0:\n        target.update(items)\n</code></pre>"},{"location":"reference/core_builder/","title":"Builder","text":"<p>Module containing the core builder definition.</p>"},{"location":"reference/core_builder/#maggma.core.builder.Builder","title":"<code>Builder</code>","text":"<p>               Bases: <code>MSONable</code></p> <p>Base Builder class At minimum this class should implement: get_items - Get items from the sources update_targets - Updates the sources with results.</p> <p>Multiprocessing and MPI processing can be used if all the data processing is  limited to process_items</p> Source code in <code>src/maggma/core/builder.py</code> <pre><code>class Builder(MSONable, metaclass=ABCMeta):\n    \"\"\"\n    Base Builder class\n    At minimum this class should implement:\n    get_items - Get items from the sources\n    update_targets - Updates the sources with results.\n\n    Multiprocessing and MPI processing can be used if all\n    the data processing is  limited to process_items\n    \"\"\"\n\n    def __init__(\n        self,\n        sources: Union[list[Store], Store],\n        targets: Union[list[Store], Store],\n        chunk_size: int = 1000,\n    ):\n        \"\"\"\n        Initialize the builder the framework.\n\n        Arguments:\n            sources: source Store(s)\n            targets: target Store(s)\n            chunk_size: chunk size for processing\n        \"\"\"\n        self.sources = sources if isinstance(sources, list) else [sources]\n        self.targets = targets if isinstance(targets, list) else [targets]\n        self.chunk_size = chunk_size\n        self.total = None  # type: Optional[int]\n        self.logger = logging.getLogger(type(self).__name__)\n        self.logger.addHandler(logging.NullHandler())\n\n    def connect(self):\n        \"\"\"\n        Connect to the builder sources and targets.\n        \"\"\"\n        for s in self.sources + self.targets:\n            s.connect()\n\n    def prechunk(self, number_splits: int) -&gt; Iterable[dict]:\n        \"\"\"\n        Part of a domain-decomposition paradigm to allow the builder to operate on\n        multiple nodes by dividing up the IO as well as the compute\n        This function should return an iterator of dictionaries that can be distributed\n        to multiple instances of the builder to get/process/update on.\n\n        Arguments:\n            number_splits: The number of groups to split the documents to work on\n        \"\"\"\n        self.logger.info(\n            f\"{self.__class__.__name__} doesn't have distributed processing capabilities.\"\n            \" Instead this builder will run on just one worker for all processing\"\n        )\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} doesn't have distributed processing capabilities.\"\n            \" Instead this builder will run on just one worker for all processing\"\n        )\n\n    @abstractmethod\n    def get_items(self) -&gt; Iterable:\n        \"\"\"\n        Returns all the items to process.\n\n        Returns:\n            generator or list of items to process\n        \"\"\"\n\n    def process_item(self, item: Any) -&gt; Any:\n        \"\"\"\n        Process an item. There should be no database operations in this method.\n        Default behavior is to return the item.\n\n        Arguments:\n            item:\n\n        Returns:\n           item: an item to update\n        \"\"\"\n        return item\n\n    @abstractmethod\n    def update_targets(self, items: list):\n        \"\"\"\n        Takes a list of items from process item and updates the targets with them.\n        Can also perform other book keeping in the process such as storing gridfs oids, etc.\n\n        Arguments:\n            items:\n\n        Returns:\n\n        \"\"\"\n\n    def finalize(self):\n        \"\"\"\n        Perform any final clean up.\n        \"\"\"\n        # Close any Mongo connections.\n        for store in self.sources + self.targets:\n            try:\n                store.close()\n            except (AttributeError, StoreError):\n                continue\n\n    def run(self, log_level=logging.DEBUG):\n        \"\"\"\n        Run the builder serially\n        This is only intended for diagnostic purposes.\n        \"\"\"\n        # Set up logging\n        root = logging.getLogger()\n        root.setLevel(log_level)\n        ch = TqdmLoggingHandler()\n        formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n        ch.setFormatter(formatter)\n        root.addHandler(ch)\n\n        self.connect()\n\n        cursor = self.get_items()\n\n        for chunk in grouper(tqdm(cursor), self.chunk_size):\n            self.logger.info(f\"Processing batch of {self.chunk_size} items\")\n            processed_chunk = [self.process_item(item) for item in chunk]\n            processed_items = [item for item in processed_chunk if item is not None]\n            self.update_targets(processed_items)\n\n        self.finalize()\n\n    def __getstate__(self):\n        return self.as_dict()\n\n    def __setstate__(self, d):\n        d = {k: v for k, v in d.items() if not k.startswith(\"@\")}\n        d = MontyDecoder().process_decoded(d)\n        self.__init__(**d)\n</code></pre>"},{"location":"reference/core_builder/#maggma.core.builder.Builder.__init__","title":"<code>__init__(sources, targets, chunk_size=1000)</code>","text":"<p>Initialize the builder the framework.</p> <p>Parameters:</p> Name Type Description Default <code>sources</code> <code>Union[list[Store], Store]</code> <p>source Store(s)</p> required <code>targets</code> <code>Union[list[Store], Store]</code> <p>target Store(s)</p> required <code>chunk_size</code> <code>int</code> <p>chunk size for processing</p> <code>1000</code> Source code in <code>src/maggma/core/builder.py</code> <pre><code>def __init__(\n    self,\n    sources: Union[list[Store], Store],\n    targets: Union[list[Store], Store],\n    chunk_size: int = 1000,\n):\n    \"\"\"\n    Initialize the builder the framework.\n\n    Arguments:\n        sources: source Store(s)\n        targets: target Store(s)\n        chunk_size: chunk size for processing\n    \"\"\"\n    self.sources = sources if isinstance(sources, list) else [sources]\n    self.targets = targets if isinstance(targets, list) else [targets]\n    self.chunk_size = chunk_size\n    self.total = None  # type: Optional[int]\n    self.logger = logging.getLogger(type(self).__name__)\n    self.logger.addHandler(logging.NullHandler())\n</code></pre>"},{"location":"reference/core_builder/#maggma.core.builder.Builder.connect","title":"<code>connect()</code>","text":"<p>Connect to the builder sources and targets.</p> Source code in <code>src/maggma/core/builder.py</code> <pre><code>def connect(self):\n    \"\"\"\n    Connect to the builder sources and targets.\n    \"\"\"\n    for s in self.sources + self.targets:\n        s.connect()\n</code></pre>"},{"location":"reference/core_builder/#maggma.core.builder.Builder.finalize","title":"<code>finalize()</code>","text":"<p>Perform any final clean up.</p> Source code in <code>src/maggma/core/builder.py</code> <pre><code>def finalize(self):\n    \"\"\"\n    Perform any final clean up.\n    \"\"\"\n    # Close any Mongo connections.\n    for store in self.sources + self.targets:\n        try:\n            store.close()\n        except (AttributeError, StoreError):\n            continue\n</code></pre>"},{"location":"reference/core_builder/#maggma.core.builder.Builder.get_items","title":"<code>get_items()</code>  <code>abstractmethod</code>","text":"<p>Returns all the items to process.</p> <p>Returns:</p> Type Description <code>Iterable</code> <p>generator or list of items to process</p> Source code in <code>src/maggma/core/builder.py</code> <pre><code>@abstractmethod\ndef get_items(self) -&gt; Iterable:\n    \"\"\"\n    Returns all the items to process.\n\n    Returns:\n        generator or list of items to process\n    \"\"\"\n</code></pre>"},{"location":"reference/core_builder/#maggma.core.builder.Builder.prechunk","title":"<code>prechunk(number_splits)</code>","text":"<p>Part of a domain-decomposition paradigm to allow the builder to operate on multiple nodes by dividing up the IO as well as the compute This function should return an iterator of dictionaries that can be distributed to multiple instances of the builder to get/process/update on.</p> <p>Parameters:</p> Name Type Description Default <code>number_splits</code> <code>int</code> <p>The number of groups to split the documents to work on</p> required Source code in <code>src/maggma/core/builder.py</code> <pre><code>def prechunk(self, number_splits: int) -&gt; Iterable[dict]:\n    \"\"\"\n    Part of a domain-decomposition paradigm to allow the builder to operate on\n    multiple nodes by dividing up the IO as well as the compute\n    This function should return an iterator of dictionaries that can be distributed\n    to multiple instances of the builder to get/process/update on.\n\n    Arguments:\n        number_splits: The number of groups to split the documents to work on\n    \"\"\"\n    self.logger.info(\n        f\"{self.__class__.__name__} doesn't have distributed processing capabilities.\"\n        \" Instead this builder will run on just one worker for all processing\"\n    )\n    raise NotImplementedError(\n        f\"{self.__class__.__name__} doesn't have distributed processing capabilities.\"\n        \" Instead this builder will run on just one worker for all processing\"\n    )\n</code></pre>"},{"location":"reference/core_builder/#maggma.core.builder.Builder.process_item","title":"<code>process_item(item)</code>","text":"<p>Process an item. There should be no database operations in this method. Default behavior is to return the item.</p> <p>Parameters:</p> Name Type Description Default <code>item</code> <code>Any</code> required <p>Returns:</p> Name Type Description <code>item</code> <code>Any</code> <p>an item to update</p> Source code in <code>src/maggma/core/builder.py</code> <pre><code>def process_item(self, item: Any) -&gt; Any:\n    \"\"\"\n    Process an item. There should be no database operations in this method.\n    Default behavior is to return the item.\n\n    Arguments:\n        item:\n\n    Returns:\n       item: an item to update\n    \"\"\"\n    return item\n</code></pre>"},{"location":"reference/core_builder/#maggma.core.builder.Builder.run","title":"<code>run(log_level=logging.DEBUG)</code>","text":"<p>Run the builder serially This is only intended for diagnostic purposes.</p> Source code in <code>src/maggma/core/builder.py</code> <pre><code>def run(self, log_level=logging.DEBUG):\n    \"\"\"\n    Run the builder serially\n    This is only intended for diagnostic purposes.\n    \"\"\"\n    # Set up logging\n    root = logging.getLogger()\n    root.setLevel(log_level)\n    ch = TqdmLoggingHandler()\n    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n    ch.setFormatter(formatter)\n    root.addHandler(ch)\n\n    self.connect()\n\n    cursor = self.get_items()\n\n    for chunk in grouper(tqdm(cursor), self.chunk_size):\n        self.logger.info(f\"Processing batch of {self.chunk_size} items\")\n        processed_chunk = [self.process_item(item) for item in chunk]\n        processed_items = [item for item in processed_chunk if item is not None]\n        self.update_targets(processed_items)\n\n    self.finalize()\n</code></pre>"},{"location":"reference/core_builder/#maggma.core.builder.Builder.update_targets","title":"<code>update_targets(items)</code>  <code>abstractmethod</code>","text":"<p>Takes a list of items from process item and updates the targets with them. Can also perform other book keeping in the process such as storing gridfs oids, etc.</p> <p>Parameters:</p> Name Type Description Default <code>items</code> <code>list</code> required <p>Returns:</p> Source code in <code>src/maggma/core/builder.py</code> <pre><code>@abstractmethod\ndef update_targets(self, items: list):\n    \"\"\"\n    Takes a list of items from process item and updates the targets with them.\n    Can also perform other book keeping in the process such as storing gridfs oids, etc.\n\n    Arguments:\n        items:\n\n    Returns:\n\n    \"\"\"\n</code></pre>"},{"location":"reference/core_store/","title":"Store","text":"<p>Module containing the core Store definition.</p>"},{"location":"reference/core_store/#maggma.core.store.DateTimeFormat","title":"<code>DateTimeFormat</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Datetime format in store document.</p> Source code in <code>src/maggma/core/store.py</code> <pre><code>class DateTimeFormat(Enum):\n    \"\"\"Datetime format in store document.\"\"\"\n\n    DateTime = \"datetime\"\n    IsoFormat = \"isoformat\"\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Sort","title":"<code>Sort</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Enumeration for sorting order.</p> Source code in <code>src/maggma/core/store.py</code> <pre><code>class Sort(Enum):\n    \"\"\"Enumeration for sorting order.\"\"\"\n\n    Ascending = 1\n    Descending = -1\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store","title":"<code>Store</code>","text":"<p>               Bases: <code>MSONable</code></p> <p>Abstract class for a data Store Defines the interface for all data going in and out of a Builder.</p> Source code in <code>src/maggma/core/store.py</code> <pre><code>class Store(MSONable, metaclass=ABCMeta):\n    \"\"\"\n    Abstract class for a data Store\n    Defines the interface for all data going in and out of a Builder.\n    \"\"\"\n\n    def __init__(\n        self,\n        key: str = \"task_id\",\n        last_updated_field: str = \"last_updated\",\n        last_updated_type: DateTimeFormat = DateTimeFormat(\"datetime\"),  # noqa: B008\n        validator: Optional[Validator] = None,\n    ):\n        \"\"\"\n        Args:\n            key: main key to index on\n            last_updated_field: field for date/time stamping the data\n            last_updated_type: the date/time format for the last_updated_field.\n                                Can be \"datetime\" or \"isoformat\"\n            validator: Validator to validate documents going into the store.\n        \"\"\"\n        self.key = key\n        self.last_updated_field = last_updated_field\n        self.last_updated_type = last_updated_type\n        self._lu_func: tuple[Callable, Callable] = (\n            LU_KEY_ISOFORMAT if DateTimeFormat(last_updated_type) == DateTimeFormat.IsoFormat else (identity, identity)\n        )\n        self.validator = validator\n        self.logger = logging.getLogger(type(self).__name__)\n        self.logger.addHandler(logging.NullHandler())\n\n    @abstractproperty\n    def _collection(self):\n        \"\"\"\n        Returns a handle to the pymongo collection object.\n        \"\"\"\n\n    @abstractproperty\n    def name(self) -&gt; str:\n        \"\"\"\n        Return a string representing this data source.\n        \"\"\"\n\n    @abstractmethod\n    def connect(self, force_reset: bool = False):\n        \"\"\"\n        Connect to the source data.\n\n        Args:\n            force_reset: whether to reset the connection or not\n        \"\"\"\n\n    @abstractmethod\n    def close(self):\n        \"\"\"\n        Closes any connections.\n        \"\"\"\n\n    @abstractmethod\n    def count(self, criteria: Optional[dict] = None) -&gt; int:\n        \"\"\"\n        Counts the number of documents matching the query criteria.\n\n        Args:\n            criteria: PyMongo filter for documents to count in\n        \"\"\"\n\n    @abstractmethod\n    def query(\n        self,\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[dict]:\n        \"\"\"\n        Queries the Store for a set of documents.\n\n        Args:\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n        \"\"\"\n\n    @abstractmethod\n    def update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n        \"\"\"\n        Update documents into the Store.\n\n        Args:\n            docs: the document or list of documents to update\n            key: field name(s) to determine uniqueness for a\n                 document, can be a list of multiple fields,\n                 a single field, or None if the Store's key\n                 field is to be used\n        \"\"\"\n\n    @abstractmethod\n    def ensure_index(self, key: str, unique: bool = False) -&gt; bool:\n        \"\"\"\n        Tries to create an index and return true if it succeeded.\n\n        Args:\n            key: single key to index\n            unique: Whether or not this index contains only unique keys\n\n        Returns:\n            bool indicating if the index exists/was created\n        \"\"\"\n\n    @abstractmethod\n    def groupby(\n        self,\n        keys: Union[list[str], str],\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[tuple[dict, list[dict]]]:\n        \"\"\"\n        Simple grouping function that will group documents\n        by keys.\n\n        Args:\n            keys: fields to group documents\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n\n        Returns:\n            generator returning tuples of (dict, list of docs)\n        \"\"\"\n\n    @abstractmethod\n    def remove_docs(self, criteria: dict):\n        \"\"\"\n        Remove docs matching the query dictionary.\n\n        Args:\n            criteria: query dictionary to match\n        \"\"\"\n\n    def query_one(\n        self,\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n    ):\n        \"\"\"\n        Queries the Store for a single document.\n\n        Args:\n            criteria: PyMongo filter for documents to search\n            properties: properties to return in the document\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n        \"\"\"\n        return next(self.query(criteria=criteria, properties=properties, sort=sort), None)\n\n    def distinct(self, field: str, criteria: Optional[dict] = None, all_exist: bool = False) -&gt; list:\n        \"\"\"\n        Get all distinct values for a field.\n\n        Args:\n            field: the field(s) to get distinct values for\n            criteria: PyMongo filter for documents to search in\n        \"\"\"\n        criteria = criteria or {}\n\n        results = [key for key, _ in self.groupby(field, properties=[field], criteria=criteria)]\n        return [get(r, field) for r in results]\n\n    @property\n    def last_updated(self) -&gt; datetime:\n        \"\"\"\n        Provides the most recent last_updated date time stamp from\n        the documents in this Store.\n        \"\"\"\n        doc = next(\n            self.query(\n                properties=[self.last_updated_field],\n                sort={self.last_updated_field: -1},\n                limit=1,\n            ),\n            None,\n        )\n        if doc and not has(doc, self.last_updated_field):\n            raise StoreError(\n                f\"No field '{self.last_updated_field}' in store document. Please ensure Store.last_updated_field \"\n                \"is a datetime field in your store that represents the time of \"\n                \"last update to each document.\"\n            )\n        if not doc or get(doc, self.last_updated_field) is None:\n            # Handle when collection has docs but `NoneType` last_updated_field.\n            return datetime.min\n\n        return self._lu_func[0](get(doc, self.last_updated_field))\n\n    def newer_in(self, target: \"Store\", criteria: Optional[dict] = None, exhaustive: bool = False) -&gt; list[str]:\n        \"\"\"\n        Returns the keys of documents that are newer in the target\n        Store than this Store.\n\n        Args:\n            target: target Store to\n            criteria: PyMongo filter for documents to search in\n            exhaustive: triggers an item-by-item check vs. checking\n                        the last_updated of the target Store and using\n                        that to filter out new items in\n        \"\"\"\n        self.ensure_index(self.key)\n        self.ensure_index(self.last_updated_field)\n\n        if exhaustive:\n            # Get our current last_updated dates for each key value\n            props = {self.key: 1, self.last_updated_field: 1, \"_id\": 0}\n            dates = {\n                d[self.key]: self._lu_func[0](d.get(self.last_updated_field, datetime.max))\n                for d in self.query(properties=props)\n            }\n\n            # Get the last_updated for the store we're comparing with\n            props = {target.key: 1, target.last_updated_field: 1, \"_id\": 0}\n            target_dates = {\n                d[target.key]: target._lu_func[0](d.get(target.last_updated_field, datetime.min))\n                for d in target.query(criteria=criteria, properties=props)\n            }\n\n            new_keys = set(target_dates.keys()) - set(dates.keys())\n            updated_keys = {key for key, date in dates.items() if target_dates.get(key, datetime.min) &gt; date}\n\n            return list(new_keys | updated_keys)\n\n        criteria = {self.last_updated_field: {\"$gt\": self._lu_func[1](self.last_updated)}}\n        return target.distinct(field=self.key, criteria=criteria)\n\n    @deprecated(message=\"Please use Store.newer_in\")\n    def lu_filter(self, targets):\n        \"\"\"Creates a MongoDB filter for new documents.\n\n        By \"new\", we mean documents in this Store that were last updated later\n        than any document in targets.\n\n        Args:\n            targets (list): A list of Stores\n\n        \"\"\"\n        if isinstance(targets, Store):\n            targets = [targets]\n\n        lu_list = [t.last_updated for t in targets]\n        return {self.last_updated_field: {\"$gt\": self._lu_func[1](max(lu_list))}}\n\n    @deprecated(message=\"Use Store.newer_in\")\n    def updated_keys(self, target, criteria=None):\n        \"\"\"\n        Returns keys for docs that are newer in the target store in comparison\n        with this store when comparing the last updated field (last_updated_field).\n\n        Args:\n            target (Store): store to look for updated documents\n            criteria (dict): mongo query to limit scope\n\n        Returns:\n            list of keys that have been updated in target store\n        \"\"\"\n        self.ensure_index(self.key)\n        self.ensure_index(self.last_updated_field)\n\n        return self.newer_in(target, criteria=criteria)\n\n    def __ne__(self, other):\n        return not self == other\n\n    def __getstate__(self):\n        return self.as_dict()\n\n    def __setstate__(self, d):\n        d = {k: v for k, v in d.items() if not k.startswith(\"@\")}\n        d = MontyDecoder().process_decoded(d)\n        self.__init__(**d)\n\n    def __enter__(self):\n        self.connect()\n        return self\n\n    def __exit__(self, exception_type, exception_value, traceback):\n        self.close()\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store.last_updated","title":"<code>last_updated</code>  <code>property</code>","text":"<p>Provides the most recent last_updated date time stamp from the documents in this Store.</p>"},{"location":"reference/core_store/#maggma.core.store.Store.__init__","title":"<code>__init__(key='task_id', last_updated_field='last_updated', last_updated_type=DateTimeFormat('datetime'), validator=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>main key to index on</p> <code>'task_id'</code> <code>last_updated_field</code> <code>str</code> <p>field for date/time stamping the data</p> <code>'last_updated'</code> <code>last_updated_type</code> <code>DateTimeFormat</code> <p>the date/time format for the last_updated_field.                 Can be \"datetime\" or \"isoformat\"</p> <code>DateTimeFormat('datetime')</code> <code>validator</code> <code>Optional[Validator]</code> <p>Validator to validate documents going into the store.</p> <code>None</code> Source code in <code>src/maggma/core/store.py</code> <pre><code>def __init__(\n    self,\n    key: str = \"task_id\",\n    last_updated_field: str = \"last_updated\",\n    last_updated_type: DateTimeFormat = DateTimeFormat(\"datetime\"),  # noqa: B008\n    validator: Optional[Validator] = None,\n):\n    \"\"\"\n    Args:\n        key: main key to index on\n        last_updated_field: field for date/time stamping the data\n        last_updated_type: the date/time format for the last_updated_field.\n                            Can be \"datetime\" or \"isoformat\"\n        validator: Validator to validate documents going into the store.\n    \"\"\"\n    self.key = key\n    self.last_updated_field = last_updated_field\n    self.last_updated_type = last_updated_type\n    self._lu_func: tuple[Callable, Callable] = (\n        LU_KEY_ISOFORMAT if DateTimeFormat(last_updated_type) == DateTimeFormat.IsoFormat else (identity, identity)\n    )\n    self.validator = validator\n    self.logger = logging.getLogger(type(self).__name__)\n    self.logger.addHandler(logging.NullHandler())\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store.close","title":"<code>close()</code>  <code>abstractmethod</code>","text":"<p>Closes any connections.</p> Source code in <code>src/maggma/core/store.py</code> <pre><code>@abstractmethod\ndef close(self):\n    \"\"\"\n    Closes any connections.\n    \"\"\"\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store.connect","title":"<code>connect(force_reset=False)</code>  <code>abstractmethod</code>","text":"<p>Connect to the source data.</p> <p>Parameters:</p> Name Type Description Default <code>force_reset</code> <code>bool</code> <p>whether to reset the connection or not</p> <code>False</code> Source code in <code>src/maggma/core/store.py</code> <pre><code>@abstractmethod\ndef connect(self, force_reset: bool = False):\n    \"\"\"\n    Connect to the source data.\n\n    Args:\n        force_reset: whether to reset the connection or not\n    \"\"\"\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store.count","title":"<code>count(criteria=None)</code>  <code>abstractmethod</code>","text":"<p>Counts the number of documents matching the query criteria.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to count in</p> <code>None</code> Source code in <code>src/maggma/core/store.py</code> <pre><code>@abstractmethod\ndef count(self, criteria: Optional[dict] = None) -&gt; int:\n    \"\"\"\n    Counts the number of documents matching the query criteria.\n\n    Args:\n        criteria: PyMongo filter for documents to count in\n    \"\"\"\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store.distinct","title":"<code>distinct(field, criteria=None, all_exist=False)</code>","text":"<p>Get all distinct values for a field.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>the field(s) to get distinct values for</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> Source code in <code>src/maggma/core/store.py</code> <pre><code>def distinct(self, field: str, criteria: Optional[dict] = None, all_exist: bool = False) -&gt; list:\n    \"\"\"\n    Get all distinct values for a field.\n\n    Args:\n        field: the field(s) to get distinct values for\n        criteria: PyMongo filter for documents to search in\n    \"\"\"\n    criteria = criteria or {}\n\n    results = [key for key, _ in self.groupby(field, properties=[field], criteria=criteria)]\n    return [get(r, field) for r in results]\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store.ensure_index","title":"<code>ensure_index(key, unique=False)</code>  <code>abstractmethod</code>","text":"<p>Tries to create an index and return true if it succeeded.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>single key to index</p> required <code>unique</code> <code>bool</code> <p>Whether or not this index contains only unique keys</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>bool indicating if the index exists/was created</p> Source code in <code>src/maggma/core/store.py</code> <pre><code>@abstractmethod\ndef ensure_index(self, key: str, unique: bool = False) -&gt; bool:\n    \"\"\"\n    Tries to create an index and return true if it succeeded.\n\n    Args:\n        key: single key to index\n        unique: Whether or not this index contains only unique keys\n\n    Returns:\n        bool indicating if the index exists/was created\n    \"\"\"\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store.groupby","title":"<code>groupby(keys, criteria=None, properties=None, sort=None, skip=0, limit=0)</code>  <code>abstractmethod</code>","text":"<p>Simple grouping function that will group documents by keys.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[list[str], str]</code> <p>fields to group documents</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> <p>Returns:</p> Type Description <code>Iterator[tuple[dict, list[dict]]]</code> <p>generator returning tuples of (dict, list of docs)</p> Source code in <code>src/maggma/core/store.py</code> <pre><code>@abstractmethod\ndef groupby(\n    self,\n    keys: Union[list[str], str],\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[tuple[dict, list[dict]]]:\n    \"\"\"\n    Simple grouping function that will group documents\n    by keys.\n\n    Args:\n        keys: fields to group documents\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n\n    Returns:\n        generator returning tuples of (dict, list of docs)\n    \"\"\"\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store.lu_filter","title":"<code>lu_filter(targets)</code>","text":"<p>Creates a MongoDB filter for new documents.</p> <p>By \"new\", we mean documents in this Store that were last updated later than any document in targets.</p> <p>Parameters:</p> Name Type Description Default <code>targets</code> <code>list</code> <p>A list of Stores</p> required Source code in <code>src/maggma/core/store.py</code> <pre><code>@deprecated(message=\"Please use Store.newer_in\")\ndef lu_filter(self, targets):\n    \"\"\"Creates a MongoDB filter for new documents.\n\n    By \"new\", we mean documents in this Store that were last updated later\n    than any document in targets.\n\n    Args:\n        targets (list): A list of Stores\n\n    \"\"\"\n    if isinstance(targets, Store):\n        targets = [targets]\n\n    lu_list = [t.last_updated for t in targets]\n    return {self.last_updated_field: {\"$gt\": self._lu_func[1](max(lu_list))}}\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store.name","title":"<code>name()</code>","text":"<p>Return a string representing this data source.</p> Source code in <code>src/maggma/core/store.py</code> <pre><code>@abstractproperty\ndef name(self) -&gt; str:\n    \"\"\"\n    Return a string representing this data source.\n    \"\"\"\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store.newer_in","title":"<code>newer_in(target, criteria=None, exhaustive=False)</code>","text":"<p>Returns the keys of documents that are newer in the target Store than this Store.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Store</code> <p>target Store to</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>exhaustive</code> <code>bool</code> <p>triggers an item-by-item check vs. checking         the last_updated of the target Store and using         that to filter out new items in</p> <code>False</code> Source code in <code>src/maggma/core/store.py</code> <pre><code>def newer_in(self, target: \"Store\", criteria: Optional[dict] = None, exhaustive: bool = False) -&gt; list[str]:\n    \"\"\"\n    Returns the keys of documents that are newer in the target\n    Store than this Store.\n\n    Args:\n        target: target Store to\n        criteria: PyMongo filter for documents to search in\n        exhaustive: triggers an item-by-item check vs. checking\n                    the last_updated of the target Store and using\n                    that to filter out new items in\n    \"\"\"\n    self.ensure_index(self.key)\n    self.ensure_index(self.last_updated_field)\n\n    if exhaustive:\n        # Get our current last_updated dates for each key value\n        props = {self.key: 1, self.last_updated_field: 1, \"_id\": 0}\n        dates = {\n            d[self.key]: self._lu_func[0](d.get(self.last_updated_field, datetime.max))\n            for d in self.query(properties=props)\n        }\n\n        # Get the last_updated for the store we're comparing with\n        props = {target.key: 1, target.last_updated_field: 1, \"_id\": 0}\n        target_dates = {\n            d[target.key]: target._lu_func[0](d.get(target.last_updated_field, datetime.min))\n            for d in target.query(criteria=criteria, properties=props)\n        }\n\n        new_keys = set(target_dates.keys()) - set(dates.keys())\n        updated_keys = {key for key, date in dates.items() if target_dates.get(key, datetime.min) &gt; date}\n\n        return list(new_keys | updated_keys)\n\n    criteria = {self.last_updated_field: {\"$gt\": self._lu_func[1](self.last_updated)}}\n    return target.distinct(field=self.key, criteria=criteria)\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store.query","title":"<code>query(criteria=None, properties=None, sort=None, skip=0, limit=0)</code>  <code>abstractmethod</code>","text":"<p>Queries the Store for a set of documents.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> Source code in <code>src/maggma/core/store.py</code> <pre><code>@abstractmethod\ndef query(\n    self,\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[dict]:\n    \"\"\"\n    Queries the Store for a set of documents.\n\n    Args:\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n    \"\"\"\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store.query_one","title":"<code>query_one(criteria=None, properties=None, sort=None)</code>","text":"<p>Queries the Store for a single document.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in the document</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> Source code in <code>src/maggma/core/store.py</code> <pre><code>def query_one(\n    self,\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n):\n    \"\"\"\n    Queries the Store for a single document.\n\n    Args:\n        criteria: PyMongo filter for documents to search\n        properties: properties to return in the document\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n    \"\"\"\n    return next(self.query(criteria=criteria, properties=properties, sort=sort), None)\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store.remove_docs","title":"<code>remove_docs(criteria)</code>  <code>abstractmethod</code>","text":"<p>Remove docs matching the query dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>dict</code> <p>query dictionary to match</p> required Source code in <code>src/maggma/core/store.py</code> <pre><code>@abstractmethod\ndef remove_docs(self, criteria: dict):\n    \"\"\"\n    Remove docs matching the query dictionary.\n\n    Args:\n        criteria: query dictionary to match\n    \"\"\"\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store.update","title":"<code>update(docs, key=None)</code>  <code>abstractmethod</code>","text":"<p>Update documents into the Store.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[list[dict], dict]</code> <p>the document or list of documents to update</p> required <code>key</code> <code>Union[list, str, None]</code> <p>field name(s) to determine uniqueness for a  document, can be a list of multiple fields,  a single field, or None if the Store's key  field is to be used</p> <code>None</code> Source code in <code>src/maggma/core/store.py</code> <pre><code>@abstractmethod\ndef update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n    \"\"\"\n    Update documents into the Store.\n\n    Args:\n        docs: the document or list of documents to update\n        key: field name(s) to determine uniqueness for a\n             document, can be a list of multiple fields,\n             a single field, or None if the Store's key\n             field is to be used\n    \"\"\"\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.Store.updated_keys","title":"<code>updated_keys(target, criteria=None)</code>","text":"<p>Returns keys for docs that are newer in the target store in comparison with this store when comparing the last updated field (last_updated_field).</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Store</code> <p>store to look for updated documents</p> required <code>criteria</code> <code>dict</code> <p>mongo query to limit scope</p> <code>None</code> <p>Returns:</p> Type Description <p>list of keys that have been updated in target store</p> Source code in <code>src/maggma/core/store.py</code> <pre><code>@deprecated(message=\"Use Store.newer_in\")\ndef updated_keys(self, target, criteria=None):\n    \"\"\"\n    Returns keys for docs that are newer in the target store in comparison\n    with this store when comparing the last updated field (last_updated_field).\n\n    Args:\n        target (Store): store to look for updated documents\n        criteria (dict): mongo query to limit scope\n\n    Returns:\n        list of keys that have been updated in target store\n    \"\"\"\n    self.ensure_index(self.key)\n    self.ensure_index(self.last_updated_field)\n\n    return self.newer_in(target, criteria=criteria)\n</code></pre>"},{"location":"reference/core_store/#maggma.core.store.StoreError","title":"<code>StoreError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>General Store-related error.</p> Source code in <code>src/maggma/core/store.py</code> <pre><code>class StoreError(Exception):\n    \"\"\"General Store-related error.\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(self, *args, **kwargs)\n</code></pre>"},{"location":"reference/core_validator/","title":"Validator","text":"<p>Validator class for document-level validation on Stores. Attach an instance of a Validator subclass to a Store .schema variable to enable validation on that Store.</p>"},{"location":"reference/core_validator/#maggma.core.validator.Validator","title":"<code>Validator</code>","text":"<p>               Bases: <code>MSONable</code></p> <p>A generic class to perform document-level validation on Stores. Attach a Validator to a Store during initialization, any all documents added to the Store will call .validate_doc() before being added.</p> Source code in <code>src/maggma/core/validator.py</code> <pre><code>class Validator(MSONable, metaclass=ABCMeta):\n    \"\"\"\n    A generic class to perform document-level validation on Stores.\n    Attach a Validator to a Store during initialization, any all documents\n    added to the Store will call .validate_doc() before being added.\n    \"\"\"\n\n    @abstractmethod\n    def is_valid(self, doc: dict) -&gt; bool:\n        \"\"\"\n        Determines if the document is valid.\n\n        Args:\n            doc: document to check\n        \"\"\"\n\n    @abstractmethod\n    def validation_errors(self, doc: dict) -&gt; list[str]:\n        \"\"\"\n        If document is not valid, provides a list of\n        strings to display for why validation has failed.\n\n        Returns empty list if the document is valid\n\n        Args:\n            doc:  document to check\n        \"\"\"\n</code></pre>"},{"location":"reference/core_validator/#maggma.core.validator.Validator.is_valid","title":"<code>is_valid(doc)</code>  <code>abstractmethod</code>","text":"<p>Determines if the document is valid.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>dict</code> <p>document to check</p> required Source code in <code>src/maggma/core/validator.py</code> <pre><code>@abstractmethod\ndef is_valid(self, doc: dict) -&gt; bool:\n    \"\"\"\n    Determines if the document is valid.\n\n    Args:\n        doc: document to check\n    \"\"\"\n</code></pre>"},{"location":"reference/core_validator/#maggma.core.validator.Validator.validation_errors","title":"<code>validation_errors(doc)</code>  <code>abstractmethod</code>","text":"<p>If document is not valid, provides a list of strings to display for why validation has failed.</p> <p>Returns empty list if the document is valid</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>dict</code> <p>document to check</p> required Source code in <code>src/maggma/core/validator.py</code> <pre><code>@abstractmethod\ndef validation_errors(self, doc: dict) -&gt; list[str]:\n    \"\"\"\n    If document is not valid, provides a list of\n    strings to display for why validation has failed.\n\n    Returns empty list if the document is valid\n\n    Args:\n        doc:  document to check\n    \"\"\"\n</code></pre>"},{"location":"reference/stores/","title":"Stores","text":"<p>Note</p> <p>Some <code>Store</code> classes require extra packages that are not installed by default. Run the following modified installation commands if you want to use these stores:</p> <p><code>MongograntStore</code>: <pre><code>pip install maggma[mongogrant]\n</code></pre> <code>MontyStore</code>: <pre><code>pip install maggma[montydb]\n</code></pre> <code>VaultStore</code>: <pre><code>pip install maggma[vault]\n</code></pre></p> <p>Module containing various definitions of Stores. Stores are a default access pattern to data and provide various utilities.</p> <p>Module defining a FileStore that enables accessing files in a local directory using typical maggma access patterns.</p> <p>Module containing various definitions of Stores. Stores are a default access pattern to data and provide various utilities.</p> <p>Stores for connecting to AWS data.</p> <p>Advanced Stores for connecting to Microsoft Azure data.</p> <p>Advanced Stores for behavior outside normal access patterns.</p> <p>Special stores that combine underlying Stores together.</p>"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore","title":"<code>JSONStore</code>","text":"<p>               Bases: <code>MemoryStore</code></p> <p>A Store for access to a single or multiple JSON files.</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>class JSONStore(MemoryStore):\n    \"\"\"\n    A Store for access to a single or multiple JSON files.\n    \"\"\"\n\n    def __init__(\n        self,\n        paths: Union[str, list[str]],\n        read_only: bool = True,\n        serialization_option: Optional[int] = None,\n        serialization_default: Optional[Callable[[Any], Any]] = None,\n        encoding: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            paths: paths for json files to turn into a Store\n            read_only: whether this JSONStore is read only. When read_only=True,\n                       the JSONStore can still apply MongoDB-like writable operations\n                       (e.g. an update) because it behaves like a MemoryStore,\n                       but it will not write those changes to the file. On the other hand,\n                       if read_only=False (i.e., it is writeable), the JSON file\n                       will be automatically updated every time a write-like operation is\n                       performed.\n\n                       Note that when read_only=False, JSONStore only supports a single JSON\n                       file. If the file does not exist, it will be automatically created\n                       when the JSONStore is initialized.\n            serialization_option:\n                option that will be passed to the orjson.dump when saving to the json the file.\n            serialization_default:\n                default that will be passed to the orjson.dump when saving to the json the file.\n            encoding: Character encoding of files to be tracked by the store. The default\n                (None) follows python's default behavior, which is to determine the character\n                encoding from the platform. This should work in the great majority of cases.\n                However, if you encounter a UnicodeDecodeError, consider setting the encoding\n                explicitly to 'utf8' or another encoding as appropriate.\n        \"\"\"\n        paths = paths if isinstance(paths, (list, tuple)) else [paths]\n        self.paths = paths\n        self.encoding = encoding\n\n        # file_writable overrides read_only for compatibility reasons\n        if \"file_writable\" in kwargs:\n            file_writable = kwargs.pop(\"file_writable\")\n            warnings.warn(\n                \"file_writable is deprecated; use read only instead.\",\n                DeprecationWarning,\n            )\n            self.read_only = not file_writable\n            if self.read_only != read_only:\n                warnings.warn(\n                    f\"Received conflicting keyword arguments file_writable={file_writable}\"\n                    f\" and read_only={read_only}. Setting read_only={file_writable}.\",\n                    UserWarning,\n                )\n        else:\n            self.read_only = read_only\n        self.kwargs = kwargs\n\n        if not self.read_only and len(paths) &gt; 1:\n            raise RuntimeError(\"Cannot instantiate file-writable JSONStore with multiple JSON files.\")\n\n        self.default_sort = None\n        self.serialization_option = serialization_option\n        self.serialization_default = serialization_default\n\n        super().__init__(**kwargs)\n\n    def connect(self, force_reset: bool = False):\n        \"\"\"\n        Loads the files into the collection in memory.\n\n        Args:\n            force_reset: whether to reset the connection or not. If False (default) and .connect()\n            has been called previously, the .json file will not be read in again. This can improve performance\n            on systems with slow storage when multiple connect / disconnects are performed.\n        \"\"\"\n        if self._coll is None or force_reset:\n            self._coll = mongomock.MongoClient().db[self.name]  # type: ignore\n\n            # create the .json file if it does not exist\n            if not self.read_only and not Path(self.paths[0]).exists():\n                with zopen(self.paths[0], mode=\"wt\", encoding=self.encoding) as f:\n                    data: list[dict] = []\n                    bytesdata = orjson.dumps(data)\n                    f.write(bytesdata.decode(\"utf-8\"))\n\n            for path in self.paths:\n                objects = self.read_json_file(path)\n                try:\n                    self.update(objects)\n                except KeyError:\n                    raise KeyError(\n                        f\"\"\"\n                        Key field '{self.key}' not found in {path.name}. This\n                        could mean that this JSONStore was initially created with a different key field.\n                        The keys found in the .json file are {list(objects[0].keys())}. Try\n                        re-initializing your JSONStore using one of these as the key arguments.\n                        \"\"\"\n                    )\n\n    def read_json_file(self, path) -&gt; list:\n        \"\"\"\n        Helper method to read the contents of a JSON file and generate\n        a list of docs.\n\n        Args:\n            path: Path to the JSON file to be read\n        \"\"\"\n        with zopen(path, mode=\"rt\", encoding=self.encoding) as f:\n            data = f.read()\n            data = data.decode() if isinstance(data, bytes) else data\n            objects = bson.json_util.loads(data) if \"$oid\" in data else orjson.loads(data)\n            objects = [objects] if not isinstance(objects, list) else objects\n            # datetime objects deserialize to str. Try to convert the last_updated\n            # field back to datetime.\n            # # TODO - there may still be problems caused if a JSONStore is init'ed from\n            # documents that don't contain a last_updated field\n            # See Store.last_updated in store.py.\n            for obj in objects:\n                if obj.get(self.last_updated_field):\n                    obj[self.last_updated_field] = to_dt(obj[self.last_updated_field])\n\n        return objects\n\n    def update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n        \"\"\"\n        Update documents into the Store.\n\n        For a file-writable JSONStore, the json file is updated.\n\n        Args:\n            docs: the document or list of documents to update\n            key: field name(s) to determine uniqueness for a\n                 document, can be a list of multiple fields,\n                 a single field, or None if the Store's key\n                 field is to be used\n        \"\"\"\n        super().update(docs=docs, key=key)\n        if not self.read_only:\n            self.update_json_file()\n\n    def remove_docs(self, criteria: dict):\n        \"\"\"\n        Remove docs matching the query dictionary.\n\n        For a file-writable JSONStore, the json file is updated.\n\n        Args:\n            criteria: query dictionary to match\n        \"\"\"\n        super().remove_docs(criteria=criteria)\n        if not self.read_only:\n            self.update_json_file()\n\n    def update_json_file(self):\n        \"\"\"\n        Updates the json file when a write-like operation is performed.\n        \"\"\"\n        with zopen(self.paths[0], mode=\"wt\", encoding=self.encoding) as f:\n            data = list(self.query())\n            for d in data:\n                d.pop(\"_id\")\n            bytesdata = orjson.dumps(\n                data,\n                option=self.serialization_option,\n                default=self.serialization_default,\n            )\n            f.write(bytesdata.decode(\"utf-8\"))\n\n    def __hash__(self):\n        return hash((*self.paths, self.last_updated_field))\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"\n        Check equality for JSONStore.\n\n        Args:\n            other: other JSONStore to compare with\n        \"\"\"\n        if not isinstance(other, JSONStore):\n            return False\n\n        fields = [\"paths\", \"last_updated_field\"]\n        return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality for JSONStore.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>object</code> <p>other JSONStore to compare with</p> required Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"\n    Check equality for JSONStore.\n\n    Args:\n        other: other JSONStore to compare with\n    \"\"\"\n    if not isinstance(other, JSONStore):\n        return False\n\n    fields = [\"paths\", \"last_updated_field\"]\n    return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.__init__","title":"<code>__init__(paths, read_only=True, serialization_option=None, serialization_default=None, encoding=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>paths</code> <code>Union[str, list[str]]</code> <p>paths for json files to turn into a Store</p> required <code>read_only</code> <code>bool</code> <p>whether this JSONStore is read only. When read_only=True,        the JSONStore can still apply MongoDB-like writable operations        (e.g. an update) because it behaves like a MemoryStore,        but it will not write those changes to the file. On the other hand,        if read_only=False (i.e., it is writeable), the JSON file        will be automatically updated every time a write-like operation is        performed.</p> <pre><code>   Note that when read_only=False, JSONStore only supports a single JSON\n   file. If the file does not exist, it will be automatically created\n   when the JSONStore is initialized.\n</code></pre> <code>True</code> <code>serialization_option</code> <code>Optional[int]</code> <p>option that will be passed to the orjson.dump when saving to the json the file.</p> <code>None</code> <code>serialization_default</code> <code>Optional[Callable[[Any], Any]]</code> <p>default that will be passed to the orjson.dump when saving to the json the file.</p> <code>None</code> <code>encoding</code> <code>Optional[str]</code> <p>Character encoding of files to be tracked by the store. The default (None) follows python's default behavior, which is to determine the character encoding from the platform. This should work in the great majority of cases. However, if you encounter a UnicodeDecodeError, consider setting the encoding explicitly to 'utf8' or another encoding as appropriate.</p> <code>None</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def __init__(\n    self,\n    paths: Union[str, list[str]],\n    read_only: bool = True,\n    serialization_option: Optional[int] = None,\n    serialization_default: Optional[Callable[[Any], Any]] = None,\n    encoding: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        paths: paths for json files to turn into a Store\n        read_only: whether this JSONStore is read only. When read_only=True,\n                   the JSONStore can still apply MongoDB-like writable operations\n                   (e.g. an update) because it behaves like a MemoryStore,\n                   but it will not write those changes to the file. On the other hand,\n                   if read_only=False (i.e., it is writeable), the JSON file\n                   will be automatically updated every time a write-like operation is\n                   performed.\n\n                   Note that when read_only=False, JSONStore only supports a single JSON\n                   file. If the file does not exist, it will be automatically created\n                   when the JSONStore is initialized.\n        serialization_option:\n            option that will be passed to the orjson.dump when saving to the json the file.\n        serialization_default:\n            default that will be passed to the orjson.dump when saving to the json the file.\n        encoding: Character encoding of files to be tracked by the store. The default\n            (None) follows python's default behavior, which is to determine the character\n            encoding from the platform. This should work in the great majority of cases.\n            However, if you encounter a UnicodeDecodeError, consider setting the encoding\n            explicitly to 'utf8' or another encoding as appropriate.\n    \"\"\"\n    paths = paths if isinstance(paths, (list, tuple)) else [paths]\n    self.paths = paths\n    self.encoding = encoding\n\n    # file_writable overrides read_only for compatibility reasons\n    if \"file_writable\" in kwargs:\n        file_writable = kwargs.pop(\"file_writable\")\n        warnings.warn(\n            \"file_writable is deprecated; use read only instead.\",\n            DeprecationWarning,\n        )\n        self.read_only = not file_writable\n        if self.read_only != read_only:\n            warnings.warn(\n                f\"Received conflicting keyword arguments file_writable={file_writable}\"\n                f\" and read_only={read_only}. Setting read_only={file_writable}.\",\n                UserWarning,\n            )\n    else:\n        self.read_only = read_only\n    self.kwargs = kwargs\n\n    if not self.read_only and len(paths) &gt; 1:\n        raise RuntimeError(\"Cannot instantiate file-writable JSONStore with multiple JSON files.\")\n\n    self.default_sort = None\n    self.serialization_option = serialization_option\n    self.serialization_default = serialization_default\n\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.connect","title":"<code>connect(force_reset=False)</code>","text":"<p>Loads the files into the collection in memory.</p> <p>Parameters:</p> Name Type Description Default <code>force_reset</code> <code>bool</code> <p>whether to reset the connection or not. If False (default) and .connect()</p> <code>False</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def connect(self, force_reset: bool = False):\n    \"\"\"\n    Loads the files into the collection in memory.\n\n    Args:\n        force_reset: whether to reset the connection or not. If False (default) and .connect()\n        has been called previously, the .json file will not be read in again. This can improve performance\n        on systems with slow storage when multiple connect / disconnects are performed.\n    \"\"\"\n    if self._coll is None or force_reset:\n        self._coll = mongomock.MongoClient().db[self.name]  # type: ignore\n\n        # create the .json file if it does not exist\n        if not self.read_only and not Path(self.paths[0]).exists():\n            with zopen(self.paths[0], mode=\"wt\", encoding=self.encoding) as f:\n                data: list[dict] = []\n                bytesdata = orjson.dumps(data)\n                f.write(bytesdata.decode(\"utf-8\"))\n\n        for path in self.paths:\n            objects = self.read_json_file(path)\n            try:\n                self.update(objects)\n            except KeyError:\n                raise KeyError(\n                    f\"\"\"\n                    Key field '{self.key}' not found in {path.name}. This\n                    could mean that this JSONStore was initially created with a different key field.\n                    The keys found in the .json file are {list(objects[0].keys())}. Try\n                    re-initializing your JSONStore using one of these as the key arguments.\n                    \"\"\"\n                )\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.read_json_file","title":"<code>read_json_file(path)</code>","text":"<p>Helper method to read the contents of a JSON file and generate a list of docs.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <p>Path to the JSON file to be read</p> required Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def read_json_file(self, path) -&gt; list:\n    \"\"\"\n    Helper method to read the contents of a JSON file and generate\n    a list of docs.\n\n    Args:\n        path: Path to the JSON file to be read\n    \"\"\"\n    with zopen(path, mode=\"rt\", encoding=self.encoding) as f:\n        data = f.read()\n        data = data.decode() if isinstance(data, bytes) else data\n        objects = bson.json_util.loads(data) if \"$oid\" in data else orjson.loads(data)\n        objects = [objects] if not isinstance(objects, list) else objects\n        # datetime objects deserialize to str. Try to convert the last_updated\n        # field back to datetime.\n        # # TODO - there may still be problems caused if a JSONStore is init'ed from\n        # documents that don't contain a last_updated field\n        # See Store.last_updated in store.py.\n        for obj in objects:\n            if obj.get(self.last_updated_field):\n                obj[self.last_updated_field] = to_dt(obj[self.last_updated_field])\n\n    return objects\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.remove_docs","title":"<code>remove_docs(criteria)</code>","text":"<p>Remove docs matching the query dictionary.</p> <p>For a file-writable JSONStore, the json file is updated.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>dict</code> <p>query dictionary to match</p> required Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def remove_docs(self, criteria: dict):\n    \"\"\"\n    Remove docs matching the query dictionary.\n\n    For a file-writable JSONStore, the json file is updated.\n\n    Args:\n        criteria: query dictionary to match\n    \"\"\"\n    super().remove_docs(criteria=criteria)\n    if not self.read_only:\n        self.update_json_file()\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.update","title":"<code>update(docs, key=None)</code>","text":"<p>Update documents into the Store.</p> <p>For a file-writable JSONStore, the json file is updated.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[list[dict], dict]</code> <p>the document or list of documents to update</p> required <code>key</code> <code>Union[list, str, None]</code> <p>field name(s) to determine uniqueness for a  document, can be a list of multiple fields,  a single field, or None if the Store's key  field is to be used</p> <code>None</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n    \"\"\"\n    Update documents into the Store.\n\n    For a file-writable JSONStore, the json file is updated.\n\n    Args:\n        docs: the document or list of documents to update\n        key: field name(s) to determine uniqueness for a\n             document, can be a list of multiple fields,\n             a single field, or None if the Store's key\n             field is to be used\n    \"\"\"\n    super().update(docs=docs, key=key)\n    if not self.read_only:\n        self.update_json_file()\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.JSONStore.update_json_file","title":"<code>update_json_file()</code>","text":"<p>Updates the json file when a write-like operation is performed.</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def update_json_file(self):\n    \"\"\"\n    Updates the json file when a write-like operation is performed.\n    \"\"\"\n    with zopen(self.paths[0], mode=\"wt\", encoding=self.encoding) as f:\n        data = list(self.query())\n        for d in data:\n            d.pop(\"_id\")\n        bytesdata = orjson.dumps(\n            data,\n            option=self.serialization_option,\n            default=self.serialization_default,\n        )\n        f.write(bytesdata.decode(\"utf-8\"))\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore","title":"<code>MemoryStore</code>","text":"<p>               Bases: <code>MongoStore</code></p> <p>An in-memory Store that functions similarly to a MongoStore.</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>class MemoryStore(MongoStore):\n    \"\"\"\n    An in-memory Store that functions similarly\n    to a MongoStore.\n    \"\"\"\n\n    def __init__(self, collection_name: str = \"memory_db\", **kwargs):\n        \"\"\"\n        Initializes the Memory Store.\n\n        Args:\n            collection_name: name for the collection in memory.\n        \"\"\"\n        self.collection_name = collection_name\n        self.default_sort = None\n        self._coll = None\n        self.kwargs = kwargs\n        super(MongoStore, self).__init__(**kwargs)\n\n    def connect(self, force_reset: bool = False):\n        \"\"\"\n        Connect to the source data.\n\n        Args:\n            force_reset: whether to reset the connection or not when the Store is\n                already connected.\n        \"\"\"\n        if self._coll is None or force_reset:\n            self._coll = mongomock.MongoClient().db[self.name]  # type: ignore\n\n    def close(self):\n        \"\"\"Close up all collections.\"\"\"\n        self._coll.database.client.close()\n\n    @property\n    def name(self):\n        \"\"\"Name for the store.\"\"\"\n        return f\"mem://{self.collection_name}\"\n\n    def __hash__(self):\n        \"\"\"Hash for the store.\"\"\"\n        return hash((self.name, self.last_updated_field))\n\n    def groupby(\n        self,\n        keys: Union[list[str], str],\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[tuple[dict, list[dict]]]:\n        \"\"\"\n        Simple grouping function that will group documents\n        by keys.\n\n        Args:\n            keys: fields to group documents\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n\n        Returns:\n            generator returning tuples of (key, list of elements)\n        \"\"\"\n        keys = keys if isinstance(keys, list) else [keys]\n\n        if properties is None:\n            properties = []\n        if isinstance(properties, dict):\n            properties = list(properties.keys())\n\n        data = [\n            doc for doc in self.query(properties=keys + properties, criteria=criteria) if all(has(doc, k) for k in keys)\n        ]\n\n        def grouping_keys(doc):\n            return tuple(get(doc, k) for k in keys)\n\n        for vals, group in groupby(sorted(data, key=grouping_keys), key=grouping_keys):\n            doc = {}  # type: ignore\n            for k, v in zip(keys, vals):\n                set_(doc, k, v)\n            yield doc, list(group)\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"\n        Check equality for MemoryStore\n        other: other MemoryStore to compare with.\n        \"\"\"\n        if not isinstance(other, MemoryStore):\n            return False\n\n        fields = [\"collection_name\", \"last_updated_field\"]\n        return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.name","title":"<code>name</code>  <code>property</code>","text":"<p>Name for the store.</p>"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality for MemoryStore other: other MemoryStore to compare with.</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"\n    Check equality for MemoryStore\n    other: other MemoryStore to compare with.\n    \"\"\"\n    if not isinstance(other, MemoryStore):\n        return False\n\n    fields = [\"collection_name\", \"last_updated_field\"]\n    return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.__hash__","title":"<code>__hash__()</code>","text":"<p>Hash for the store.</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def __hash__(self):\n    \"\"\"Hash for the store.\"\"\"\n    return hash((self.name, self.last_updated_field))\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.__init__","title":"<code>__init__(collection_name='memory_db', **kwargs)</code>","text":"<p>Initializes the Memory Store.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>name for the collection in memory.</p> <code>'memory_db'</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def __init__(self, collection_name: str = \"memory_db\", **kwargs):\n    \"\"\"\n    Initializes the Memory Store.\n\n    Args:\n        collection_name: name for the collection in memory.\n    \"\"\"\n    self.collection_name = collection_name\n    self.default_sort = None\n    self._coll = None\n    self.kwargs = kwargs\n    super(MongoStore, self).__init__(**kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.close","title":"<code>close()</code>","text":"<p>Close up all collections.</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def close(self):\n    \"\"\"Close up all collections.\"\"\"\n    self._coll.database.client.close()\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.connect","title":"<code>connect(force_reset=False)</code>","text":"<p>Connect to the source data.</p> <p>Parameters:</p> Name Type Description Default <code>force_reset</code> <code>bool</code> <p>whether to reset the connection or not when the Store is already connected.</p> <code>False</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def connect(self, force_reset: bool = False):\n    \"\"\"\n    Connect to the source data.\n\n    Args:\n        force_reset: whether to reset the connection or not when the Store is\n            already connected.\n    \"\"\"\n    if self._coll is None or force_reset:\n        self._coll = mongomock.MongoClient().db[self.name]  # type: ignore\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MemoryStore.groupby","title":"<code>groupby(keys, criteria=None, properties=None, sort=None, skip=0, limit=0)</code>","text":"<p>Simple grouping function that will group documents by keys.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[list[str], str]</code> <p>fields to group documents</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> <p>Returns:</p> Type Description <code>Iterator[tuple[dict, list[dict]]]</code> <p>generator returning tuples of (key, list of elements)</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def groupby(\n    self,\n    keys: Union[list[str], str],\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[tuple[dict, list[dict]]]:\n    \"\"\"\n    Simple grouping function that will group documents\n    by keys.\n\n    Args:\n        keys: fields to group documents\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n\n    Returns:\n        generator returning tuples of (key, list of elements)\n    \"\"\"\n    keys = keys if isinstance(keys, list) else [keys]\n\n    if properties is None:\n        properties = []\n    if isinstance(properties, dict):\n        properties = list(properties.keys())\n\n    data = [\n        doc for doc in self.query(properties=keys + properties, criteria=criteria) if all(has(doc, k) for k in keys)\n    ]\n\n    def grouping_keys(doc):\n        return tuple(get(doc, k) for k in keys)\n\n    for vals, group in groupby(sorted(data, key=grouping_keys), key=grouping_keys):\n        doc = {}  # type: ignore\n        for k, v in zip(keys, vals):\n            set_(doc, k, v)\n        yield doc, list(group)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore","title":"<code>MongoStore</code>","text":"<p>               Bases: <code>Store</code></p> <p>A Store that connects to a Mongo collection.</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>class MongoStore(Store):\n    \"\"\"\n    A Store that connects to a Mongo collection.\n    \"\"\"\n\n    def __init__(\n        self,\n        database: str,\n        collection_name: str,\n        host: str = \"localhost\",\n        port: int = 27017,\n        username: str = \"\",\n        password: str = \"\",\n        ssh_tunnel: Optional[SSHTunnel] = None,\n        safe_update: bool = False,\n        auth_source: Optional[str] = None,\n        mongoclient_kwargs: Optional[dict] = None,\n        default_sort: Optional[dict[str, Union[Sort, int]]] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            database: The database name\n            collection_name: The collection name\n            host: Hostname for the database\n            port: TCP port to connect to\n            username: Username for the collection\n            password: Password to connect with\n            safe_update: fail gracefully on DocumentTooLarge errors on update\n            auth_source: The database to authenticate on. Defaults to the database name.\n            default_sort: Default sort field and direction to use when querying. Can be used to\n                ensure determinacy in query results.\n        \"\"\"\n        self.database = database\n        self.collection_name = collection_name\n        self.host = host\n        self.port = port\n        self.username = username\n        self.password = password\n        self.ssh_tunnel = ssh_tunnel\n        self.safe_update = safe_update\n        self.default_sort = default_sort\n        self._coll = None  # type: ignore\n        self.kwargs = kwargs\n\n        if auth_source is None:\n            auth_source = self.database\n        self.auth_source = auth_source\n        self.mongoclient_kwargs = mongoclient_kwargs or {}\n\n        super().__init__(**kwargs)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        Return a string representing this data source.\n        \"\"\"\n        return f\"mongo://{self.host}/{self.database}/{self.collection_name}\"\n\n    def connect(self, force_reset: bool = False):\n        \"\"\"\n        Connect to the source data.\n\n        Args:\n            force_reset: whether to reset the connection or not when the Store is\n                already connected.\n        \"\"\"\n        if self._coll is None or force_reset:\n            if self.ssh_tunnel is None:\n                host = self.host\n                port = self.port\n            else:\n                self.ssh_tunnel.start()\n                host, port = self.ssh_tunnel.local_address\n\n            conn: MongoClient = (\n                MongoClient(\n                    host=host,\n                    port=port,\n                    username=self.username,\n                    password=self.password,\n                    authSource=self.auth_source,\n                    **self.mongoclient_kwargs,\n                )\n                if self.username != \"\"\n                else MongoClient(host, port, **self.mongoclient_kwargs)\n            )\n            db = conn[self.database]\n            self._coll = db[self.collection_name]  # type: ignore\n\n    def __hash__(self) -&gt; int:\n        \"\"\"Hash for MongoStore.\"\"\"\n        return hash((self.database, self.collection_name, self.last_updated_field))\n\n    @classmethod\n    def from_db_file(cls, filename: str, **kwargs):\n        \"\"\"\n        Convenience method to construct MongoStore from db_file\n        from old QueryEngine format.\n        \"\"\"\n        kwargs = loadfn(filename)\n        if \"collection\" in kwargs:\n            kwargs[\"collection_name\"] = kwargs.pop(\"collection\")\n        # Get rid of aliases from traditional query engine db docs\n        kwargs.pop(\"aliases\", None)\n        return cls(**kwargs)\n\n    @classmethod\n    def from_launchpad_file(cls, lp_file, collection_name, **kwargs):\n        \"\"\"\n        Convenience method to construct MongoStore from a launchpad file.\n\n        Note: A launchpad file is a special formatted yaml file used in fireworks\n\n        Returns:\n        \"\"\"\n        with open(lp_file) as f:\n            yaml = YAML(typ=\"safe\", pure=True)\n            lp_creds = yaml.load(f.read())\n\n        db_creds = lp_creds.copy()\n        db_creds[\"database\"] = db_creds[\"name\"]\n        for key in list(db_creds.keys()):\n            if key not in [\"database\", \"host\", \"port\", \"username\", \"password\"]:\n                db_creds.pop(key)\n        db_creds[\"collection_name\"] = collection_name\n\n        return cls(**db_creds, **kwargs)\n\n    def distinct(self, field: str, criteria: Optional[dict] = None, all_exist: bool = False) -&gt; list:\n        \"\"\"\n        Get all distinct values for a field.\n\n        Args:\n            field: the field(s) to get distinct values for\n            criteria: PyMongo filter for documents to search in\n        \"\"\"\n        criteria = criteria or {}\n        try:\n            distinct_vals = self._collection.distinct(field, criteria)\n        except (OperationFailure, DocumentTooLarge):\n            distinct_vals = [\n                d[\"_id\"] for d in self._collection.aggregate([{\"$match\": criteria}, {\"$group\": {\"_id\": f\"${field}\"}}])\n            ]\n            if all(isinstance(d, list) for d in filter(None, distinct_vals)):  # type: ignore\n                distinct_vals = list(chain.from_iterable(filter(None, distinct_vals)))\n\n        return distinct_vals if distinct_vals is not None else []\n\n    def groupby(\n        self,\n        keys: Union[list[str], str],\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[tuple[dict, list[dict]]]:\n        \"\"\"\n        Simple grouping function that will group documents\n        by keys.\n\n        Args:\n            keys: fields to group documents\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n\n        Returns:\n            generator returning tuples of (key, list of docs)\n        \"\"\"\n        pipeline = []\n        if isinstance(keys, str):\n            keys = [keys]\n\n        if properties is None:\n            properties = []\n        if isinstance(properties, dict):\n            properties = list(properties.keys())\n\n        if criteria is not None:\n            pipeline.append({\"$match\": criteria})\n\n        if len(properties) &gt; 0:\n            pipeline.append({\"$project\": {p: 1 for p in properties + keys}})\n\n        alpha = \"abcdefghijklmnopqrstuvwxyz\"\n        group_id = {letter: f\"${key}\" for letter, key in zip(alpha, keys)}\n        pipeline.append({\"$group\": {\"_id\": group_id, \"docs\": {\"$push\": \"$$ROOT\"}}})\n        for d in self._collection.aggregate(pipeline, allowDiskUse=True):\n            id_doc = {}  # type: ignore\n            for letter, key in group_id.items():\n                if has(d[\"_id\"], letter):\n                    set_(id_doc, key[1:], d[\"_id\"][letter])\n            yield (id_doc, d[\"docs\"])\n\n    @classmethod\n    def from_collection(cls, collection):\n        \"\"\"\n        Generates a MongoStore from a pymongo collection object\n        This is not a fully safe operation as it gives dummy information to the MongoStore\n        As a result, this will not serialize and can not reset its connection.\n\n        Args:\n            collection: the PyMongo collection to create a MongoStore around\n        \"\"\"\n        # TODO: How do we make this safer?\n        coll_name = collection.name\n        db_name = collection.database.name\n\n        store = cls(db_name, coll_name)\n        store._coll = collection\n        return store\n\n    @property\n    def _collection(self):\n        \"\"\"Property referring to underlying pymongo collection.\"\"\"\n        if self._coll is None:\n            raise StoreError(\"Must connect Mongo-like store before attempting to use it\")\n        return self._coll\n\n    def count(\n        self,\n        criteria: Optional[dict] = None,\n        hint: Optional[dict[str, Union[Sort, int]]] = None,\n    ) -&gt; int:\n        \"\"\"\n        Counts the number of documents matching the query criteria.\n\n        Args:\n            criteria: PyMongo filter for documents to count in\n            hint: Dictionary of indexes to use as hints for query optimizer.\n                Keys are field names and values are 1 for ascending or -1 for descending.\n        \"\"\"\n        criteria = criteria if criteria else {}\n\n        hint_list = (\n            [(k, Sort(v).value) if isinstance(v, int) else (k, v.value) for k, v in hint.items()] if hint else None\n        )\n\n        if hint_list is not None:  # pragma: no cover\n            return self._collection.count_documents(filter=criteria, hint=hint_list)\n\n        return (\n            self._collection.count_documents(filter=criteria)\n            if criteria\n            else self._collection.estimated_document_count()\n        )\n\n    def query(  # type: ignore\n        self,\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        hint: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n        **kwargs,\n    ) -&gt; Iterator[dict]:\n        \"\"\"\n        Queries the Store for a set of documents.\n\n        Args:\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            hint: Dictionary of indexes to use as hints for query optimizer.\n                Keys are field names and values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n            mongoclient_kwargs: Dict of extra kwargs to pass to pymongo find.\n        \"\"\"\n        if isinstance(properties, list):\n            properties = {p: 1 for p in properties}\n\n        default_sort_formatted = None\n\n        if self.default_sort is not None:\n            default_sort_formatted = [\n                (k, Sort(v).value) if isinstance(v, int) else (k, v.value) for k, v in self.default_sort.items()\n            ]\n\n        sort_list = (\n            [(k, Sort(v).value) if isinstance(v, int) else (k, v.value) for k, v in sort.items()]\n            if sort\n            else default_sort_formatted\n        )\n\n        hint_list = (\n            [(k, Sort(v).value) if isinstance(v, int) else (k, v.value) for k, v in hint.items()] if hint else None\n        )\n\n        yield from self._collection.find(\n            filter=criteria,\n            projection=properties,\n            skip=skip,\n            limit=limit,\n            sort=sort_list,\n            hint=hint_list,\n            **kwargs,\n        )\n\n    def ensure_index(self, key: str, unique: Optional[bool] = False) -&gt; bool:\n        \"\"\"\n        Tries to create an index and return true if it succeeded.\n\n        Args:\n            key: single key to index\n            unique: Whether or not this index contains only unique keys.\n\n        Returns:\n            bool indicating if the index exists/was created\n        \"\"\"\n        if confirm_field_index(self._collection, key):\n            return True\n\n        try:\n            self._collection.create_index(key, unique=unique, background=True)\n            return True\n        except Exception:\n            return False\n\n    def update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n        \"\"\"\n        Update documents into the Store.\n\n        Args:\n            docs: the document or list of documents to update\n            key: field name(s) to determine uniqueness for a\n                 document, can be a list of multiple fields,\n                 a single field, or None if the Store's key\n                 field is to be used\n        \"\"\"\n        requests = []\n\n        if not isinstance(docs, list):\n            docs = [docs]\n\n        for d in (jsanitize(x, allow_bson=True, recursive_msonable=True) for x in docs):\n            # document-level validation is optional\n            validates = True\n            if self.validator:\n                validates = self.validator.is_valid(d)\n                if not validates:\n                    if self.validator.strict:\n                        raise ValueError(self.validator.validation_errors(d))\n                    self.logger.error(self.validator.validation_errors(d))\n\n            if validates:\n                key = key or self.key\n                search_doc = {k: d[k] for k in key} if isinstance(key, list) else {key: d[key]}\n\n                requests.append(ReplaceOne(search_doc, d, upsert=True))\n\n        if len(requests) &gt; 0:\n            try:\n                self._collection.bulk_write(requests, ordered=False)\n            except (OperationFailure, DocumentTooLarge) as e:\n                if self.safe_update:\n                    for req in requests:\n                        try:\n                            self._collection.bulk_write([req], ordered=False)\n                        except (OperationFailure, DocumentTooLarge):\n                            self.logger.error(\n                                f\"Could not upload document for {req._filter} as it was too large for Mongo\"\n                            )\n                else:\n                    raise e\n\n    def remove_docs(self, criteria: dict):\n        \"\"\"\n        Remove docs matching the query dictionary.\n\n        Args:\n            criteria: query dictionary to match\n        \"\"\"\n        self._collection.delete_many(filter=criteria)\n\n    def close(self):\n        \"\"\"Close up all collections.\"\"\"\n        self._collection.database.client.close()\n        self._coll = None\n        if self.ssh_tunnel is not None:\n            self.ssh_tunnel.stop()\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"\n        Check equality for MongoStore\n        other: other mongostore to compare with.\n        \"\"\"\n        if not isinstance(other, MongoStore):\n            return False\n\n        fields = [\"database\", \"collection_name\", \"host\", \"port\", \"last_updated_field\"]\n        return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return a string representing this data source.</p>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality for MongoStore other: other mongostore to compare with.</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"\n    Check equality for MongoStore\n    other: other mongostore to compare with.\n    \"\"\"\n    if not isinstance(other, MongoStore):\n        return False\n\n    fields = [\"database\", \"collection_name\", \"host\", \"port\", \"last_updated_field\"]\n    return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.__hash__","title":"<code>__hash__()</code>","text":"<p>Hash for MongoStore.</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def __hash__(self) -&gt; int:\n    \"\"\"Hash for MongoStore.\"\"\"\n    return hash((self.database, self.collection_name, self.last_updated_field))\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.__init__","title":"<code>__init__(database, collection_name, host='localhost', port=27017, username='', password='', ssh_tunnel=None, safe_update=False, auth_source=None, mongoclient_kwargs=None, default_sort=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>The database name</p> required <code>collection_name</code> <code>str</code> <p>The collection name</p> required <code>host</code> <code>str</code> <p>Hostname for the database</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>TCP port to connect to</p> <code>27017</code> <code>username</code> <code>str</code> <p>Username for the collection</p> <code>''</code> <code>password</code> <code>str</code> <p>Password to connect with</p> <code>''</code> <code>safe_update</code> <code>bool</code> <p>fail gracefully on DocumentTooLarge errors on update</p> <code>False</code> <code>auth_source</code> <code>Optional[str]</code> <p>The database to authenticate on. Defaults to the database name.</p> <code>None</code> <code>default_sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Default sort field and direction to use when querying. Can be used to ensure determinacy in query results.</p> <code>None</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def __init__(\n    self,\n    database: str,\n    collection_name: str,\n    host: str = \"localhost\",\n    port: int = 27017,\n    username: str = \"\",\n    password: str = \"\",\n    ssh_tunnel: Optional[SSHTunnel] = None,\n    safe_update: bool = False,\n    auth_source: Optional[str] = None,\n    mongoclient_kwargs: Optional[dict] = None,\n    default_sort: Optional[dict[str, Union[Sort, int]]] = None,\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        database: The database name\n        collection_name: The collection name\n        host: Hostname for the database\n        port: TCP port to connect to\n        username: Username for the collection\n        password: Password to connect with\n        safe_update: fail gracefully on DocumentTooLarge errors on update\n        auth_source: The database to authenticate on. Defaults to the database name.\n        default_sort: Default sort field and direction to use when querying. Can be used to\n            ensure determinacy in query results.\n    \"\"\"\n    self.database = database\n    self.collection_name = collection_name\n    self.host = host\n    self.port = port\n    self.username = username\n    self.password = password\n    self.ssh_tunnel = ssh_tunnel\n    self.safe_update = safe_update\n    self.default_sort = default_sort\n    self._coll = None  # type: ignore\n    self.kwargs = kwargs\n\n    if auth_source is None:\n        auth_source = self.database\n    self.auth_source = auth_source\n    self.mongoclient_kwargs = mongoclient_kwargs or {}\n\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.close","title":"<code>close()</code>","text":"<p>Close up all collections.</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def close(self):\n    \"\"\"Close up all collections.\"\"\"\n    self._collection.database.client.close()\n    self._coll = None\n    if self.ssh_tunnel is not None:\n        self.ssh_tunnel.stop()\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.connect","title":"<code>connect(force_reset=False)</code>","text":"<p>Connect to the source data.</p> <p>Parameters:</p> Name Type Description Default <code>force_reset</code> <code>bool</code> <p>whether to reset the connection or not when the Store is already connected.</p> <code>False</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def connect(self, force_reset: bool = False):\n    \"\"\"\n    Connect to the source data.\n\n    Args:\n        force_reset: whether to reset the connection or not when the Store is\n            already connected.\n    \"\"\"\n    if self._coll is None or force_reset:\n        if self.ssh_tunnel is None:\n            host = self.host\n            port = self.port\n        else:\n            self.ssh_tunnel.start()\n            host, port = self.ssh_tunnel.local_address\n\n        conn: MongoClient = (\n            MongoClient(\n                host=host,\n                port=port,\n                username=self.username,\n                password=self.password,\n                authSource=self.auth_source,\n                **self.mongoclient_kwargs,\n            )\n            if self.username != \"\"\n            else MongoClient(host, port, **self.mongoclient_kwargs)\n        )\n        db = conn[self.database]\n        self._coll = db[self.collection_name]  # type: ignore\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.count","title":"<code>count(criteria=None, hint=None)</code>","text":"<p>Counts the number of documents matching the query criteria.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to count in</p> <code>None</code> <code>hint</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def count(\n    self,\n    criteria: Optional[dict] = None,\n    hint: Optional[dict[str, Union[Sort, int]]] = None,\n) -&gt; int:\n    \"\"\"\n    Counts the number of documents matching the query criteria.\n\n    Args:\n        criteria: PyMongo filter for documents to count in\n        hint: Dictionary of indexes to use as hints for query optimizer.\n            Keys are field names and values are 1 for ascending or -1 for descending.\n    \"\"\"\n    criteria = criteria if criteria else {}\n\n    hint_list = (\n        [(k, Sort(v).value) if isinstance(v, int) else (k, v.value) for k, v in hint.items()] if hint else None\n    )\n\n    if hint_list is not None:  # pragma: no cover\n        return self._collection.count_documents(filter=criteria, hint=hint_list)\n\n    return (\n        self._collection.count_documents(filter=criteria)\n        if criteria\n        else self._collection.estimated_document_count()\n    )\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.distinct","title":"<code>distinct(field, criteria=None, all_exist=False)</code>","text":"<p>Get all distinct values for a field.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>the field(s) to get distinct values for</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def distinct(self, field: str, criteria: Optional[dict] = None, all_exist: bool = False) -&gt; list:\n    \"\"\"\n    Get all distinct values for a field.\n\n    Args:\n        field: the field(s) to get distinct values for\n        criteria: PyMongo filter for documents to search in\n    \"\"\"\n    criteria = criteria or {}\n    try:\n        distinct_vals = self._collection.distinct(field, criteria)\n    except (OperationFailure, DocumentTooLarge):\n        distinct_vals = [\n            d[\"_id\"] for d in self._collection.aggregate([{\"$match\": criteria}, {\"$group\": {\"_id\": f\"${field}\"}}])\n        ]\n        if all(isinstance(d, list) for d in filter(None, distinct_vals)):  # type: ignore\n            distinct_vals = list(chain.from_iterable(filter(None, distinct_vals)))\n\n    return distinct_vals if distinct_vals is not None else []\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.ensure_index","title":"<code>ensure_index(key, unique=False)</code>","text":"<p>Tries to create an index and return true if it succeeded.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>single key to index</p> required <code>unique</code> <code>Optional[bool]</code> <p>Whether or not this index contains only unique keys.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>bool indicating if the index exists/was created</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def ensure_index(self, key: str, unique: Optional[bool] = False) -&gt; bool:\n    \"\"\"\n    Tries to create an index and return true if it succeeded.\n\n    Args:\n        key: single key to index\n        unique: Whether or not this index contains only unique keys.\n\n    Returns:\n        bool indicating if the index exists/was created\n    \"\"\"\n    if confirm_field_index(self._collection, key):\n        return True\n\n    try:\n        self._collection.create_index(key, unique=unique, background=True)\n        return True\n    except Exception:\n        return False\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.from_collection","title":"<code>from_collection(collection)</code>  <code>classmethod</code>","text":"<p>Generates a MongoStore from a pymongo collection object This is not a fully safe operation as it gives dummy information to the MongoStore As a result, this will not serialize and can not reset its connection.</p> <p>Parameters:</p> Name Type Description Default <code>collection</code> <p>the PyMongo collection to create a MongoStore around</p> required Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>@classmethod\ndef from_collection(cls, collection):\n    \"\"\"\n    Generates a MongoStore from a pymongo collection object\n    This is not a fully safe operation as it gives dummy information to the MongoStore\n    As a result, this will not serialize and can not reset its connection.\n\n    Args:\n        collection: the PyMongo collection to create a MongoStore around\n    \"\"\"\n    # TODO: How do we make this safer?\n    coll_name = collection.name\n    db_name = collection.database.name\n\n    store = cls(db_name, coll_name)\n    store._coll = collection\n    return store\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.from_db_file","title":"<code>from_db_file(filename, **kwargs)</code>  <code>classmethod</code>","text":"<p>Convenience method to construct MongoStore from db_file from old QueryEngine format.</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>@classmethod\ndef from_db_file(cls, filename: str, **kwargs):\n    \"\"\"\n    Convenience method to construct MongoStore from db_file\n    from old QueryEngine format.\n    \"\"\"\n    kwargs = loadfn(filename)\n    if \"collection\" in kwargs:\n        kwargs[\"collection_name\"] = kwargs.pop(\"collection\")\n    # Get rid of aliases from traditional query engine db docs\n    kwargs.pop(\"aliases\", None)\n    return cls(**kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.from_launchpad_file","title":"<code>from_launchpad_file(lp_file, collection_name, **kwargs)</code>  <code>classmethod</code>","text":"<p>Convenience method to construct MongoStore from a launchpad file.</p> <p>Note: A launchpad file is a special formatted yaml file used in fireworks</p> <p>Returns:</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>@classmethod\ndef from_launchpad_file(cls, lp_file, collection_name, **kwargs):\n    \"\"\"\n    Convenience method to construct MongoStore from a launchpad file.\n\n    Note: A launchpad file is a special formatted yaml file used in fireworks\n\n    Returns:\n    \"\"\"\n    with open(lp_file) as f:\n        yaml = YAML(typ=\"safe\", pure=True)\n        lp_creds = yaml.load(f.read())\n\n    db_creds = lp_creds.copy()\n    db_creds[\"database\"] = db_creds[\"name\"]\n    for key in list(db_creds.keys()):\n        if key not in [\"database\", \"host\", \"port\", \"username\", \"password\"]:\n            db_creds.pop(key)\n    db_creds[\"collection_name\"] = collection_name\n\n    return cls(**db_creds, **kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.groupby","title":"<code>groupby(keys, criteria=None, properties=None, sort=None, skip=0, limit=0)</code>","text":"<p>Simple grouping function that will group documents by keys.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[list[str], str]</code> <p>fields to group documents</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> <p>Returns:</p> Type Description <code>Iterator[tuple[dict, list[dict]]]</code> <p>generator returning tuples of (key, list of docs)</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def groupby(\n    self,\n    keys: Union[list[str], str],\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[tuple[dict, list[dict]]]:\n    \"\"\"\n    Simple grouping function that will group documents\n    by keys.\n\n    Args:\n        keys: fields to group documents\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n\n    Returns:\n        generator returning tuples of (key, list of docs)\n    \"\"\"\n    pipeline = []\n    if isinstance(keys, str):\n        keys = [keys]\n\n    if properties is None:\n        properties = []\n    if isinstance(properties, dict):\n        properties = list(properties.keys())\n\n    if criteria is not None:\n        pipeline.append({\"$match\": criteria})\n\n    if len(properties) &gt; 0:\n        pipeline.append({\"$project\": {p: 1 for p in properties + keys}})\n\n    alpha = \"abcdefghijklmnopqrstuvwxyz\"\n    group_id = {letter: f\"${key}\" for letter, key in zip(alpha, keys)}\n    pipeline.append({\"$group\": {\"_id\": group_id, \"docs\": {\"$push\": \"$$ROOT\"}}})\n    for d in self._collection.aggregate(pipeline, allowDiskUse=True):\n        id_doc = {}  # type: ignore\n        for letter, key in group_id.items():\n            if has(d[\"_id\"], letter):\n                set_(id_doc, key[1:], d[\"_id\"][letter])\n        yield (id_doc, d[\"docs\"])\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.query","title":"<code>query(criteria=None, properties=None, sort=None, hint=None, skip=0, limit=0, **kwargs)</code>","text":"<p>Queries the Store for a set of documents.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>hint</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> <code>mongoclient_kwargs</code> <p>Dict of extra kwargs to pass to pymongo find.</p> required Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def query(  # type: ignore\n    self,\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    hint: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n    **kwargs,\n) -&gt; Iterator[dict]:\n    \"\"\"\n    Queries the Store for a set of documents.\n\n    Args:\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        hint: Dictionary of indexes to use as hints for query optimizer.\n            Keys are field names and values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n        mongoclient_kwargs: Dict of extra kwargs to pass to pymongo find.\n    \"\"\"\n    if isinstance(properties, list):\n        properties = {p: 1 for p in properties}\n\n    default_sort_formatted = None\n\n    if self.default_sort is not None:\n        default_sort_formatted = [\n            (k, Sort(v).value) if isinstance(v, int) else (k, v.value) for k, v in self.default_sort.items()\n        ]\n\n    sort_list = (\n        [(k, Sort(v).value) if isinstance(v, int) else (k, v.value) for k, v in sort.items()]\n        if sort\n        else default_sort_formatted\n    )\n\n    hint_list = (\n        [(k, Sort(v).value) if isinstance(v, int) else (k, v.value) for k, v in hint.items()] if hint else None\n    )\n\n    yield from self._collection.find(\n        filter=criteria,\n        projection=properties,\n        skip=skip,\n        limit=limit,\n        sort=sort_list,\n        hint=hint_list,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.remove_docs","title":"<code>remove_docs(criteria)</code>","text":"<p>Remove docs matching the query dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>dict</code> <p>query dictionary to match</p> required Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def remove_docs(self, criteria: dict):\n    \"\"\"\n    Remove docs matching the query dictionary.\n\n    Args:\n        criteria: query dictionary to match\n    \"\"\"\n    self._collection.delete_many(filter=criteria)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoStore.update","title":"<code>update(docs, key=None)</code>","text":"<p>Update documents into the Store.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[list[dict], dict]</code> <p>the document or list of documents to update</p> required <code>key</code> <code>Union[list, str, None]</code> <p>field name(s) to determine uniqueness for a  document, can be a list of multiple fields,  a single field, or None if the Store's key  field is to be used</p> <code>None</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n    \"\"\"\n    Update documents into the Store.\n\n    Args:\n        docs: the document or list of documents to update\n        key: field name(s) to determine uniqueness for a\n             document, can be a list of multiple fields,\n             a single field, or None if the Store's key\n             field is to be used\n    \"\"\"\n    requests = []\n\n    if not isinstance(docs, list):\n        docs = [docs]\n\n    for d in (jsanitize(x, allow_bson=True, recursive_msonable=True) for x in docs):\n        # document-level validation is optional\n        validates = True\n        if self.validator:\n            validates = self.validator.is_valid(d)\n            if not validates:\n                if self.validator.strict:\n                    raise ValueError(self.validator.validation_errors(d))\n                self.logger.error(self.validator.validation_errors(d))\n\n        if validates:\n            key = key or self.key\n            search_doc = {k: d[k] for k in key} if isinstance(key, list) else {key: d[key]}\n\n            requests.append(ReplaceOne(search_doc, d, upsert=True))\n\n    if len(requests) &gt; 0:\n        try:\n            self._collection.bulk_write(requests, ordered=False)\n        except (OperationFailure, DocumentTooLarge) as e:\n            if self.safe_update:\n                for req in requests:\n                    try:\n                        self._collection.bulk_write([req], ordered=False)\n                    except (OperationFailure, DocumentTooLarge):\n                        self.logger.error(\n                            f\"Could not upload document for {req._filter} as it was too large for Mongo\"\n                        )\n            else:\n                raise e\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoURIStore","title":"<code>MongoURIStore</code>","text":"<p>               Bases: <code>MongoStore</code></p> <p>A Store that connects to a Mongo collection via a URI This is expected to be a special mongodb+srv:// URIs that include client parameters via TXT records.</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>class MongoURIStore(MongoStore):\n    \"\"\"\n    A Store that connects to a Mongo collection via a URI\n    This is expected to be a special mongodb+srv:// URIs that include\n    client parameters via TXT records.\n    \"\"\"\n\n    def __init__(\n        self,\n        uri: str,\n        collection_name: str,\n        database: Optional[str] = None,\n        ssh_tunnel: Optional[SSHTunnel] = None,\n        safe_update: bool = False,\n        mongoclient_kwargs: Optional[dict] = None,\n        default_sort: Optional[dict[str, Union[Sort, int]]] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            uri: MongoDB+SRV URI\n            database: database to connect to\n            collection_name: The collection name\n            default_sort: Default sort field and direction to use when querying. Can be used to\n                ensure determinacy in query results.\n        \"\"\"\n        self.uri = uri\n        if ssh_tunnel:\n            raise ValueError(f\"At the moment ssh_tunnel is not supported for {self.__class__.__name__}\")\n        self.ssh_tunnel = None\n        self.default_sort = default_sort\n        self.safe_update = safe_update\n        self.mongoclient_kwargs = mongoclient_kwargs or {}\n\n        # parse the dbname from the uri\n        if database is None:\n            d_uri = uri_parser.parse_uri(uri)\n            if d_uri[\"database\"] is None:\n                raise ConfigurationError(\"If database name is not supplied, a database must be set in the uri\")\n            self.database = d_uri[\"database\"]\n        else:\n            self.database = database\n\n        self.collection_name = collection_name\n        self.kwargs = kwargs\n        self._coll = None\n        super(MongoStore, self).__init__(**kwargs)  # lgtm\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        Return a string representing this data source.\n        \"\"\"\n        # TODO: This is not very safe since it exposes the username/password info\n        return self.uri\n\n    def connect(self, force_reset: bool = False):\n        \"\"\"\n        Connect to the source data.\n\n        Args:\n            force_reset: whether to reset the connection or not when the Store is\n                already connected.\n        \"\"\"\n        if self._coll is None or force_reset:  # pragma: no cover\n            conn: MongoClient = MongoClient(self.uri, **self.mongoclient_kwargs)\n            db = conn[self.database]\n            self._coll = db[self.collection_name]  # type: ignore\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoURIStore.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return a string representing this data source.</p>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoURIStore.__init__","title":"<code>__init__(uri, collection_name, database=None, ssh_tunnel=None, safe_update=False, mongoclient_kwargs=None, default_sort=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>MongoDB+SRV URI</p> required <code>database</code> <code>Optional[str]</code> <p>database to connect to</p> <code>None</code> <code>collection_name</code> <code>str</code> <p>The collection name</p> required <code>default_sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Default sort field and direction to use when querying. Can be used to ensure determinacy in query results.</p> <code>None</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def __init__(\n    self,\n    uri: str,\n    collection_name: str,\n    database: Optional[str] = None,\n    ssh_tunnel: Optional[SSHTunnel] = None,\n    safe_update: bool = False,\n    mongoclient_kwargs: Optional[dict] = None,\n    default_sort: Optional[dict[str, Union[Sort, int]]] = None,\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        uri: MongoDB+SRV URI\n        database: database to connect to\n        collection_name: The collection name\n        default_sort: Default sort field and direction to use when querying. Can be used to\n            ensure determinacy in query results.\n    \"\"\"\n    self.uri = uri\n    if ssh_tunnel:\n        raise ValueError(f\"At the moment ssh_tunnel is not supported for {self.__class__.__name__}\")\n    self.ssh_tunnel = None\n    self.default_sort = default_sort\n    self.safe_update = safe_update\n    self.mongoclient_kwargs = mongoclient_kwargs or {}\n\n    # parse the dbname from the uri\n    if database is None:\n        d_uri = uri_parser.parse_uri(uri)\n        if d_uri[\"database\"] is None:\n            raise ConfigurationError(\"If database name is not supplied, a database must be set in the uri\")\n        self.database = d_uri[\"database\"]\n    else:\n        self.database = database\n\n    self.collection_name = collection_name\n    self.kwargs = kwargs\n    self._coll = None\n    super(MongoStore, self).__init__(**kwargs)  # lgtm\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MongoURIStore.connect","title":"<code>connect(force_reset=False)</code>","text":"<p>Connect to the source data.</p> <p>Parameters:</p> Name Type Description Default <code>force_reset</code> <code>bool</code> <p>whether to reset the connection or not when the Store is already connected.</p> <code>False</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def connect(self, force_reset: bool = False):\n    \"\"\"\n    Connect to the source data.\n\n    Args:\n        force_reset: whether to reset the connection or not when the Store is\n            already connected.\n    \"\"\"\n    if self._coll is None or force_reset:  # pragma: no cover\n        conn: MongoClient = MongoClient(self.uri, **self.mongoclient_kwargs)\n        db = conn[self.database]\n        self._coll = db[self.collection_name]  # type: ignore\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MontyStore","title":"<code>MontyStore</code>","text":"<p>               Bases: <code>MemoryStore</code></p> <p>A MongoDB compatible store that uses on disk files for storage.</p> <p>This is handled under the hood using MontyDB. A number of on-disk storage options are available but MontyDB provides a mongo style interface for all options. The options include:</p> <ul> <li>sqlite: Uses an sqlite database to store documents.</li> <li>lightning: Uses Lightning Memory-Mapped Database (LMDB) for storage. This can   provide fast read and write times but requires lmdb to be installed (in most cases   this can be achieved using <code>pip install lmdb</code>).</li> <li>flatfile: Uses a system of flat json files. This is not recommended as multiple   simultaneous connections to the store will not work correctly.</li> </ul> <p>Note that MontyDB (and, therefore, MontyStore) will write out a new database to the disk but cannot be used to read an existing (e.g. SQLite) database that wasn't formatted by MontyDB.</p> <p>See the MontyDB repository for more information: https://github.com/davidlatwe/montydb</p> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>@requires(\n    MontyClient is not None,\n    \"MontyStore requires MontyDB to be installed. See the MontyDB repository for more \"\n    \"information: https://github.com/davidlatwe/montydb\",\n)\nclass MontyStore(MemoryStore):\n    \"\"\"\n    A MongoDB compatible store that uses on disk files for storage.\n\n    This is handled under the hood using MontyDB. A number of on-disk storage options\n    are available but MontyDB provides a mongo style interface for all options. The\n    options include:\n\n    - sqlite: Uses an sqlite database to store documents.\n    - lightning: Uses Lightning Memory-Mapped Database (LMDB) for storage. This can\n      provide fast read and write times but requires lmdb to be installed (in most cases\n      this can be achieved using ``pip install lmdb``).\n    - flatfile: Uses a system of flat json files. This is not recommended as multiple\n      simultaneous connections to the store will not work correctly.\n\n    Note that MontyDB (and, therefore, MontyStore) will write out a new database to\n    the disk but cannot be used to read an existing (e.g. SQLite) database that wasn't\n    formatted by MontyDB.\n\n    See the MontyDB repository for more information: https://github.com/davidlatwe/montydb\n    \"\"\"\n\n    def __init__(\n        self,\n        collection_name,\n        database_path: Optional[str] = None,\n        database_name: str = \"db\",\n        storage: Literal[\"sqlite\", \"flatfile\", \"lightning\"] = \"sqlite\",\n        storage_kwargs: Optional[dict] = None,\n        client_kwargs: Optional[dict] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes the Monty Store.\n\n        Args:\n            collection_name: Name for the collection.\n            database_path: Path to on-disk database files. If None, the current working\n                directory will be used.\n            database_name: The database name.\n            storage: The storage type. Options include \"sqlite\", \"lightning\", \"flatfile\". Note that\n            although MontyDB supports in memory storage, this capability is disabled in maggma to avoid unintended\n            behavior, since multiple in-memory MontyStore would actually point to the same data.\n            storage_kwargs: Keyword arguments passed to ``montydb.set_storage``.\n            client_kwargs: Keyword arguments passed to the ``montydb.MontyClient``\n                constructor.\n            **kwargs: Additional keyword arguments passed to the Store constructor.\n        \"\"\"\n        if database_path is None:\n            database_path = str(Path.cwd())\n\n        self.database_path = database_path\n        self.database_name = database_name\n        self.collection_name = collection_name\n        self._coll = None  # type: ignore\n        self.default_sort = None\n        self.ssh_tunnel = None  # This is to fix issues with the tunnel on close\n        self.kwargs = kwargs\n        self.storage = storage\n        self.storage_kwargs = storage_kwargs or {\n            \"use_bson\": True,  # import pymongo's BSON; do not use montydb's\n            \"mongo_version\": \"4.0\",\n        }\n        self.client_kwargs = client_kwargs or {}\n        super(MongoStore, self).__init__(**kwargs)\n\n    def connect(self, force_reset: bool = False):\n        \"\"\"\n        Connect to the database store.\n\n        Args:\n            force_reset: whether to reset the connection or not when the Store is\n                already connected.\n        \"\"\"\n        if not self._coll or force_reset:\n            # TODO - workaround, may be obviated by a future montydb update\n            if self.database_path != \":memory:\":\n                set_storage(self.database_path, storage=self.storage, **self.storage_kwargs)\n            client = MontyClient(self.database_path, **self.client_kwargs)\n            self._coll = client[self.database_name][self.collection_name]\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"Return a string representing this data source.\"\"\"\n        return f\"monty://{self.database_path}/{self.database_name}/{self.collection_name}\"\n\n    def count(\n        self,\n        criteria: Optional[dict] = None,\n        hint: Optional[dict[str, Union[Sort, int]]] = None,\n    ) -&gt; int:\n        \"\"\"\n        Counts the number of documents matching the query criteria.\n\n        Args:\n            criteria: PyMongo filter for documents to count in\n            hint: Dictionary of indexes to use as hints for query optimizer.\n                Keys are field names and values are 1 for ascending or -1 for descending.\n        \"\"\"\n        criteria = criteria if criteria else {}\n\n        hint_list = (\n            [(k, Sort(v).value) if isinstance(v, int) else (k, v.value) for k, v in hint.items()] if hint else None\n        )\n\n        if hint_list is not None:  # pragma: no cover\n            return self._collection.count_documents(filter=criteria, hint=hint_list)\n\n        return self._collection.count_documents(filter=criteria)\n\n    def update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n        \"\"\"\n        Update documents into the Store.\n\n        Args:\n            docs: The document or list of documents to update.\n            key: Field name(s) to determine uniqueness for a document, can be a list of\n                multiple fields, a single field, or None if the Store's key field is to be\n                used.\n        \"\"\"\n        if not isinstance(docs, list):\n            docs = [docs]\n\n        for d in docs:\n            d = jsanitize(d, allow_bson=True)\n\n            # document-level validation is optional\n            validates = True\n            if self.validator:\n                validates = self.validator.is_valid(d)\n                if not validates:\n                    if self.validator.strict:\n                        raise ValueError(self.validator.validation_errors(d))\n                    self.logger.error(self.validator.validation_errors(d))\n\n            if validates:\n                key = key or self.key\n                search_doc = {k: d[k] for k in key} if isinstance(key, list) else {key: d[key]}\n\n                self._collection.replace_one(search_doc, d, upsert=True)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MontyStore.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return a string representing this data source.</p>"},{"location":"reference/stores/#maggma.stores.mongolike.MontyStore.__init__","title":"<code>__init__(collection_name, database_path=None, database_name='db', storage='sqlite', storage_kwargs=None, client_kwargs=None, **kwargs)</code>","text":"<p>Initializes the Monty Store.</p> <p>Parameters:</p> Name Type Description Default <code>collection_name</code> <p>Name for the collection.</p> required <code>database_path</code> <code>Optional[str]</code> <p>Path to on-disk database files. If None, the current working directory will be used.</p> <code>None</code> <code>database_name</code> <code>str</code> <p>The database name.</p> <code>'db'</code> <code>storage</code> <code>Literal['sqlite', 'flatfile', 'lightning']</code> <p>The storage type. Options include \"sqlite\", \"lightning\", \"flatfile\". Note that</p> <code>'sqlite'</code> <code>storage_kwargs</code> <code>Optional[dict]</code> <p>Keyword arguments passed to <code>montydb.set_storage</code>.</p> <code>None</code> <code>client_kwargs</code> <code>Optional[dict]</code> <p>Keyword arguments passed to the <code>montydb.MontyClient</code> constructor.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments passed to the Store constructor.</p> <code>{}</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def __init__(\n    self,\n    collection_name,\n    database_path: Optional[str] = None,\n    database_name: str = \"db\",\n    storage: Literal[\"sqlite\", \"flatfile\", \"lightning\"] = \"sqlite\",\n    storage_kwargs: Optional[dict] = None,\n    client_kwargs: Optional[dict] = None,\n    **kwargs,\n):\n    \"\"\"\n    Initializes the Monty Store.\n\n    Args:\n        collection_name: Name for the collection.\n        database_path: Path to on-disk database files. If None, the current working\n            directory will be used.\n        database_name: The database name.\n        storage: The storage type. Options include \"sqlite\", \"lightning\", \"flatfile\". Note that\n        although MontyDB supports in memory storage, this capability is disabled in maggma to avoid unintended\n        behavior, since multiple in-memory MontyStore would actually point to the same data.\n        storage_kwargs: Keyword arguments passed to ``montydb.set_storage``.\n        client_kwargs: Keyword arguments passed to the ``montydb.MontyClient``\n            constructor.\n        **kwargs: Additional keyword arguments passed to the Store constructor.\n    \"\"\"\n    if database_path is None:\n        database_path = str(Path.cwd())\n\n    self.database_path = database_path\n    self.database_name = database_name\n    self.collection_name = collection_name\n    self._coll = None  # type: ignore\n    self.default_sort = None\n    self.ssh_tunnel = None  # This is to fix issues with the tunnel on close\n    self.kwargs = kwargs\n    self.storage = storage\n    self.storage_kwargs = storage_kwargs or {\n        \"use_bson\": True,  # import pymongo's BSON; do not use montydb's\n        \"mongo_version\": \"4.0\",\n    }\n    self.client_kwargs = client_kwargs or {}\n    super(MongoStore, self).__init__(**kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MontyStore.connect","title":"<code>connect(force_reset=False)</code>","text":"<p>Connect to the database store.</p> <p>Parameters:</p> Name Type Description Default <code>force_reset</code> <code>bool</code> <p>whether to reset the connection or not when the Store is already connected.</p> <code>False</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def connect(self, force_reset: bool = False):\n    \"\"\"\n    Connect to the database store.\n\n    Args:\n        force_reset: whether to reset the connection or not when the Store is\n            already connected.\n    \"\"\"\n    if not self._coll or force_reset:\n        # TODO - workaround, may be obviated by a future montydb update\n        if self.database_path != \":memory:\":\n            set_storage(self.database_path, storage=self.storage, **self.storage_kwargs)\n        client = MontyClient(self.database_path, **self.client_kwargs)\n        self._coll = client[self.database_name][self.collection_name]\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MontyStore.count","title":"<code>count(criteria=None, hint=None)</code>","text":"<p>Counts the number of documents matching the query criteria.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to count in</p> <code>None</code> <code>hint</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def count(\n    self,\n    criteria: Optional[dict] = None,\n    hint: Optional[dict[str, Union[Sort, int]]] = None,\n) -&gt; int:\n    \"\"\"\n    Counts the number of documents matching the query criteria.\n\n    Args:\n        criteria: PyMongo filter for documents to count in\n        hint: Dictionary of indexes to use as hints for query optimizer.\n            Keys are field names and values are 1 for ascending or -1 for descending.\n    \"\"\"\n    criteria = criteria if criteria else {}\n\n    hint_list = (\n        [(k, Sort(v).value) if isinstance(v, int) else (k, v.value) for k, v in hint.items()] if hint else None\n    )\n\n    if hint_list is not None:  # pragma: no cover\n        return self._collection.count_documents(filter=criteria, hint=hint_list)\n\n    return self._collection.count_documents(filter=criteria)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.mongolike.MontyStore.update","title":"<code>update(docs, key=None)</code>","text":"<p>Update documents into the Store.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[list[dict], dict]</code> <p>The document or list of documents to update.</p> required <code>key</code> <code>Union[list, str, None]</code> <p>Field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used.</p> <code>None</code> Source code in <code>src/maggma/stores/mongolike.py</code> <pre><code>def update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n    \"\"\"\n    Update documents into the Store.\n\n    Args:\n        docs: The document or list of documents to update.\n        key: Field name(s) to determine uniqueness for a document, can be a list of\n            multiple fields, a single field, or None if the Store's key field is to be\n            used.\n    \"\"\"\n    if not isinstance(docs, list):\n        docs = [docs]\n\n    for d in docs:\n        d = jsanitize(d, allow_bson=True)\n\n        # document-level validation is optional\n        validates = True\n        if self.validator:\n            validates = self.validator.is_valid(d)\n            if not validates:\n                if self.validator.strict:\n                    raise ValueError(self.validator.validation_errors(d))\n                self.logger.error(self.validator.validation_errors(d))\n\n        if validates:\n            key = key or self.key\n            search_doc = {k: d[k] for k in key} if isinstance(key, list) else {key: d[key]}\n\n            self._collection.replace_one(search_doc, d, upsert=True)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.file_store.FileStore","title":"<code>FileStore</code>","text":"<p>               Bases: <code>MemoryStore</code></p> <p>A Store for files on disk. Provides a common access method consistent with other stores. Each Item in the Store represents one file. Files can be organized into any type of directory structure.</p> <p>A hash of the full path to each file is used to define a file_id that uniquely identifies each item.</p> <p>Any metadata added to the items is written to a .json file in the root directory of the FileStore.</p> Source code in <code>src/maggma/stores/file_store.py</code> <pre><code>class FileStore(MemoryStore):\n    \"\"\"\n    A Store for files on disk. Provides a common access method consistent with\n    other stores. Each Item in the Store represents one file. Files can be organized\n    into any type of directory structure.\n\n    A hash of the full path to each file is used to define a file_id that uniquely\n    identifies each item.\n\n    Any metadata added to the items is written to a .json file in the root directory\n    of the FileStore.\n    \"\"\"\n\n    def __init__(\n        self,\n        path: Union[str, Path],\n        file_filters: Optional[list] = None,\n        max_depth: Optional[int] = None,\n        read_only: bool = True,\n        include_orphans: bool = False,\n        json_name: str = \"FileStore.json\",\n        encoding: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes a FileStore.\n\n        Args:\n            path: parent directory containing all files and subdirectories to process\n            file_filters: List of fnmatch patterns defining the files to be tracked by\n                the FileStore. Only files that match one of the patterns  provided will\n                be included in the Store If None (default), all files are included.\n\n                Examples: [\"*.txt\", \"test-[abcd].txt\"], etc.\n                See https://docs.python.org/3/library/fnmatch.html for full syntax\n            max_depth: The maximum depth to look into subdirectories. 0 = no recursion,\n                1 = include files 1 directory below the FileStore, etc.\n                None (default) will scan all files below\n                the FileStore root directory, regardless of depth.\n            read_only: If True (default), the .update() and .remove_docs()\n                methods are disabled, preventing any changes to the files on\n                disk. In addition, metadata cannot be written to disk.\n            include_orphans: Whether to include orphaned metadata records in query results.\n                Orphaned metadata records are records found in the local JSON file that can\n                no longer be associated to a file on disk. This can happen if a file is renamed\n                or deleted, or if the FileStore is re-initialized with a more restrictive\n                file_filters or max_depth argument. By default (False), these records\n                do not appear in query results. Nevertheless, the metadata records are\n                retained in the JSON file and the FileStore to prevent accidental data loss.\n            json_name: Name of the .json file to which metadata is saved. If read_only\n                is False, this file will be created in the root directory of the\n                FileStore.\n            encoding: Character encoding of files to be tracked by the store. The default\n                (None) follows python's default behavior, which is to determine the character\n                encoding from the platform. This should work in the great majority of cases.\n                However, if you encounter a UnicodeDecodeError, consider setting the encoding\n                explicitly to 'utf8' or another encoding as appropriate.\n            kwargs: kwargs passed to MemoryStore.__init__()\n        \"\"\"\n        # this conditional block is needed in order to guarantee that the 'name'\n        # property, which is passed to `MemoryStore`, works correctly\n        # collection names passed to MemoryStore cannot end with '.'\n        if path == \".\":\n            path = Path.cwd()\n        self.path = Path(path) if isinstance(path, str) else path\n\n        self.json_name = json_name\n        file_filters = file_filters if file_filters else [\"*\"]\n        self.file_filters = re.compile(\"|\".join(fnmatch.translate(p) for p in file_filters))\n        self.collection_name = \"file_store\"\n        self.key = \"file_id\"\n        self.include_orphans = include_orphans\n        self.read_only = read_only\n        self.max_depth = max_depth\n        self.encoding = encoding\n\n        self.metadata_store = JSONStore(\n            paths=[str(self.path / self.json_name)],\n            read_only=self.read_only,\n            collection_name=self.collection_name,\n            key=self.key,\n        )\n\n        self.kwargs = kwargs\n\n        super().__init__(\n            collection_name=self.collection_name,\n            key=self.key,\n            **self.kwargs,\n        )\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        Return a string representing this data source.\n        \"\"\"\n        return f\"file://{self.path}\"\n\n    def add_metadata(\n        self,\n        metadata: Optional[dict] = None,\n        query: Optional[dict] = None,\n        auto_data: Optional[Callable[[dict], dict]] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Add metadata to a record in the FileStore, either manually or by computing it automatically\n        from another field, such as name or path (see auto_data).\n\n        Args:\n            metadata: dict of additional data to add to the records returned by query.\n                      Note that any protected keys (such as 'name', 'path', etc.)\n                      will be ignored.\n            query: Query passed to FileStore.query()\n            auto_data: A function that automatically computes metadata based on a field in\n                    the record itself. The function must take in the item as a dict and\n                    return a dict containing the desired metadata. A typical use case is\n                    to assign metadata based on the name of a file. For example, for\n                    data files named like `2022-04-01_april_fool_experiment.txt`, the\n                    auto_data function could be:\n\n                    def get_metadata_from_filename(d):\n                        return {\"date\": d[\"name\"].split(\"_\")[0],\n                                \"test_name\": d[\"name\"].split(\"_\")[1]\n                                }\n\n                    Note that in the case of conflict between manual and automatically\n                    computed metadata (for example, if metadata={\"name\": \"another_name\"} was\n                    supplied alongside the auto_data function above), the manually-supplied\n                    metadata is used.\n            kwargs: kwargs passed to FileStore.query()\n        \"\"\"\n        if metadata is None:\n            metadata = {}\n        # sanitize the metadata\n        filtered_metadata = self._filter_data(metadata)\n        updated_docs = []\n\n        for doc in self.query(query, **kwargs):\n            if auto_data:\n                extra_data = self._filter_data(auto_data(doc))\n                doc.update(extra_data)\n            doc.update(filtered_metadata)\n            updated_docs.append(doc)\n\n        self.update(updated_docs, key=self.key)\n\n    def read(self) -&gt; list[dict]:\n        \"\"\"\n        Iterate through all files in the Store folder and populate\n        the Store with dictionaries containing basic information about each file.\n\n        The keys of the documents added to the Store are:\n\n        - name: str = File name\n        - path: Path = Absolute path of this file\n        - parent: str = Name of the parent directory (if any)\n        - file_id: str = Unique identifier for this file, computed from the hash\n                    of its path relative to the base FileStore directory and\n                    the file creation time. The key of this field is 'file_id'\n                    by default but can be changed via the 'key' kwarg to\n                    `FileStore.__init__()`.\n        - size: int = Size of this file in bytes\n        - last_updated: datetime = Time this file was last modified\n        - hash: str = Hash of the file contents\n        - orphan: bool = Whether this record is an orphan\n        \"\"\"\n        file_list = []\n        # generate a list of files in subdirectories\n        for root, _dirs, files in os.walk(self.path):\n            # for pattern in self.file_filters:\n            for match in filter(self.file_filters.match, files):\n                # for match in fnmatch.filter(files, pattern):\n                path = Path(os.path.join(root, match))\n                # ignore the .json file created by the Store\n                if path.is_file() and path.name != self.json_name:\n                    # filter based on depth\n                    depth = len(path.relative_to(self.path).parts) - 1\n                    if self.max_depth is None or depth &lt;= self.max_depth:\n                        file_list.append(self._create_record_from_file(path))\n\n        return file_list\n\n    def _create_record_from_file(self, f: Path) -&gt; dict:\n        \"\"\"\n        Given the path to a file, return a Dict that constitutes a record of\n        basic information about that file. The keys in the returned dict\n        are:\n\n        - name: str = File name\n        - path: Path = Absolute path of this file\n        - parent: str = Name of the parent directory (if any)\n        - file_id: str = Unique identifier for this file, computed from the hash\n                    of its path relative to the base FileStore directory and\n                    the file creation time. The key of this field is 'file_id'\n                    by default but can be changed via the 'key' kwarg to\n                    FileStore.__init__().\n        - size: int = Size of this file in bytes\n        - last_updated: datetime = Time this file was last modified\n        - hash: str = Hash of the file contents\n        - orphan: bool = Whether this record is an orphan\n        \"\"\"\n        # compute the file_id from the relative path\n        relative_path = f.relative_to(self.path)\n        digest = hashlib.md5()\n        digest.update(str(relative_path).encode())\n        file_id = str(digest.hexdigest())\n\n        # hash the file contents\n        digest2 = hashlib.md5()\n        b = bytearray(128 * 2056)\n        mv = memoryview(b)\n        digest2.update(self.name.encode())\n        with open(f.as_posix(), \"rb\", buffering=0) as file:\n            # this block copied from the file_digest method in python 3.11+\n            # see https://github.com/python/cpython/blob/0ba07b2108d4763273f3fb85544dde34c5acd40a/Lib/hashlib.py#L213\n            if hasattr(file, \"getbuffer\"):\n                # io.BytesIO object, use zero-copy buffer\n                digest2.update(file.getbuffer())\n            else:\n                for n in iter(lambda: file.readinto(mv), 0):\n                    digest2.update(mv[:n])\n\n        content_hash = str(digest2.hexdigest())\n        stats = f.stat()\n\n        return {\n            \"name\": f.name,\n            \"path\": f,\n            \"path_relative\": relative_path,\n            \"parent\": f.parent.name,\n            \"size\": stats.st_size,\n            \"last_updated\": datetime.fromtimestamp(stats.st_mtime, tz=timezone.utc),\n            \"orphan\": False,\n            \"hash\": content_hash,\n            self.key: file_id,\n        }\n\n    def connect(self, force_reset: bool = False):\n        \"\"\"\n        Connect to the source data.\n\n        Read all the files in the directory, create corresponding File\n        items in the internal MemoryStore.\n\n        If there is a metadata .json file in the directory, read its\n        contents into the MemoryStore\n\n        Args:\n            force_reset: whether to reset the connection or not when the Store is\n                already connected.\n        \"\"\"\n        # read all files and place them in the MemoryStore\n        # use super.update to bypass the read_only guard statement\n        # because we want the file data to be populated in memory\n        super().connect(force_reset=force_reset)\n        super().update(self.read())\n\n        # now read any metadata from the .json file\n        try:\n            self.metadata_store.connect(force_reset=force_reset)\n            metadata = list(self.metadata_store.query())\n        except FileNotFoundError:\n            metadata = []\n            warnings.warn(\n                f\"\"\"\n                JSON file '{self.json_name}' not found. To create this file automatically, re-initialize\n                the FileStore with read_only=False.\n                \"\"\"\n            )\n\n        # merge metadata with file data and check for orphaned metadata\n        requests = []\n        found_orphans = False\n        key = self.key\n        file_ids = self.distinct(self.key)\n        for d in metadata:\n            search_doc = {k: d[k] for k in key} if isinstance(key, list) else {key: d[key]}\n\n            if d[key] not in file_ids:\n                found_orphans = True\n                d.update({\"orphan\": True})\n\n            del d[\"_id\"]\n\n            requests.append(UpdateOne(search_doc, {\"$set\": d}, upsert=True))\n\n        if found_orphans:\n            warnings.warn(\n                f\"Orphaned metadata was found in {self.json_name}. This metadata\"\n                \"will be added to the store with {'orphan': True}\"\n            )\n        if len(requests) &gt; 0:\n            self._collection.bulk_write(requests, ordered=False)\n\n    def update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n        \"\"\"\n        Update items in the Store. Only possible if the store is not read only. Any new\n        fields that are added will be written to the JSON file in the root directory\n        of the FileStore.\n\n        Note that certain fields that come from file metadata on disk are protected and\n        cannot be updated with this method. This prevents the contents of the FileStore\n        from becoming out of sync with the files on which it is based. The protected fields\n        are keys in the dict returned by _create_record_from_file, e.g. 'name', 'parent',\n        'path', 'last_updated', 'hash', 'size', 'contents', and 'orphan'. The 'path_relative' and key fields are\n        retained to make each document in the JSON file identifiable by manual inspection.\n\n        Args:\n            docs: the document or list of documents to update\n            key: field name(s) to determine uniqueness for a\n                 document, can be a list of multiple fields,\n                 a single field, or None if the Store's key\n                 field is to be used\n        \"\"\"\n        if self.read_only:\n            raise StoreError(\n                \"This Store is read-only. To enable file I/O, re-initialize the store with read_only=False.\"\n            )\n\n        super().update(docs, key)\n        data = list(self.query())\n        filtered_data = []\n        # remove fields that are populated by .read()\n        for d in data:\n            filtered_d = self._filter_data(d)\n            # don't write records that contain only file_id\n            if len(set(filtered_d.keys()).difference({\"path_relative\", self.key})) != 0:\n                filtered_data.append(filtered_d)\n        self.metadata_store.update(filtered_data, self.key)\n\n    def _filter_data(self, d):\n        \"\"\"\n        Remove any protected keys from a dictionary.\n\n        Args:\n            d: Dictionary whose keys are to be filtered\n        \"\"\"\n        return {k: v for k, v in d.items() if k not in PROTECTED_KEYS.union({self.last_updated_field})}\n\n    def query(  # type: ignore\n        self,\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        hint: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n        contents_size_limit: Optional[int] = 0,\n    ) -&gt; Iterator[dict]:\n        \"\"\"\n        Queries the Store for a set of documents.\n\n        Args:\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            hint: Dictionary of indexes to use as hints for query optimizer.\n                Keys are field names and values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n            contents_size_limit: Maximum file size in bytes for which to return contents.\n                The FileStore will attempt to read the file and populate the 'contents' key\n                with its content at query time, unless the file size is larger than this value.\n                By default, reading content is disabled. Note that enabling content reading\n                can substantially slow down the query operation, especially when there\n                are large numbers of files.\n        \"\"\"\n        return_contents = False\n        criteria = criteria if criteria else {}\n        if criteria.get(\"orphan\", None) is None and not self.include_orphans:\n            criteria.update({\"orphan\": False})\n\n        if criteria.get(\"contents\"):\n            warnings.warn(\"'contents' is not a queryable field! Ignoring.\")\n\n        if isinstance(properties, list):\n            properties = {p: 1 for p in properties}\n\n        orig_properties = properties.copy() if properties else None\n\n        if properties is None:\n            # None means return all fields, including contents\n            return_contents = True\n        elif properties.get(\"contents\"):\n            return_contents = True\n            # remove contents b/c it isn't stored in the MemoryStore\n            properties.pop(\"contents\")\n            # add size and path to query so that file can be read\n            properties.update({\"size\": 1})\n            properties.update({\"path\": 1})\n\n        for d in super().query(\n            criteria=criteria,\n            properties=properties,\n            sort=sort,\n            hint=hint,\n            skip=skip,\n            limit=limit,\n        ):\n            # add file contents to the returned documents, if appropriate\n            if return_contents and not d.get(\"orphan\"):\n                if contents_size_limit is None or d[\"size\"] &lt;= contents_size_limit:\n                    # attempt to read the file contents and inject into the document\n                    # TODO - could add more logic for detecting different file types\n                    # and more nuanced exception handling\n                    try:\n                        with zopen(d[\"path\"], mode=\"rt\", encoding=self.encoding) as f:\n                            data = f.read()\n                    except Exception as e:\n                        data = f\"Unable to read: {e}\"\n\n                elif d[\"size\"] &gt; contents_size_limit:\n                    data = f\"File exceeds size limit of {contents_size_limit} bytes\"\n                else:\n                    data = \"Unable to read: Unknown error\"\n\n                d.update({\"contents\": data})\n\n                # remove size and path if not explicitly requested\n                if orig_properties is not None and \"size\" not in orig_properties:\n                    d.pop(\"size\")\n                if orig_properties is not None and \"path\" not in orig_properties:\n                    d.pop(\"path\")\n\n            yield d\n\n    def query_one(\n        self,\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        contents_size_limit: Optional[int] = None,\n    ):\n        \"\"\"\n        Queries the Store for a single document.\n\n        Args:\n            criteria: PyMongo filter for documents to search\n            properties: properties to return in the document\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            contents_size_limit: Maximum file size in bytes for which to return contents.\n                The FileStore will attempt to read the file and populate the 'contents' key\n                with its content at query time, unless the file size is larger than this value.\n        \"\"\"\n        return next(\n            self.query(\n                criteria=criteria,\n                properties=properties,\n                sort=sort,\n                contents_size_limit=contents_size_limit,\n            ),\n            None,\n        )\n\n    def remove_docs(self, criteria: dict, confirm: bool = False):\n        \"\"\"\n        Remove items matching the query dictionary.\n\n        Args:\n            criteria: query dictionary to match\n            confirm: Boolean flag to confirm that remove_docs should delete\n                     files on disk. Default: False.\n        \"\"\"\n        if self.read_only:\n            raise StoreError(\n                \"This Store is read-only. To enable file I/O, re-initialize the store with read_only=False.\"\n            )\n\n        docs = list(self.query(criteria))\n        # this ensures that any modifications to criteria made by self.query\n        # (e.g., related to orphans or contents) are propagated through to the superclass\n        new_criteria = {\"file_id\": {\"$in\": [d[\"file_id\"] for d in docs]}}\n\n        if len(docs) &gt; 0 and not confirm:\n            raise StoreError(\n                f\"Warning! This command is about to delete {len(docs)} items from disk! \"\n                \"If this is what you want, reissue this command with confirm=True.\"\n            )\n\n        for d in docs:\n            Path(d[\"path\"]).unlink()\n            super().remove_docs(criteria=new_criteria)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.file_store.FileStore.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return a string representing this data source.</p>"},{"location":"reference/stores/#maggma.stores.file_store.FileStore.__init__","title":"<code>__init__(path, file_filters=None, max_depth=None, read_only=True, include_orphans=False, json_name='FileStore.json', encoding=None, **kwargs)</code>","text":"<p>Initializes a FileStore.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[str, Path]</code> <p>parent directory containing all files and subdirectories to process</p> required <code>file_filters</code> <code>Optional[list]</code> <p>List of fnmatch patterns defining the files to be tracked by the FileStore. Only files that match one of the patterns  provided will be included in the Store If None (default), all files are included.</p> <p>Examples: [\"*.txt\", \"test-[abcd].txt\"], etc. See https://docs.python.org/3/library/fnmatch.html for full syntax</p> <code>None</code> <code>max_depth</code> <code>Optional[int]</code> <p>The maximum depth to look into subdirectories. 0 = no recursion, 1 = include files 1 directory below the FileStore, etc. None (default) will scan all files below the FileStore root directory, regardless of depth.</p> <code>None</code> <code>read_only</code> <code>bool</code> <p>If True (default), the .update() and .remove_docs() methods are disabled, preventing any changes to the files on disk. In addition, metadata cannot be written to disk.</p> <code>True</code> <code>include_orphans</code> <code>bool</code> <p>Whether to include orphaned metadata records in query results. Orphaned metadata records are records found in the local JSON file that can no longer be associated to a file on disk. This can happen if a file is renamed or deleted, or if the FileStore is re-initialized with a more restrictive file_filters or max_depth argument. By default (False), these records do not appear in query results. Nevertheless, the metadata records are retained in the JSON file and the FileStore to prevent accidental data loss.</p> <code>False</code> <code>json_name</code> <code>str</code> <p>Name of the .json file to which metadata is saved. If read_only is False, this file will be created in the root directory of the FileStore.</p> <code>'FileStore.json'</code> <code>encoding</code> <code>Optional[str]</code> <p>Character encoding of files to be tracked by the store. The default (None) follows python's default behavior, which is to determine the character encoding from the platform. This should work in the great majority of cases. However, if you encounter a UnicodeDecodeError, consider setting the encoding explicitly to 'utf8' or another encoding as appropriate.</p> <code>None</code> <code>kwargs</code> <p>kwargs passed to MemoryStore.init()</p> <code>{}</code> Source code in <code>src/maggma/stores/file_store.py</code> <pre><code>def __init__(\n    self,\n    path: Union[str, Path],\n    file_filters: Optional[list] = None,\n    max_depth: Optional[int] = None,\n    read_only: bool = True,\n    include_orphans: bool = False,\n    json_name: str = \"FileStore.json\",\n    encoding: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Initializes a FileStore.\n\n    Args:\n        path: parent directory containing all files and subdirectories to process\n        file_filters: List of fnmatch patterns defining the files to be tracked by\n            the FileStore. Only files that match one of the patterns  provided will\n            be included in the Store If None (default), all files are included.\n\n            Examples: [\"*.txt\", \"test-[abcd].txt\"], etc.\n            See https://docs.python.org/3/library/fnmatch.html for full syntax\n        max_depth: The maximum depth to look into subdirectories. 0 = no recursion,\n            1 = include files 1 directory below the FileStore, etc.\n            None (default) will scan all files below\n            the FileStore root directory, regardless of depth.\n        read_only: If True (default), the .update() and .remove_docs()\n            methods are disabled, preventing any changes to the files on\n            disk. In addition, metadata cannot be written to disk.\n        include_orphans: Whether to include orphaned metadata records in query results.\n            Orphaned metadata records are records found in the local JSON file that can\n            no longer be associated to a file on disk. This can happen if a file is renamed\n            or deleted, or if the FileStore is re-initialized with a more restrictive\n            file_filters or max_depth argument. By default (False), these records\n            do not appear in query results. Nevertheless, the metadata records are\n            retained in the JSON file and the FileStore to prevent accidental data loss.\n        json_name: Name of the .json file to which metadata is saved. If read_only\n            is False, this file will be created in the root directory of the\n            FileStore.\n        encoding: Character encoding of files to be tracked by the store. The default\n            (None) follows python's default behavior, which is to determine the character\n            encoding from the platform. This should work in the great majority of cases.\n            However, if you encounter a UnicodeDecodeError, consider setting the encoding\n            explicitly to 'utf8' or another encoding as appropriate.\n        kwargs: kwargs passed to MemoryStore.__init__()\n    \"\"\"\n    # this conditional block is needed in order to guarantee that the 'name'\n    # property, which is passed to `MemoryStore`, works correctly\n    # collection names passed to MemoryStore cannot end with '.'\n    if path == \".\":\n        path = Path.cwd()\n    self.path = Path(path) if isinstance(path, str) else path\n\n    self.json_name = json_name\n    file_filters = file_filters if file_filters else [\"*\"]\n    self.file_filters = re.compile(\"|\".join(fnmatch.translate(p) for p in file_filters))\n    self.collection_name = \"file_store\"\n    self.key = \"file_id\"\n    self.include_orphans = include_orphans\n    self.read_only = read_only\n    self.max_depth = max_depth\n    self.encoding = encoding\n\n    self.metadata_store = JSONStore(\n        paths=[str(self.path / self.json_name)],\n        read_only=self.read_only,\n        collection_name=self.collection_name,\n        key=self.key,\n    )\n\n    self.kwargs = kwargs\n\n    super().__init__(\n        collection_name=self.collection_name,\n        key=self.key,\n        **self.kwargs,\n    )\n</code></pre>"},{"location":"reference/stores/#maggma.stores.file_store.FileStore.add_metadata","title":"<code>add_metadata(metadata=None, query=None, auto_data=None, **kwargs)</code>","text":"<p>Add metadata to a record in the FileStore, either manually or by computing it automatically from another field, such as name or path (see auto_data).</p> <p>Parameters:</p> Name Type Description Default <code>metadata</code> <code>Optional[dict]</code> <p>dict of additional data to add to the records returned by query.       Note that any protected keys (such as 'name', 'path', etc.)       will be ignored.</p> <code>None</code> <code>query</code> <code>Optional[dict]</code> <p>Query passed to FileStore.query()</p> <code>None</code> <code>auto_data</code> <code>Optional[Callable[[dict], dict]]</code> <p>A function that automatically computes metadata based on a field in     the record itself. The function must take in the item as a dict and     return a dict containing the desired metadata. A typical use case is     to assign metadata based on the name of a file. For example, for     data files named like <code>2022-04-01_april_fool_experiment.txt</code>, the     auto_data function could be:</p> <pre><code>def get_metadata_from_filename(d):\n    return {\"date\": d[\"name\"].split(\"_\")[0],\n            \"test_name\": d[\"name\"].split(\"_\")[1]\n            }\n\nNote that in the case of conflict between manual and automatically\ncomputed metadata (for example, if metadata={\"name\": \"another_name\"} was\nsupplied alongside the auto_data function above), the manually-supplied\nmetadata is used.\n</code></pre> <code>None</code> <code>kwargs</code> <p>kwargs passed to FileStore.query()</p> <code>{}</code> Source code in <code>src/maggma/stores/file_store.py</code> <pre><code>def add_metadata(\n    self,\n    metadata: Optional[dict] = None,\n    query: Optional[dict] = None,\n    auto_data: Optional[Callable[[dict], dict]] = None,\n    **kwargs,\n):\n    \"\"\"\n    Add metadata to a record in the FileStore, either manually or by computing it automatically\n    from another field, such as name or path (see auto_data).\n\n    Args:\n        metadata: dict of additional data to add to the records returned by query.\n                  Note that any protected keys (such as 'name', 'path', etc.)\n                  will be ignored.\n        query: Query passed to FileStore.query()\n        auto_data: A function that automatically computes metadata based on a field in\n                the record itself. The function must take in the item as a dict and\n                return a dict containing the desired metadata. A typical use case is\n                to assign metadata based on the name of a file. For example, for\n                data files named like `2022-04-01_april_fool_experiment.txt`, the\n                auto_data function could be:\n\n                def get_metadata_from_filename(d):\n                    return {\"date\": d[\"name\"].split(\"_\")[0],\n                            \"test_name\": d[\"name\"].split(\"_\")[1]\n                            }\n\n                Note that in the case of conflict between manual and automatically\n                computed metadata (for example, if metadata={\"name\": \"another_name\"} was\n                supplied alongside the auto_data function above), the manually-supplied\n                metadata is used.\n        kwargs: kwargs passed to FileStore.query()\n    \"\"\"\n    if metadata is None:\n        metadata = {}\n    # sanitize the metadata\n    filtered_metadata = self._filter_data(metadata)\n    updated_docs = []\n\n    for doc in self.query(query, **kwargs):\n        if auto_data:\n            extra_data = self._filter_data(auto_data(doc))\n            doc.update(extra_data)\n        doc.update(filtered_metadata)\n        updated_docs.append(doc)\n\n    self.update(updated_docs, key=self.key)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.file_store.FileStore.connect","title":"<code>connect(force_reset=False)</code>","text":"<p>Connect to the source data.</p> <p>Read all the files in the directory, create corresponding File items in the internal MemoryStore.</p> <p>If there is a metadata .json file in the directory, read its contents into the MemoryStore</p> <p>Parameters:</p> Name Type Description Default <code>force_reset</code> <code>bool</code> <p>whether to reset the connection or not when the Store is already connected.</p> <code>False</code> Source code in <code>src/maggma/stores/file_store.py</code> <pre><code>def connect(self, force_reset: bool = False):\n    \"\"\"\n    Connect to the source data.\n\n    Read all the files in the directory, create corresponding File\n    items in the internal MemoryStore.\n\n    If there is a metadata .json file in the directory, read its\n    contents into the MemoryStore\n\n    Args:\n        force_reset: whether to reset the connection or not when the Store is\n            already connected.\n    \"\"\"\n    # read all files and place them in the MemoryStore\n    # use super.update to bypass the read_only guard statement\n    # because we want the file data to be populated in memory\n    super().connect(force_reset=force_reset)\n    super().update(self.read())\n\n    # now read any metadata from the .json file\n    try:\n        self.metadata_store.connect(force_reset=force_reset)\n        metadata = list(self.metadata_store.query())\n    except FileNotFoundError:\n        metadata = []\n        warnings.warn(\n            f\"\"\"\n            JSON file '{self.json_name}' not found. To create this file automatically, re-initialize\n            the FileStore with read_only=False.\n            \"\"\"\n        )\n\n    # merge metadata with file data and check for orphaned metadata\n    requests = []\n    found_orphans = False\n    key = self.key\n    file_ids = self.distinct(self.key)\n    for d in metadata:\n        search_doc = {k: d[k] for k in key} if isinstance(key, list) else {key: d[key]}\n\n        if d[key] not in file_ids:\n            found_orphans = True\n            d.update({\"orphan\": True})\n\n        del d[\"_id\"]\n\n        requests.append(UpdateOne(search_doc, {\"$set\": d}, upsert=True))\n\n    if found_orphans:\n        warnings.warn(\n            f\"Orphaned metadata was found in {self.json_name}. This metadata\"\n            \"will be added to the store with {'orphan': True}\"\n        )\n    if len(requests) &gt; 0:\n        self._collection.bulk_write(requests, ordered=False)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.file_store.FileStore.query","title":"<code>query(criteria=None, properties=None, sort=None, hint=None, skip=0, limit=0, contents_size_limit=0)</code>","text":"<p>Queries the Store for a set of documents.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>hint</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of indexes to use as hints for query optimizer. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> <code>contents_size_limit</code> <code>Optional[int]</code> <p>Maximum file size in bytes for which to return contents. The FileStore will attempt to read the file and populate the 'contents' key with its content at query time, unless the file size is larger than this value. By default, reading content is disabled. Note that enabling content reading can substantially slow down the query operation, especially when there are large numbers of files.</p> <code>0</code> Source code in <code>src/maggma/stores/file_store.py</code> <pre><code>def query(  # type: ignore\n    self,\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    hint: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n    contents_size_limit: Optional[int] = 0,\n) -&gt; Iterator[dict]:\n    \"\"\"\n    Queries the Store for a set of documents.\n\n    Args:\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        hint: Dictionary of indexes to use as hints for query optimizer.\n            Keys are field names and values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n        contents_size_limit: Maximum file size in bytes for which to return contents.\n            The FileStore will attempt to read the file and populate the 'contents' key\n            with its content at query time, unless the file size is larger than this value.\n            By default, reading content is disabled. Note that enabling content reading\n            can substantially slow down the query operation, especially when there\n            are large numbers of files.\n    \"\"\"\n    return_contents = False\n    criteria = criteria if criteria else {}\n    if criteria.get(\"orphan\", None) is None and not self.include_orphans:\n        criteria.update({\"orphan\": False})\n\n    if criteria.get(\"contents\"):\n        warnings.warn(\"'contents' is not a queryable field! Ignoring.\")\n\n    if isinstance(properties, list):\n        properties = {p: 1 for p in properties}\n\n    orig_properties = properties.copy() if properties else None\n\n    if properties is None:\n        # None means return all fields, including contents\n        return_contents = True\n    elif properties.get(\"contents\"):\n        return_contents = True\n        # remove contents b/c it isn't stored in the MemoryStore\n        properties.pop(\"contents\")\n        # add size and path to query so that file can be read\n        properties.update({\"size\": 1})\n        properties.update({\"path\": 1})\n\n    for d in super().query(\n        criteria=criteria,\n        properties=properties,\n        sort=sort,\n        hint=hint,\n        skip=skip,\n        limit=limit,\n    ):\n        # add file contents to the returned documents, if appropriate\n        if return_contents and not d.get(\"orphan\"):\n            if contents_size_limit is None or d[\"size\"] &lt;= contents_size_limit:\n                # attempt to read the file contents and inject into the document\n                # TODO - could add more logic for detecting different file types\n                # and more nuanced exception handling\n                try:\n                    with zopen(d[\"path\"], mode=\"rt\", encoding=self.encoding) as f:\n                        data = f.read()\n                except Exception as e:\n                    data = f\"Unable to read: {e}\"\n\n            elif d[\"size\"] &gt; contents_size_limit:\n                data = f\"File exceeds size limit of {contents_size_limit} bytes\"\n            else:\n                data = \"Unable to read: Unknown error\"\n\n            d.update({\"contents\": data})\n\n            # remove size and path if not explicitly requested\n            if orig_properties is not None and \"size\" not in orig_properties:\n                d.pop(\"size\")\n            if orig_properties is not None and \"path\" not in orig_properties:\n                d.pop(\"path\")\n\n        yield d\n</code></pre>"},{"location":"reference/stores/#maggma.stores.file_store.FileStore.query_one","title":"<code>query_one(criteria=None, properties=None, sort=None, contents_size_limit=None)</code>","text":"<p>Queries the Store for a single document.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in the document</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>contents_size_limit</code> <code>Optional[int]</code> <p>Maximum file size in bytes for which to return contents. The FileStore will attempt to read the file and populate the 'contents' key with its content at query time, unless the file size is larger than this value.</p> <code>None</code> Source code in <code>src/maggma/stores/file_store.py</code> <pre><code>def query_one(\n    self,\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    contents_size_limit: Optional[int] = None,\n):\n    \"\"\"\n    Queries the Store for a single document.\n\n    Args:\n        criteria: PyMongo filter for documents to search\n        properties: properties to return in the document\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        contents_size_limit: Maximum file size in bytes for which to return contents.\n            The FileStore will attempt to read the file and populate the 'contents' key\n            with its content at query time, unless the file size is larger than this value.\n    \"\"\"\n    return next(\n        self.query(\n            criteria=criteria,\n            properties=properties,\n            sort=sort,\n            contents_size_limit=contents_size_limit,\n        ),\n        None,\n    )\n</code></pre>"},{"location":"reference/stores/#maggma.stores.file_store.FileStore.read","title":"<code>read()</code>","text":"<p>Iterate through all files in the Store folder and populate the Store with dictionaries containing basic information about each file.</p> <p>The keys of the documents added to the Store are:</p> <ul> <li>name: str = File name</li> <li>path: Path = Absolute path of this file</li> <li>parent: str = Name of the parent directory (if any)</li> <li>file_id: str = Unique identifier for this file, computed from the hash             of its path relative to the base FileStore directory and             the file creation time. The key of this field is 'file_id'             by default but can be changed via the 'key' kwarg to             <code>FileStore.__init__()</code>.</li> <li>size: int = Size of this file in bytes</li> <li>last_updated: datetime = Time this file was last modified</li> <li>hash: str = Hash of the file contents</li> <li>orphan: bool = Whether this record is an orphan</li> </ul> Source code in <code>src/maggma/stores/file_store.py</code> <pre><code>def read(self) -&gt; list[dict]:\n    \"\"\"\n    Iterate through all files in the Store folder and populate\n    the Store with dictionaries containing basic information about each file.\n\n    The keys of the documents added to the Store are:\n\n    - name: str = File name\n    - path: Path = Absolute path of this file\n    - parent: str = Name of the parent directory (if any)\n    - file_id: str = Unique identifier for this file, computed from the hash\n                of its path relative to the base FileStore directory and\n                the file creation time. The key of this field is 'file_id'\n                by default but can be changed via the 'key' kwarg to\n                `FileStore.__init__()`.\n    - size: int = Size of this file in bytes\n    - last_updated: datetime = Time this file was last modified\n    - hash: str = Hash of the file contents\n    - orphan: bool = Whether this record is an orphan\n    \"\"\"\n    file_list = []\n    # generate a list of files in subdirectories\n    for root, _dirs, files in os.walk(self.path):\n        # for pattern in self.file_filters:\n        for match in filter(self.file_filters.match, files):\n            # for match in fnmatch.filter(files, pattern):\n            path = Path(os.path.join(root, match))\n            # ignore the .json file created by the Store\n            if path.is_file() and path.name != self.json_name:\n                # filter based on depth\n                depth = len(path.relative_to(self.path).parts) - 1\n                if self.max_depth is None or depth &lt;= self.max_depth:\n                    file_list.append(self._create_record_from_file(path))\n\n    return file_list\n</code></pre>"},{"location":"reference/stores/#maggma.stores.file_store.FileStore.remove_docs","title":"<code>remove_docs(criteria, confirm=False)</code>","text":"<p>Remove items matching the query dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>dict</code> <p>query dictionary to match</p> required <code>confirm</code> <code>bool</code> <p>Boolean flag to confirm that remove_docs should delete      files on disk. Default: False.</p> <code>False</code> Source code in <code>src/maggma/stores/file_store.py</code> <pre><code>def remove_docs(self, criteria: dict, confirm: bool = False):\n    \"\"\"\n    Remove items matching the query dictionary.\n\n    Args:\n        criteria: query dictionary to match\n        confirm: Boolean flag to confirm that remove_docs should delete\n                 files on disk. Default: False.\n    \"\"\"\n    if self.read_only:\n        raise StoreError(\n            \"This Store is read-only. To enable file I/O, re-initialize the store with read_only=False.\"\n        )\n\n    docs = list(self.query(criteria))\n    # this ensures that any modifications to criteria made by self.query\n    # (e.g., related to orphans or contents) are propagated through to the superclass\n    new_criteria = {\"file_id\": {\"$in\": [d[\"file_id\"] for d in docs]}}\n\n    if len(docs) &gt; 0 and not confirm:\n        raise StoreError(\n            f\"Warning! This command is about to delete {len(docs)} items from disk! \"\n            \"If this is what you want, reissue this command with confirm=True.\"\n        )\n\n    for d in docs:\n        Path(d[\"path\"]).unlink()\n        super().remove_docs(criteria=new_criteria)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.file_store.FileStore.update","title":"<code>update(docs, key=None)</code>","text":"<p>Update items in the Store. Only possible if the store is not read only. Any new fields that are added will be written to the JSON file in the root directory of the FileStore.</p> <p>Note that certain fields that come from file metadata on disk are protected and cannot be updated with this method. This prevents the contents of the FileStore from becoming out of sync with the files on which it is based. The protected fields are keys in the dict returned by _create_record_from_file, e.g. 'name', 'parent', 'path', 'last_updated', 'hash', 'size', 'contents', and 'orphan'. The 'path_relative' and key fields are retained to make each document in the JSON file identifiable by manual inspection.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[list[dict], dict]</code> <p>the document or list of documents to update</p> required <code>key</code> <code>Union[list, str, None]</code> <p>field name(s) to determine uniqueness for a  document, can be a list of multiple fields,  a single field, or None if the Store's key  field is to be used</p> <code>None</code> Source code in <code>src/maggma/stores/file_store.py</code> <pre><code>def update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n    \"\"\"\n    Update items in the Store. Only possible if the store is not read only. Any new\n    fields that are added will be written to the JSON file in the root directory\n    of the FileStore.\n\n    Note that certain fields that come from file metadata on disk are protected and\n    cannot be updated with this method. This prevents the contents of the FileStore\n    from becoming out of sync with the files on which it is based. The protected fields\n    are keys in the dict returned by _create_record_from_file, e.g. 'name', 'parent',\n    'path', 'last_updated', 'hash', 'size', 'contents', and 'orphan'. The 'path_relative' and key fields are\n    retained to make each document in the JSON file identifiable by manual inspection.\n\n    Args:\n        docs: the document or list of documents to update\n        key: field name(s) to determine uniqueness for a\n             document, can be a list of multiple fields,\n             a single field, or None if the Store's key\n             field is to be used\n    \"\"\"\n    if self.read_only:\n        raise StoreError(\n            \"This Store is read-only. To enable file I/O, re-initialize the store with read_only=False.\"\n        )\n\n    super().update(docs, key)\n    data = list(self.query())\n    filtered_data = []\n    # remove fields that are populated by .read()\n    for d in data:\n        filtered_d = self._filter_data(d)\n        # don't write records that contain only file_id\n        if len(set(filtered_d.keys()).difference({\"path_relative\", self.key})) != 0:\n            filtered_data.append(filtered_d)\n    self.metadata_store.update(filtered_data, self.key)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore","title":"<code>GridFSStore</code>","text":"<p>               Bases: <code>Store</code></p> <p>A Store for GridFS backend. Provides a common access method consistent with other stores.</p> Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>class GridFSStore(Store):\n    \"\"\"\n    A Store for GridFS backend. Provides a common access method consistent with other stores.\n    \"\"\"\n\n    def __init__(\n        self,\n        database: str,\n        collection_name: str,\n        host: str = \"localhost\",\n        port: int = 27017,\n        username: str = \"\",\n        password: str = \"\",\n        compression: bool = False,\n        ensure_metadata: bool = False,\n        searchable_fields: Optional[list[str]] = None,\n        auth_source: Optional[str] = None,\n        mongoclient_kwargs: Optional[dict] = None,\n        ssh_tunnel: Optional[SSHTunnel] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes a GridFS Store for binary data\n        Args:\n            database: database name\n            collection_name: The name of the collection.\n                This is the string portion before the GridFS extensions\n            host: hostname for the database\n            port: port to connect to\n            username: username to connect as\n            password: password to authenticate as\n            compression: compress the data as it goes into GridFS\n            ensure_metadata: ensure returned documents have the metadata fields\n            searchable_fields: fields to keep in the index store\n            auth_source: The database to authenticate on. Defaults to the database name.\n            ssh_tunnel: An SSHTunnel object to use.\n        \"\"\"\n        self.database = database\n        self.collection_name = collection_name\n        self.host = host\n        self.port = port\n        self.username = username\n        self.password = password\n        self._coll: Any = None\n        self.compression = compression\n        self.ensure_metadata = ensure_metadata\n        self.searchable_fields = [] if searchable_fields is None else searchable_fields\n        self.kwargs = kwargs\n        self.ssh_tunnel = ssh_tunnel\n        self._fs = None\n\n        if auth_source is None:\n            auth_source = self.database\n        self.auth_source = auth_source\n        self.mongoclient_kwargs = mongoclient_kwargs or {}\n\n        if \"key\" not in kwargs:\n            kwargs[\"key\"] = \"_id\"\n        super().__init__(**kwargs)\n\n    @classmethod\n    def from_launchpad_file(cls, lp_file, collection_name, **kwargs):\n        \"\"\"\n        Convenience method to construct a GridFSStore from a launchpad file.\n\n        Note: A launchpad file is a special formatted yaml file used in fireworks\n\n        Returns:\n        \"\"\"\n        with open(lp_file) as f:\n            yaml = YAML(typ=\"safe\", pure=True)\n            lp_creds = yaml.load(f.read())\n\n        db_creds = lp_creds.copy()\n        db_creds[\"database\"] = db_creds[\"name\"]\n        for key in list(db_creds.keys()):\n            if key not in [\"database\", \"host\", \"port\", \"username\", \"password\"]:\n                db_creds.pop(key)\n        db_creds[\"collection_name\"] = collection_name\n\n        return cls(**db_creds, **kwargs)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        Return a string representing this data source.\n        \"\"\"\n        return f\"gridfs://{self.host}/{self.database}/{self.collection_name}\"\n\n    def connect(self, force_reset: bool = False):\n        \"\"\"\n        Connect to the source data.\n\n        Args:\n            force_reset: whether to reset the connection or not when the Store is\n                already connected.\n        \"\"\"\n        if not self._coll or force_reset:\n            if self.ssh_tunnel is None:\n                host = self.host\n                port = self.port\n            else:\n                self.ssh_tunnel.start()\n                host, port = self.ssh_tunnel.local_address\n\n            conn: MongoClient = (\n                MongoClient(\n                    host=host,\n                    port=port,\n                    username=self.username,\n                    password=self.password,\n                    authSource=self.auth_source,\n                    **self.mongoclient_kwargs,\n                )\n                if self.username != \"\"\n                else MongoClient(host, port, **self.mongoclient_kwargs)\n            )\n            db = conn[self.database]\n            self._coll = gridfs.GridFS(db, self.collection_name)\n            self._files_collection = db[f\"{self.collection_name}.files\"]\n            self._fs = MongoStore.from_collection(self._files_collection)\n            self._fs.last_updated_field = f\"metadata.{self.last_updated_field}\"\n            self._fs.key = self.key\n            self._chunks_collection = db[f\"{self.collection_name}.chunks\"]\n\n    @property\n    def _collection(self):\n        \"\"\"Property referring to underlying pymongo collection.\"\"\"\n        if self._coll is None:\n            raise StoreError(\"Must connect Mongo-like store before attempting to use it\")\n        return self._coll\n\n    @property\n    def _files_store(self):\n        \"\"\"Property referring to MongoStore associated to the files_collection.\"\"\"\n        if self._fs is None:\n            raise StoreError(\"Must connect Mongo-like store before attempting to use it\")\n        return self._fs\n\n    @property\n    def last_updated(self) -&gt; datetime:\n        \"\"\"\n        Provides the most recent last_updated date time stamp from\n        the documents in this Store.\n        \"\"\"\n        return self._files_store.last_updated\n\n    @classmethod\n    def transform_criteria(cls, criteria: dict) -&gt; dict:\n        \"\"\"\n        Allow client to not need to prepend 'metadata.' to query fields.\n\n        Args:\n            criteria: Query criteria\n        \"\"\"\n        new_criteria = dict()\n        for field in criteria:\n            if field not in files_collection_fields and not field.startswith(\"metadata.\"):\n                new_criteria[\"metadata.\" + field] = copy.copy(criteria[field])\n            else:\n                new_criteria[field] = copy.copy(criteria[field])\n\n        return new_criteria\n\n    def count(self, criteria: Optional[dict] = None) -&gt; int:\n        \"\"\"\n        Counts the number of documents matching the query criteria.\n\n        Args:\n            criteria: PyMongo filter for documents to count in\n        \"\"\"\n        if isinstance(criteria, dict):\n            criteria = self.transform_criteria(criteria)\n\n        return self._files_store.count(criteria)\n\n    def query(\n        self,\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[dict]:\n        \"\"\"\n        Queries the GridFS Store for a set of documents.\n        Will check to see if data can be returned from\n        files store first.\n        If the data from the gridfs is not a json serialized string\n        a dict will be returned with the data in the \"data\" key\n        plus the self.key and self.last_updated_field.\n\n        Args:\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n        \"\"\"\n        if isinstance(criteria, dict):\n            criteria = self.transform_criteria(criteria)\n        elif criteria is not None:\n            raise ValueError(\"Criteria must be a dictionary or None\")\n\n        prop_keys = set()\n        if isinstance(properties, dict):\n            prop_keys = set(properties.keys())\n        elif isinstance(properties, list):\n            prop_keys = set(properties)\n\n        for doc in self._files_store.query(criteria=criteria, sort=sort, limit=limit, skip=skip):\n            if properties is not None and prop_keys.issubset(set(doc.keys())):\n                yield {p: doc[p] for p in properties if p in doc}\n            else:\n                metadata = doc.get(\"metadata\", {})\n\n                data = self._collection.find_one(\n                    filter={\"_id\": doc[\"_id\"]},\n                    skip=skip,\n                    limit=limit,\n                    sort=sort,\n                ).read()\n\n                if metadata.get(\"compression\", \"\") == \"zlib\":\n                    data = zlib.decompress(data).decode(\"UTF-8\")\n\n                try:\n                    data = json.loads(data)\n                except Exception:\n                    if not isinstance(data, dict):\n                        data = {\n                            \"data\": data,\n                            self.key: doc.get(self.key),\n                            self.last_updated_field: doc.get(self.last_updated_field),\n                        }\n\n                if self.ensure_metadata and isinstance(data, dict):\n                    data.update(metadata)\n\n                yield data\n\n    def distinct(self, field: str, criteria: Optional[dict] = None, all_exist: bool = False) -&gt; list:\n        \"\"\"\n        Get all distinct values for a field. This function only operates\n        on the metadata in the files collection.\n\n        Args:\n            field: the field(s) to get distinct values for\n            criteria: PyMongo filter for documents to search in\n        \"\"\"\n        criteria = self.transform_criteria(criteria) if isinstance(criteria, dict) else criteria\n\n        field = (\n            f\"metadata.{field}\" if field not in files_collection_fields and not field.startswith(\"metadata.\") else field\n        )\n\n        return self._files_store.distinct(field=field, criteria=criteria)\n\n    def groupby(\n        self,\n        keys: Union[list[str], str],\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[tuple[dict, list[dict]]]:\n        \"\"\"\n        Simple grouping function that will group documents\n        by keys. Will only work if the keys are included in the files\n        collection for GridFS.\n\n        Args:\n            keys: fields to group documents\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n\n        Returns:\n            generator returning tuples of (dict, list of docs)\n        \"\"\"\n        criteria = self.transform_criteria(criteria) if isinstance(criteria, dict) else criteria\n        keys = [keys] if not isinstance(keys, list) else keys\n        keys = [\n            f\"metadata.{k}\" if k not in files_collection_fields and not k.startswith(\"metadata.\") else k for k in keys\n        ]\n        for group, ids in self._files_store.groupby(keys, criteria=criteria, properties=[f\"metadata.{self.key}\"]):\n            ids = [get(doc, f\"metadata.{self.key}\") for doc in ids if has(doc, f\"metadata.{self.key}\")]\n\n            group = {k.replace(\"metadata.\", \"\"): get(group, k) for k in keys if has(group, k)}\n\n            yield group, list(self.query(criteria={self.key: {\"$in\": ids}}))\n\n    def ensure_index(self, key: str, unique: Optional[bool] = False) -&gt; bool:\n        \"\"\"\n        Tries to create an index and return true if it succeeded\n        Currently operators on the GridFS files collection\n        Args:\n            key: single key to index\n            unique: Whether or not this index contains only unique keys.\n\n        Returns:\n            bool indicating if the index exists/was created\n        \"\"\"\n        # Transform key for gridfs first\n        if key not in files_collection_fields:\n            files_col_key = f\"metadata.{key}\"\n            return self._files_store.ensure_index(files_col_key, unique=unique)\n        return self._files_store.ensure_index(key, unique=unique)\n\n    def update(\n        self,\n        docs: Union[list[dict], dict],\n        key: Union[list, str, None] = None,\n        additional_metadata: Union[str, list[str], None] = None,\n    ):\n        \"\"\"\n        Update documents into the Store.\n\n        Args:\n            docs: the document or list of documents to update\n            key: field name(s) to determine uniqueness for a\n                 document, can be a list of multiple fields,\n                 a single field, or None if the Store's key\n                 field is to be used\n            additional_metadata: field(s) to include in the gridfs metadata\n        \"\"\"\n        if not isinstance(docs, list):\n            docs = [docs]\n\n        if isinstance(key, str):\n            key = [key]\n        elif not key:\n            key = [self.key]\n\n        key = list(set(key) - set(files_collection_fields))\n\n        if additional_metadata is None:\n            additional_metadata = []\n        elif isinstance(additional_metadata, str):\n            additional_metadata = [additional_metadata]\n        else:\n            additional_metadata = list(additional_metadata)\n\n        for d in docs:\n            search_doc = {k: d[k] for k in key}\n\n            metadata = {\n                k: get(d, k)\n                for k in [self.last_updated_field, *additional_metadata, *self.searchable_fields]\n                if has(d, k)\n            }\n            metadata.update(search_doc)\n            data = json.dumps(jsanitize(d, recursive_msonable=True)).encode(\"UTF-8\")\n            if self.compression:\n                data = zlib.compress(data)\n                metadata[\"compression\"] = \"zlib\"\n\n            self._collection.put(data, metadata=metadata)\n            search_doc = self.transform_criteria(search_doc)\n\n            # Cleans up old gridfs entries\n            for fdoc in self._files_collection.find(search_doc, [\"_id\"]).sort(\"uploadDate\", -1).skip(1):\n                self._collection.delete(fdoc[\"_id\"])\n\n    def remove_docs(self, criteria: dict):\n        \"\"\"\n        Remove docs matching the query dictionary.\n\n        Args:\n            criteria: query dictionary to match\n        \"\"\"\n        if isinstance(criteria, dict):\n            criteria = self.transform_criteria(criteria)\n        ids = [cursor._id for cursor in self._collection.find(criteria)]\n\n        for _id in ids:\n            self._collection.delete(_id)\n\n    def close(self):\n        self._files_store.close()\n        self._coll = None\n        if self.ssh_tunnel is not None:\n            self.ssh_tunnel.stop()\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"\n        Check equality for GridFSStore\n        other: other GridFSStore to compare with.\n        \"\"\"\n        if not isinstance(other, GridFSStore):\n            return False\n\n        fields = [\"database\", \"collection_name\", \"host\", \"port\"]\n        return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.last_updated","title":"<code>last_updated</code>  <code>property</code>","text":"<p>Provides the most recent last_updated date time stamp from the documents in this Store.</p>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return a string representing this data source.</p>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality for GridFSStore other: other GridFSStore to compare with.</p> Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"\n    Check equality for GridFSStore\n    other: other GridFSStore to compare with.\n    \"\"\"\n    if not isinstance(other, GridFSStore):\n        return False\n\n    fields = [\"database\", \"collection_name\", \"host\", \"port\"]\n    return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.__init__","title":"<code>__init__(database, collection_name, host='localhost', port=27017, username='', password='', compression=False, ensure_metadata=False, searchable_fields=None, auth_source=None, mongoclient_kwargs=None, ssh_tunnel=None, **kwargs)</code>","text":"<p>Initializes a GridFS Store for binary data Args:     database: database name     collection_name: The name of the collection.         This is the string portion before the GridFS extensions     host: hostname for the database     port: port to connect to     username: username to connect as     password: password to authenticate as     compression: compress the data as it goes into GridFS     ensure_metadata: ensure returned documents have the metadata fields     searchable_fields: fields to keep in the index store     auth_source: The database to authenticate on. Defaults to the database name.     ssh_tunnel: An SSHTunnel object to use.</p> Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>def __init__(\n    self,\n    database: str,\n    collection_name: str,\n    host: str = \"localhost\",\n    port: int = 27017,\n    username: str = \"\",\n    password: str = \"\",\n    compression: bool = False,\n    ensure_metadata: bool = False,\n    searchable_fields: Optional[list[str]] = None,\n    auth_source: Optional[str] = None,\n    mongoclient_kwargs: Optional[dict] = None,\n    ssh_tunnel: Optional[SSHTunnel] = None,\n    **kwargs,\n):\n    \"\"\"\n    Initializes a GridFS Store for binary data\n    Args:\n        database: database name\n        collection_name: The name of the collection.\n            This is the string portion before the GridFS extensions\n        host: hostname for the database\n        port: port to connect to\n        username: username to connect as\n        password: password to authenticate as\n        compression: compress the data as it goes into GridFS\n        ensure_metadata: ensure returned documents have the metadata fields\n        searchable_fields: fields to keep in the index store\n        auth_source: The database to authenticate on. Defaults to the database name.\n        ssh_tunnel: An SSHTunnel object to use.\n    \"\"\"\n    self.database = database\n    self.collection_name = collection_name\n    self.host = host\n    self.port = port\n    self.username = username\n    self.password = password\n    self._coll: Any = None\n    self.compression = compression\n    self.ensure_metadata = ensure_metadata\n    self.searchable_fields = [] if searchable_fields is None else searchable_fields\n    self.kwargs = kwargs\n    self.ssh_tunnel = ssh_tunnel\n    self._fs = None\n\n    if auth_source is None:\n        auth_source = self.database\n    self.auth_source = auth_source\n    self.mongoclient_kwargs = mongoclient_kwargs or {}\n\n    if \"key\" not in kwargs:\n        kwargs[\"key\"] = \"_id\"\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.connect","title":"<code>connect(force_reset=False)</code>","text":"<p>Connect to the source data.</p> <p>Parameters:</p> Name Type Description Default <code>force_reset</code> <code>bool</code> <p>whether to reset the connection or not when the Store is already connected.</p> <code>False</code> Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>def connect(self, force_reset: bool = False):\n    \"\"\"\n    Connect to the source data.\n\n    Args:\n        force_reset: whether to reset the connection or not when the Store is\n            already connected.\n    \"\"\"\n    if not self._coll or force_reset:\n        if self.ssh_tunnel is None:\n            host = self.host\n            port = self.port\n        else:\n            self.ssh_tunnel.start()\n            host, port = self.ssh_tunnel.local_address\n\n        conn: MongoClient = (\n            MongoClient(\n                host=host,\n                port=port,\n                username=self.username,\n                password=self.password,\n                authSource=self.auth_source,\n                **self.mongoclient_kwargs,\n            )\n            if self.username != \"\"\n            else MongoClient(host, port, **self.mongoclient_kwargs)\n        )\n        db = conn[self.database]\n        self._coll = gridfs.GridFS(db, self.collection_name)\n        self._files_collection = db[f\"{self.collection_name}.files\"]\n        self._fs = MongoStore.from_collection(self._files_collection)\n        self._fs.last_updated_field = f\"metadata.{self.last_updated_field}\"\n        self._fs.key = self.key\n        self._chunks_collection = db[f\"{self.collection_name}.chunks\"]\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.count","title":"<code>count(criteria=None)</code>","text":"<p>Counts the number of documents matching the query criteria.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to count in</p> <code>None</code> Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>def count(self, criteria: Optional[dict] = None) -&gt; int:\n    \"\"\"\n    Counts the number of documents matching the query criteria.\n\n    Args:\n        criteria: PyMongo filter for documents to count in\n    \"\"\"\n    if isinstance(criteria, dict):\n        criteria = self.transform_criteria(criteria)\n\n    return self._files_store.count(criteria)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.distinct","title":"<code>distinct(field, criteria=None, all_exist=False)</code>","text":"<p>Get all distinct values for a field. This function only operates on the metadata in the files collection.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>the field(s) to get distinct values for</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>def distinct(self, field: str, criteria: Optional[dict] = None, all_exist: bool = False) -&gt; list:\n    \"\"\"\n    Get all distinct values for a field. This function only operates\n    on the metadata in the files collection.\n\n    Args:\n        field: the field(s) to get distinct values for\n        criteria: PyMongo filter for documents to search in\n    \"\"\"\n    criteria = self.transform_criteria(criteria) if isinstance(criteria, dict) else criteria\n\n    field = (\n        f\"metadata.{field}\" if field not in files_collection_fields and not field.startswith(\"metadata.\") else field\n    )\n\n    return self._files_store.distinct(field=field, criteria=criteria)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.ensure_index","title":"<code>ensure_index(key, unique=False)</code>","text":"<p>Tries to create an index and return true if it succeeded Currently operators on the GridFS files collection Args:     key: single key to index     unique: Whether or not this index contains only unique keys.</p> <p>Returns:</p> Type Description <code>bool</code> <p>bool indicating if the index exists/was created</p> Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>def ensure_index(self, key: str, unique: Optional[bool] = False) -&gt; bool:\n    \"\"\"\n    Tries to create an index and return true if it succeeded\n    Currently operators on the GridFS files collection\n    Args:\n        key: single key to index\n        unique: Whether or not this index contains only unique keys.\n\n    Returns:\n        bool indicating if the index exists/was created\n    \"\"\"\n    # Transform key for gridfs first\n    if key not in files_collection_fields:\n        files_col_key = f\"metadata.{key}\"\n        return self._files_store.ensure_index(files_col_key, unique=unique)\n    return self._files_store.ensure_index(key, unique=unique)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.from_launchpad_file","title":"<code>from_launchpad_file(lp_file, collection_name, **kwargs)</code>  <code>classmethod</code>","text":"<p>Convenience method to construct a GridFSStore from a launchpad file.</p> <p>Note: A launchpad file is a special formatted yaml file used in fireworks</p> <p>Returns:</p> Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>@classmethod\ndef from_launchpad_file(cls, lp_file, collection_name, **kwargs):\n    \"\"\"\n    Convenience method to construct a GridFSStore from a launchpad file.\n\n    Note: A launchpad file is a special formatted yaml file used in fireworks\n\n    Returns:\n    \"\"\"\n    with open(lp_file) as f:\n        yaml = YAML(typ=\"safe\", pure=True)\n        lp_creds = yaml.load(f.read())\n\n    db_creds = lp_creds.copy()\n    db_creds[\"database\"] = db_creds[\"name\"]\n    for key in list(db_creds.keys()):\n        if key not in [\"database\", \"host\", \"port\", \"username\", \"password\"]:\n            db_creds.pop(key)\n    db_creds[\"collection_name\"] = collection_name\n\n    return cls(**db_creds, **kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.groupby","title":"<code>groupby(keys, criteria=None, properties=None, sort=None, skip=0, limit=0)</code>","text":"<p>Simple grouping function that will group documents by keys. Will only work if the keys are included in the files collection for GridFS.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[list[str], str]</code> <p>fields to group documents</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> <p>Returns:</p> Type Description <code>Iterator[tuple[dict, list[dict]]]</code> <p>generator returning tuples of (dict, list of docs)</p> Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>def groupby(\n    self,\n    keys: Union[list[str], str],\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[tuple[dict, list[dict]]]:\n    \"\"\"\n    Simple grouping function that will group documents\n    by keys. Will only work if the keys are included in the files\n    collection for GridFS.\n\n    Args:\n        keys: fields to group documents\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n\n    Returns:\n        generator returning tuples of (dict, list of docs)\n    \"\"\"\n    criteria = self.transform_criteria(criteria) if isinstance(criteria, dict) else criteria\n    keys = [keys] if not isinstance(keys, list) else keys\n    keys = [\n        f\"metadata.{k}\" if k not in files_collection_fields and not k.startswith(\"metadata.\") else k for k in keys\n    ]\n    for group, ids in self._files_store.groupby(keys, criteria=criteria, properties=[f\"metadata.{self.key}\"]):\n        ids = [get(doc, f\"metadata.{self.key}\") for doc in ids if has(doc, f\"metadata.{self.key}\")]\n\n        group = {k.replace(\"metadata.\", \"\"): get(group, k) for k in keys if has(group, k)}\n\n        yield group, list(self.query(criteria={self.key: {\"$in\": ids}}))\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.query","title":"<code>query(criteria=None, properties=None, sort=None, skip=0, limit=0)</code>","text":"<p>Queries the GridFS Store for a set of documents. Will check to see if data can be returned from files store first. If the data from the gridfs is not a json serialized string a dict will be returned with the data in the \"data\" key plus the self.key and self.last_updated_field.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>def query(\n    self,\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[dict]:\n    \"\"\"\n    Queries the GridFS Store for a set of documents.\n    Will check to see if data can be returned from\n    files store first.\n    If the data from the gridfs is not a json serialized string\n    a dict will be returned with the data in the \"data\" key\n    plus the self.key and self.last_updated_field.\n\n    Args:\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n    \"\"\"\n    if isinstance(criteria, dict):\n        criteria = self.transform_criteria(criteria)\n    elif criteria is not None:\n        raise ValueError(\"Criteria must be a dictionary or None\")\n\n    prop_keys = set()\n    if isinstance(properties, dict):\n        prop_keys = set(properties.keys())\n    elif isinstance(properties, list):\n        prop_keys = set(properties)\n\n    for doc in self._files_store.query(criteria=criteria, sort=sort, limit=limit, skip=skip):\n        if properties is not None and prop_keys.issubset(set(doc.keys())):\n            yield {p: doc[p] for p in properties if p in doc}\n        else:\n            metadata = doc.get(\"metadata\", {})\n\n            data = self._collection.find_one(\n                filter={\"_id\": doc[\"_id\"]},\n                skip=skip,\n                limit=limit,\n                sort=sort,\n            ).read()\n\n            if metadata.get(\"compression\", \"\") == \"zlib\":\n                data = zlib.decompress(data).decode(\"UTF-8\")\n\n            try:\n                data = json.loads(data)\n            except Exception:\n                if not isinstance(data, dict):\n                    data = {\n                        \"data\": data,\n                        self.key: doc.get(self.key),\n                        self.last_updated_field: doc.get(self.last_updated_field),\n                    }\n\n            if self.ensure_metadata and isinstance(data, dict):\n                data.update(metadata)\n\n            yield data\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.remove_docs","title":"<code>remove_docs(criteria)</code>","text":"<p>Remove docs matching the query dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>dict</code> <p>query dictionary to match</p> required Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>def remove_docs(self, criteria: dict):\n    \"\"\"\n    Remove docs matching the query dictionary.\n\n    Args:\n        criteria: query dictionary to match\n    \"\"\"\n    if isinstance(criteria, dict):\n        criteria = self.transform_criteria(criteria)\n    ids = [cursor._id for cursor in self._collection.find(criteria)]\n\n    for _id in ids:\n        self._collection.delete(_id)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.transform_criteria","title":"<code>transform_criteria(criteria)</code>  <code>classmethod</code>","text":"<p>Allow client to not need to prepend 'metadata.' to query fields.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>dict</code> <p>Query criteria</p> required Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>@classmethod\ndef transform_criteria(cls, criteria: dict) -&gt; dict:\n    \"\"\"\n    Allow client to not need to prepend 'metadata.' to query fields.\n\n    Args:\n        criteria: Query criteria\n    \"\"\"\n    new_criteria = dict()\n    for field in criteria:\n        if field not in files_collection_fields and not field.startswith(\"metadata.\"):\n            new_criteria[\"metadata.\" + field] = copy.copy(criteria[field])\n        else:\n            new_criteria[field] = copy.copy(criteria[field])\n\n    return new_criteria\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSStore.update","title":"<code>update(docs, key=None, additional_metadata=None)</code>","text":"<p>Update documents into the Store.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[list[dict], dict]</code> <p>the document or list of documents to update</p> required <code>key</code> <code>Union[list, str, None]</code> <p>field name(s) to determine uniqueness for a  document, can be a list of multiple fields,  a single field, or None if the Store's key  field is to be used</p> <code>None</code> <code>additional_metadata</code> <code>Union[str, list[str], None]</code> <p>field(s) to include in the gridfs metadata</p> <code>None</code> Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>def update(\n    self,\n    docs: Union[list[dict], dict],\n    key: Union[list, str, None] = None,\n    additional_metadata: Union[str, list[str], None] = None,\n):\n    \"\"\"\n    Update documents into the Store.\n\n    Args:\n        docs: the document or list of documents to update\n        key: field name(s) to determine uniqueness for a\n             document, can be a list of multiple fields,\n             a single field, or None if the Store's key\n             field is to be used\n        additional_metadata: field(s) to include in the gridfs metadata\n    \"\"\"\n    if not isinstance(docs, list):\n        docs = [docs]\n\n    if isinstance(key, str):\n        key = [key]\n    elif not key:\n        key = [self.key]\n\n    key = list(set(key) - set(files_collection_fields))\n\n    if additional_metadata is None:\n        additional_metadata = []\n    elif isinstance(additional_metadata, str):\n        additional_metadata = [additional_metadata]\n    else:\n        additional_metadata = list(additional_metadata)\n\n    for d in docs:\n        search_doc = {k: d[k] for k in key}\n\n        metadata = {\n            k: get(d, k)\n            for k in [self.last_updated_field, *additional_metadata, *self.searchable_fields]\n            if has(d, k)\n        }\n        metadata.update(search_doc)\n        data = json.dumps(jsanitize(d, recursive_msonable=True)).encode(\"UTF-8\")\n        if self.compression:\n            data = zlib.compress(data)\n            metadata[\"compression\"] = \"zlib\"\n\n        self._collection.put(data, metadata=metadata)\n        search_doc = self.transform_criteria(search_doc)\n\n        # Cleans up old gridfs entries\n        for fdoc in self._files_collection.find(search_doc, [\"_id\"]).sort(\"uploadDate\", -1).skip(1):\n            self._collection.delete(fdoc[\"_id\"])\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSURIStore","title":"<code>GridFSURIStore</code>","text":"<p>               Bases: <code>GridFSStore</code></p> <p>A Store for GridFS backend, with connection via a mongo URI string.</p> <p>This is expected to be a special mongodb+srv:// URIs that include client parameters via TXT records</p> Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>class GridFSURIStore(GridFSStore):\n    \"\"\"\n    A Store for GridFS backend, with connection via a mongo URI string.\n\n    This is expected to be a special mongodb+srv:// URIs that include client parameters\n    via TXT records\n    \"\"\"\n\n    def __init__(\n        self,\n        uri: str,\n        collection_name: str,\n        database: Optional[str] = None,\n        compression: bool = False,\n        ensure_metadata: bool = False,\n        searchable_fields: Optional[list[str]] = None,\n        mongoclient_kwargs: Optional[dict] = None,\n        ssh_tunnel: Optional[SSHTunnel] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes a GridFS Store for binary data.\n\n        Args:\n            uri: MongoDB+SRV URI\n            database: database to connect to\n            collection_name: The collection name\n            compression: compress the data as it goes into GridFS\n            ensure_metadata: ensure returned documents have the metadata fields\n            searchable_fields: fields to keep in the index store.\n        \"\"\"\n        self.uri = uri\n\n        if ssh_tunnel:\n            raise ValueError(f\"At the moment ssh_tunnel is not supported for {self.__class__.__name__}\")\n        self.ssh_tunnel = None\n\n        # parse the dbname from the uri\n        if database is None:\n            d_uri = uri_parser.parse_uri(uri)\n            if d_uri[\"database\"] is None:\n                raise ConfigurationError(\"If database name is not supplied, a database must be set in the uri\")\n            self.database = d_uri[\"database\"]\n        else:\n            self.database = database\n\n        self.collection_name = collection_name\n        self._coll: Any = None\n        self.compression = compression\n        self.ensure_metadata = ensure_metadata\n        self.searchable_fields = [] if searchable_fields is None else searchable_fields\n        self.kwargs = kwargs\n        self.mongoclient_kwargs = mongoclient_kwargs or {}\n        self._fs = None\n\n        if \"key\" not in kwargs:\n            kwargs[\"key\"] = \"_id\"\n        super(GridFSStore, self).__init__(**kwargs)  # lgtm\n\n    def connect(self, force_reset: bool = False):\n        \"\"\"\n        Connect to the source data.\n\n        Args:\n            force_reset: whether to reset the connection or not when the Store is\n                already connected.\n        \"\"\"\n        if not self._coll or force_reset:  # pragma: no cover\n            conn: MongoClient = MongoClient(self.uri, **self.mongoclient_kwargs)\n            db = conn[self.database]\n            self._coll = gridfs.GridFS(db, self.collection_name)\n            self._files_collection = db[f\"{self.collection_name}.files\"]\n            self._fs = MongoStore.from_collection(self._files_collection)\n            self._fs.last_updated_field = f\"metadata.{self.last_updated_field}\"\n            self._fs.key = self.key\n            self._chunks_collection = db[f\"{self.collection_name}.chunks\"]\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        Return a string representing this data source.\n        \"\"\"\n        # TODO: This is not very safe since it exposes the username/password info\n        return self.uri\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"\n        Check equality for GridFSURIStore\n        other: other GridFSURIStore to compare with.\n        \"\"\"\n        if not isinstance(other, GridFSStore):\n            return False\n\n        fields = [\"uri\", \"database\", \"collection_name\"]\n        return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSURIStore.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return a string representing this data source.</p>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSURIStore.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality for GridFSURIStore other: other GridFSURIStore to compare with.</p> Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"\n    Check equality for GridFSURIStore\n    other: other GridFSURIStore to compare with.\n    \"\"\"\n    if not isinstance(other, GridFSStore):\n        return False\n\n    fields = [\"uri\", \"database\", \"collection_name\"]\n    return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSURIStore.__init__","title":"<code>__init__(uri, collection_name, database=None, compression=False, ensure_metadata=False, searchable_fields=None, mongoclient_kwargs=None, ssh_tunnel=None, **kwargs)</code>","text":"<p>Initializes a GridFS Store for binary data.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>MongoDB+SRV URI</p> required <code>database</code> <code>Optional[str]</code> <p>database to connect to</p> <code>None</code> <code>collection_name</code> <code>str</code> <p>The collection name</p> required <code>compression</code> <code>bool</code> <p>compress the data as it goes into GridFS</p> <code>False</code> <code>ensure_metadata</code> <code>bool</code> <p>ensure returned documents have the metadata fields</p> <code>False</code> <code>searchable_fields</code> <code>Optional[list[str]]</code> <p>fields to keep in the index store.</p> <code>None</code> Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>def __init__(\n    self,\n    uri: str,\n    collection_name: str,\n    database: Optional[str] = None,\n    compression: bool = False,\n    ensure_metadata: bool = False,\n    searchable_fields: Optional[list[str]] = None,\n    mongoclient_kwargs: Optional[dict] = None,\n    ssh_tunnel: Optional[SSHTunnel] = None,\n    **kwargs,\n):\n    \"\"\"\n    Initializes a GridFS Store for binary data.\n\n    Args:\n        uri: MongoDB+SRV URI\n        database: database to connect to\n        collection_name: The collection name\n        compression: compress the data as it goes into GridFS\n        ensure_metadata: ensure returned documents have the metadata fields\n        searchable_fields: fields to keep in the index store.\n    \"\"\"\n    self.uri = uri\n\n    if ssh_tunnel:\n        raise ValueError(f\"At the moment ssh_tunnel is not supported for {self.__class__.__name__}\")\n    self.ssh_tunnel = None\n\n    # parse the dbname from the uri\n    if database is None:\n        d_uri = uri_parser.parse_uri(uri)\n        if d_uri[\"database\"] is None:\n            raise ConfigurationError(\"If database name is not supplied, a database must be set in the uri\")\n        self.database = d_uri[\"database\"]\n    else:\n        self.database = database\n\n    self.collection_name = collection_name\n    self._coll: Any = None\n    self.compression = compression\n    self.ensure_metadata = ensure_metadata\n    self.searchable_fields = [] if searchable_fields is None else searchable_fields\n    self.kwargs = kwargs\n    self.mongoclient_kwargs = mongoclient_kwargs or {}\n    self._fs = None\n\n    if \"key\" not in kwargs:\n        kwargs[\"key\"] = \"_id\"\n    super(GridFSStore, self).__init__(**kwargs)  # lgtm\n</code></pre>"},{"location":"reference/stores/#maggma.stores.gridfs.GridFSURIStore.connect","title":"<code>connect(force_reset=False)</code>","text":"<p>Connect to the source data.</p> <p>Parameters:</p> Name Type Description Default <code>force_reset</code> <code>bool</code> <p>whether to reset the connection or not when the Store is already connected.</p> <code>False</code> Source code in <code>src/maggma/stores/gridfs.py</code> <pre><code>def connect(self, force_reset: bool = False):\n    \"\"\"\n    Connect to the source data.\n\n    Args:\n        force_reset: whether to reset the connection or not when the Store is\n            already connected.\n    \"\"\"\n    if not self._coll or force_reset:  # pragma: no cover\n        conn: MongoClient = MongoClient(self.uri, **self.mongoclient_kwargs)\n        db = conn[self.database]\n        self._coll = gridfs.GridFS(db, self.collection_name)\n        self._files_collection = db[f\"{self.collection_name}.files\"]\n        self._fs = MongoStore.from_collection(self._files_collection)\n        self._fs.last_updated_field = f\"metadata.{self.last_updated_field}\"\n        self._fs.key = self.key\n        self._chunks_collection = db[f\"{self.collection_name}.chunks\"]\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store","title":"<code>S3Store</code>","text":"<p>               Bases: <code>Store</code></p> <p>GridFS like storage using Amazon S3 and a regular store for indexing.</p> <p>Assumes Amazon AWS key and secret key are set in environment or default config file.</p> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>class S3Store(Store):\n    \"\"\"\n    GridFS like storage using Amazon S3 and a regular store for indexing.\n\n    Assumes Amazon AWS key and secret key are set in environment or default config file.\n    \"\"\"\n\n    def __init__(\n        self,\n        index: Store,\n        bucket: str,\n        s3_profile: Optional[Union[str, dict]] = None,\n        compress: bool = False,\n        endpoint_url: Optional[str] = None,\n        sub_dir: Optional[str] = None,\n        s3_workers: int = 1,\n        s3_resource_kwargs: Optional[dict] = None,\n        ssh_tunnel: Optional[SSHTunnel] = None,\n        key: str = \"fs_id\",\n        store_hash: bool = True,\n        unpack_data: bool = True,\n        searchable_fields: Optional[list[str]] = None,\n        index_store_kwargs: Optional[dict] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes an S3 Store.\n\n        Args:\n            index: a store to use to index the S3 bucket.\n            bucket: name of the bucket.\n            s3_profile: name of AWS profile containing the credentials. Alternatively\n                you can pass in a dictionary with the full credentials:\n                    aws_access_key_id (string) -- AWS access key ID\n                    aws_secret_access_key (string) -- AWS secret access key\n                    aws_session_token (string) -- AWS temporary session token\n                    region_name (string) -- Default region when creating new connections\n            compress: compress files inserted into the store.\n            endpoint_url: this allows the interface with minio service; ignored if\n                `ssh_tunnel` is provided, in which case it is inferred.\n            sub_dir: subdirectory of the S3 bucket to store the data.\n            s3_workers: number of concurrent S3 puts to run.\n            s3_resource_kwargs: additional kwargs to pass to the boto3 session resource.\n            ssh_tunnel: optional SSH tunnel to use for the S3 connection.\n            key: main key to index on.\n            store_hash: store the SHA1 hash right before insertion to the database.\n            unpack_data: whether to decompress and unpack byte data when querying from\n                the bucket.\n            searchable_fields: fields to keep in the index store.\n            index_store_kwargs: kwargs to pass to the index store. Allows the user to\n                use kwargs here to update the index store.\n        \"\"\"\n        if boto3 is None:\n            raise RuntimeError(\"boto3 and botocore are required for S3Store\")\n        self.index_store_kwargs = index_store_kwargs or {}\n        if index_store_kwargs:\n            d_ = index.as_dict()\n            d_.update(index_store_kwargs)\n            self.index = index.__class__.from_dict(d_)\n        else:\n            self.index = index\n        self.bucket = bucket\n        self.s3_profile = s3_profile\n        self.compress = compress\n        self.endpoint_url = endpoint_url\n        self.sub_dir = sub_dir.strip(\"/\") + \"/\" if sub_dir else \"\"\n        self.s3: Any = None\n        self.s3_bucket: Any = None\n        self.s3_workers = s3_workers\n        self.s3_resource_kwargs = s3_resource_kwargs if s3_resource_kwargs is not None else {}\n        self.ssh_tunnel = ssh_tunnel\n        self.unpack_data = unpack_data\n        self.searchable_fields = searchable_fields if searchable_fields is not None else []\n        self.store_hash = store_hash\n\n        # Force the key to be the same as the index\n        assert isinstance(index.key, str), \"Since we are using the key as a file name in S3, they key must be a string\"\n        if key != index.key:\n            warnings.warn(\n                f'The desired S3Store key \"{key}\" does not match the index key \"{index.key},\"'\n                \"the index key will be used\",\n                UserWarning,\n            )\n        kwargs[\"key\"] = str(index.key)\n\n        self._thread_local = threading.local()\n        super().__init__(**kwargs)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"String representing this data source.\"\"\"\n        return f\"s3://{self.bucket}\"\n\n    def connect(self, force_reset: bool = False):  # lgtm[py/conflicting-attributes]\n        \"\"\"Connect to the source data.\n\n        Args:\n            force_reset: whether to force a reset of the connection\n        \"\"\"\n        if self.s3 is None or force_reset:\n            self.s3, self.s3_bucket = self._get_resource_and_bucket()\n        self.index.connect(force_reset=force_reset)\n\n    def close(self):\n        \"\"\"Closes any connections.\"\"\"\n        self.index.close()\n\n        self.s3.meta.client.close()\n        self.s3 = None\n        self.s3_bucket = None\n\n        if self.ssh_tunnel is not None:\n            self.ssh_tunnel.stop()\n\n    @property\n    def _collection(self):\n        \"\"\"\n        A handle to the pymongo collection object.\n\n        Important:\n            Not guaranteed to exist in the future.\n        \"\"\"\n        # For now returns the index collection since that is what we would \"search\" on\n        return self.index._collection\n\n    def count(self, criteria: Optional[dict] = None) -&gt; int:\n        \"\"\"\n        Counts the number of documents matching the query criteria.\n\n        Args:\n            criteria: PyMongo filter for documents to count in.\n        \"\"\"\n        return self.index.count(criteria)\n\n    def query(\n        self,\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[dict]:\n        \"\"\"\n        Queries the Store for a set of documents.\n\n        Args:\n            criteria: PyMongo filter for documents to search in.\n            properties: properties to return in grouped documents.\n            sort: Dictionary of sort order for fields. Keys are field names and values\n                are 1 for ascending or -1 for descending.\n            skip: number documents to skip.\n            limit: limit on total number of documents returned.\n\n        \"\"\"\n        prop_keys = set()\n        if isinstance(properties, dict):\n            prop_keys = set(properties.keys())\n        elif isinstance(properties, list):\n            prop_keys = set(properties)\n\n        for doc in self.index.query(criteria=criteria, sort=sort, limit=limit, skip=skip):\n            if properties is not None and prop_keys.issubset(set(doc.keys())):\n                yield {p: doc[p] for p in properties if p in doc}\n            else:\n                try:\n                    # TODO: This is ugly and unsafe, do some real checking before pulling data\n                    data = self.s3_bucket.Object(self._get_full_key_path(doc[self.key])).get()[\"Body\"].read()\n                except botocore.exceptions.ClientError as e:\n                    # If a client error is thrown, then check that it was a NoSuchKey or NoSuchBucket error.\n                    # If it was a NoSuchKey error, then the object does not exist.\n                    error_code = e.response[\"Error\"][\"Code\"]\n                    if error_code in [\"NoSuchKey\", \"NoSuchBucket\"]:\n                        error_message = e.response[\"Error\"][\"Message\"]\n                        self.logger.error(\n                            f\"S3 returned '{error_message}' while querying '{self.bucket}' for '{doc[self.key]}'\"\n                        )\n                        continue\n                    else:\n                        raise e\n\n                if self.unpack_data:\n                    data = self._read_data(data=data, compress_header=doc.get(\"compression\", \"\"))\n\n                    if self.last_updated_field in doc:\n                        data[self.last_updated_field] = doc[self.last_updated_field]\n\n                yield data\n\n    def _read_data(self, data: bytes, compress_header: str) -&gt; dict:\n        \"\"\"Reads the data and transforms it into a dictionary.\n        Allows for subclasses to apply custom schemes for transforming\n        the data retrieved from S3.\n\n        Args:\n            data (bytes): The raw byte representation of the data.\n            compress_header (str): String representing the type of compression used on the data.\n\n        Returns:\n            Dict: Dictionary representation of the data.\n        \"\"\"\n        return self._unpack(data=data, compressed=compress_header == \"zlib\")\n\n    @staticmethod\n    def _unpack(data: bytes, compressed: bool):\n        if compressed:\n            data = zlib.decompress(data)\n        # requires msgpack-python to be installed to fix string encoding problem\n        # https://github.com/msgpack/msgpack/issues/121\n        # During recursion\n        # msgpack.unpackb goes as deep as possible during reconstruction\n        # MontyDecoder().process_decode only goes until it finds a from_dict\n        # as such, we cannot just use msgpack.unpackb(data, object_hook=monty_object_hook, raw=False)\n        # Should just return the unpacked object then let the user run process_decoded\n        return msgpack.unpackb(data, raw=False)\n\n    def distinct(self, field: str, criteria: Optional[dict] = None, all_exist: bool = False) -&gt; list:\n        \"\"\"\n        Get all distinct values for a field.\n\n        Args:\n            field: the field(s) to get distinct values for.\n            criteria: PyMongo filter for documents to search in.\n        \"\"\"\n        # Index is a store so it should have its own distinct function\n        return self.index.distinct(field, criteria=criteria)\n\n    def groupby(\n        self,\n        keys: Union[list[str], str],\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[tuple[dict, list[dict]]]:\n        \"\"\"\n        Simple grouping function that will group documents by keys.\n\n        Args:\n            keys: fields to group documents.\n            criteria: PyMongo filter for documents to search in.\n            properties: properties to return in grouped documents.\n            sort: Dictionary of sort order for fields. Keys are field names and values\n            are 1 for ascending or -1 for descending.\n            skip: number documents to skip.\n            limit: limit on total number of documents returned.\n\n        Returns:\n            generator returning tuples of (dict, list of docs)\n        \"\"\"\n        return self.index.groupby(\n            keys=keys,\n            criteria=criteria,\n            properties=properties,\n            sort=sort,\n            skip=skip,\n            limit=limit,\n        )\n\n    def ensure_index(self, key: str, unique: bool = False) -&gt; bool:\n        \"\"\"\n        Tries to create an index and return true if it succeeded.\n\n        Args:\n            key: single key to index.\n            unique: whether this index contains only unique keys.\n\n        Returns:\n            bool indicating if the index exists/was created.\n        \"\"\"\n        return self.index.ensure_index(key, unique=unique)\n\n    def update(\n        self,\n        docs: Union[list[dict], dict],\n        key: Union[list, str, None] = None,\n        additional_metadata: Union[str, list[str], None] = None,\n    ):\n        \"\"\"\n        Update documents into the Store.\n\n        Args:\n            docs: the document or list of documents to update.\n            key: field name(s) to determine uniqueness for a document, can be a list of\n                multiple fields, a single field, or None if the Store's key field is to\n                be used.\n            additional_metadata: field(s) to include in the S3 store's metadata.\n        \"\"\"\n        if not isinstance(docs, list):\n            docs = [docs]\n\n        if isinstance(key, str):\n            key = [key]\n        elif not key:\n            key = [self.key]\n\n        if additional_metadata is None:\n            additional_metadata = []\n        elif isinstance(additional_metadata, str):\n            additional_metadata = [additional_metadata]\n        else:\n            additional_metadata = list(additional_metadata)\n\n        self._write_to_s3_and_index(docs, key + additional_metadata + self.searchable_fields)\n\n    def _write_to_s3_and_index(self, docs: list[dict], search_keys: list[str]):\n        \"\"\"Implements updating of the provided documents in S3 and the index.\n        Allows for subclasses to apply custom approaches to parellizing the writing.\n\n        Args:\n            docs (List[Dict]): The documents to update\n            search_keys (List[str]): The keys of the information to be updated in the index\n        \"\"\"\n        with ThreadPoolExecutor(max_workers=self.s3_workers) as pool:\n            fs = {\n                pool.submit(\n                    self.write_doc_to_s3,\n                    doc=itr_doc,\n                    search_keys=search_keys,\n                )\n                for itr_doc in docs\n            }\n            fs, _ = wait(fs)\n\n            search_docs = [sdoc.result() for sdoc in fs]\n\n        # Use store's update to remove key clashes\n        self.index.update(search_docs, key=self.key)\n\n    def _get_session(self):\n        if self.ssh_tunnel is not None:\n            self.ssh_tunnel.start()\n\n        if not hasattr(self._thread_local, \"s3_bucket\"):\n            if isinstance(self.s3_profile, dict):\n                return Session(**self.s3_profile)\n            return Session(profile_name=self.s3_profile)\n\n        return None\n\n    def _get_endpoint_url(self):\n        if self.ssh_tunnel is None:\n            return self.endpoint_url\n        host, port = self.ssh_tunnel.local_address\n        return f\"http://{host}:{port}\"\n\n    def _get_bucket(self):\n        \"\"\"If on the main thread return the bucket created above, else create a new\n        bucket on each thread.\n        \"\"\"\n        if threading.current_thread().name == \"MainThread\":\n            return self.s3_bucket\n\n        if not hasattr(self._thread_local, \"s3_bucket\"):\n            _, bucket = self._get_resource_and_bucket()\n            self._thread_local.s3_bucket = bucket\n\n        return self._thread_local.s3_bucket\n\n    def _get_resource_and_bucket(self):\n        \"\"\"Helper function to create the resource and bucket objects.\"\"\"\n        session = self._get_session()\n        endpoint_url = self._get_endpoint_url()\n        resource = session.resource(\"s3\", endpoint_url=endpoint_url, **self.s3_resource_kwargs)\n        try:\n            resource.meta.client.head_bucket(Bucket=self.bucket)\n        except ClientError:\n            raise RuntimeError(\"Bucket not present on AWS\")\n        bucket = resource.Bucket(self.bucket)\n\n        return resource, bucket\n\n    def _get_full_key_path(self, id: str) -&gt; str:\n        \"\"\"Produces the full key path for S3 items.\n\n        Args:\n            id (str): The value of the key identifier.\n\n        Returns:\n            str: The full key path\n        \"\"\"\n        return self.sub_dir + str(id)\n\n    def _get_compression_function(self) -&gt; Callable:\n        \"\"\"Returns the function to use for compressing data.\"\"\"\n        return zlib.compress\n\n    def _get_decompression_function(self) -&gt; Callable:\n        \"\"\"Returns the function to use for decompressing data.\"\"\"\n        return zlib.decompress\n\n    def write_doc_to_s3(self, doc: dict, search_keys: list[str]) -&gt; dict:\n        \"\"\"\n        Write the data to s3 and return the metadata to be inserted into the index db.\n\n        Args:\n            doc: the document.\n            search_keys: list of keys to pull from the docs and be inserted into the\n                index db.\n\n        Returns:\n            Dict: The metadata to be inserted into the index db\n        \"\"\"\n        s3_bucket = self._get_bucket()\n\n        search_doc = {k: doc[k] for k in search_keys}\n        search_doc[self.key] = doc[self.key]  # Ensure key is in metadata\n        if self.sub_dir != \"\":\n            search_doc[\"sub_dir\"] = self.sub_dir\n\n        # Remove MongoDB _id from search\n        if \"_id\" in search_doc:\n            del search_doc[\"_id\"]\n\n        # to make hashing more meaningful, make sure last updated field is removed\n        lu_info = doc.pop(self.last_updated_field, None)\n        data = msgpack.packb(doc, default=monty_default)\n\n        if self.compress:\n            # Compress with zlib if chosen\n            search_doc[\"compression\"] = \"zlib\"\n            data = self._get_compression_function()(data)\n\n        # keep a record of original keys, in case these are important for the individual researcher\n        # it is not expected that this information will be used except in disaster recovery\n        s3_to_mongo_keys = {k: self._sanitize_key(k) for k in search_doc}\n        s3_to_mongo_keys[\"s3-to-mongo-keys\"] = \"s3-to-mongo-keys\"  # inception\n        # encode dictionary since values have to be strings\n        search_doc[\"s3-to-mongo-keys\"] = dumps(s3_to_mongo_keys)\n        s3_bucket.upload_fileobj(\n            Fileobj=BytesIO(data),\n            Key=self._get_full_key_path(str(doc[self.key])),\n            ExtraArgs={\"Metadata\": {s3_to_mongo_keys[k]: str(v) for k, v in search_doc.items()}},\n        )\n\n        if lu_info is not None:\n            search_doc[self.last_updated_field] = lu_info\n\n        if self.store_hash:\n            hasher = sha1()\n            hasher.update(data)\n            obj_hash = hasher.hexdigest()\n            search_doc[\"obj_hash\"] = obj_hash\n        return search_doc\n\n    @staticmethod\n    def _sanitize_key(key):\n        \"\"\"Sanitize keys to store in S3/MinIO metadata.\"\"\"\n        # Any underscores are encoded as double dashes in metadata, since keys with\n        # underscores may be result in the corresponding HTTP header being stripped\n        # by certain server configurations (e.g. default nginx), leading to:\n        # `botocore.exceptions.ClientError: An error occurred (AccessDenied) when\n        # calling the PutObject operation: There were headers present in the request\n        # which were not signed`\n        # Metadata stored in the MongoDB index (self.index) is stored unchanged.\n\n        # Additionally, MinIO requires lowercase keys\n        return str(key).replace(\"_\", \"-\").lower()\n\n    def remove_docs(self, criteria: dict, remove_s3_object: bool = False):\n        \"\"\"\n        Remove docs matching the query dictionary.\n\n        Args:\n            criteria: query dictionary to match.\n            remove_s3_object: whether to remove the actual S3 object or not.\n        \"\"\"\n        if not remove_s3_object:\n            self.index.remove_docs(criteria=criteria)\n        else:\n            to_remove = self.index.distinct(self.key, criteria=criteria)\n            self.index.remove_docs(criteria=criteria)\n\n            # Can remove up to 1000 items at a time via boto\n            to_remove_chunks = list(grouper(to_remove, n=1000))\n            for chunk_to_remove in to_remove_chunks:\n                objlist = [{\"Key\": self._get_full_key_path(obj)} for obj in chunk_to_remove]\n                self.s3_bucket.delete_objects(Delete={\"Objects\": objlist})\n\n    @property\n    def last_updated(self):\n        return self.index.last_updated\n\n    def newer_in(self, target: Store, criteria: Optional[dict] = None, exhaustive: bool = False) -&gt; list[str]:\n        \"\"\"\n        Returns the keys of documents that are newer in the target Store than this Store.\n\n        Args:\n            target: target Store.\n            criteria: PyMongo filter for documents to search in.\n            exhaustive: triggers an item-by-item check vs. checking the last_updated of\n                the target Store and using that to filter out new items in.\n        \"\"\"\n        if hasattr(target, \"index\"):\n            return self.index.newer_in(target=target.index, criteria=criteria, exhaustive=exhaustive)\n        return self.index.newer_in(target=target, criteria=criteria, exhaustive=exhaustive)\n\n    def __hash__(self):\n        return hash((self.index.__hash__, self.bucket))\n\n    def rebuild_index_from_s3_data(self, **kwargs):\n        \"\"\"\n        Rebuilds the index Store from the data in S3.\n\n        Relies on the index document being stores as the metadata for the file. This can\n        help recover lost databases.\n        \"\"\"\n        bucket = self.s3_bucket\n        objects = bucket.objects.filter(Prefix=self.sub_dir)\n        for obj in objects:\n            key_ = self._get_full_key_path(obj.key)\n            data = self.s3_bucket.Object(key_).get()[\"Body\"].read()\n\n            if self.compress:\n                data = self._get_decompression_function()(data)\n            unpacked_data = msgpack.unpackb(data, raw=False)\n            self.update(unpacked_data, **kwargs)\n\n    def rebuild_metadata_from_index(self, index_query: Optional[dict] = None):\n        \"\"\"\n        Read data from the index store and populate the metadata of the S3 bucket.\n        Force all the keys to be lower case to be Minio compatible.\n\n        Args:\n            index_query: query on the index store.\n        \"\"\"\n        qq = {} if index_query is None else index_query\n        for index_doc in self.index.query(qq):\n            key_ = self._get_full_key_path(index_doc[self.key])\n            s3_object = self.s3_bucket.Object(key_)\n            new_meta = {self._sanitize_key(k): v for k, v in s3_object.metadata.items()}\n            for k, v in index_doc.items():\n                new_meta[str(k).lower()] = v\n            new_meta.pop(\"_id\")\n            if self.last_updated_field in new_meta:\n                new_meta[self.last_updated_field] = str(to_isoformat_ceil_ms(new_meta[self.last_updated_field]))\n            # s3_object.metadata.update(new_meta)\n            s3_object.copy_from(\n                CopySource={\"Bucket\": self.s3_bucket.name, \"Key\": key_},\n                Metadata=new_meta,\n                MetadataDirective=\"REPLACE\",\n            )\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"\n        Check equality for S3Store.\n\n        other: other S3Store to compare with.\n        \"\"\"\n        if not isinstance(other, S3Store):\n            return False\n\n        fields = [\"index\", \"bucket\", \"last_updated_field\"]\n        return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.name","title":"<code>name</code>  <code>property</code>","text":"<p>String representing this data source.</p>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality for S3Store.</p> <p>other: other S3Store to compare with.</p> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"\n    Check equality for S3Store.\n\n    other: other S3Store to compare with.\n    \"\"\"\n    if not isinstance(other, S3Store):\n        return False\n\n    fields = [\"index\", \"bucket\", \"last_updated_field\"]\n    return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.__init__","title":"<code>__init__(index, bucket, s3_profile=None, compress=False, endpoint_url=None, sub_dir=None, s3_workers=1, s3_resource_kwargs=None, ssh_tunnel=None, key='fs_id', store_hash=True, unpack_data=True, searchable_fields=None, index_store_kwargs=None, **kwargs)</code>","text":"<p>Initializes an S3 Store.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Store</code> <p>a store to use to index the S3 bucket.</p> required <code>bucket</code> <code>str</code> <p>name of the bucket.</p> required <code>s3_profile</code> <code>Optional[Union[str, dict]]</code> <p>name of AWS profile containing the credentials. Alternatively you can pass in a dictionary with the full credentials:     aws_access_key_id (string) -- AWS access key ID     aws_secret_access_key (string) -- AWS secret access key     aws_session_token (string) -- AWS temporary session token     region_name (string) -- Default region when creating new connections</p> <code>None</code> <code>compress</code> <code>bool</code> <p>compress files inserted into the store.</p> <code>False</code> <code>endpoint_url</code> <code>Optional[str]</code> <p>this allows the interface with minio service; ignored if <code>ssh_tunnel</code> is provided, in which case it is inferred.</p> <code>None</code> <code>sub_dir</code> <code>Optional[str]</code> <p>subdirectory of the S3 bucket to store the data.</p> <code>None</code> <code>s3_workers</code> <code>int</code> <p>number of concurrent S3 puts to run.</p> <code>1</code> <code>s3_resource_kwargs</code> <code>Optional[dict]</code> <p>additional kwargs to pass to the boto3 session resource.</p> <code>None</code> <code>ssh_tunnel</code> <code>Optional[SSHTunnel]</code> <p>optional SSH tunnel to use for the S3 connection.</p> <code>None</code> <code>key</code> <code>str</code> <p>main key to index on.</p> <code>'fs_id'</code> <code>store_hash</code> <code>bool</code> <p>store the SHA1 hash right before insertion to the database.</p> <code>True</code> <code>unpack_data</code> <code>bool</code> <p>whether to decompress and unpack byte data when querying from the bucket.</p> <code>True</code> <code>searchable_fields</code> <code>Optional[list[str]]</code> <p>fields to keep in the index store.</p> <code>None</code> <code>index_store_kwargs</code> <code>Optional[dict]</code> <p>kwargs to pass to the index store. Allows the user to use kwargs here to update the index store.</p> <code>None</code> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>def __init__(\n    self,\n    index: Store,\n    bucket: str,\n    s3_profile: Optional[Union[str, dict]] = None,\n    compress: bool = False,\n    endpoint_url: Optional[str] = None,\n    sub_dir: Optional[str] = None,\n    s3_workers: int = 1,\n    s3_resource_kwargs: Optional[dict] = None,\n    ssh_tunnel: Optional[SSHTunnel] = None,\n    key: str = \"fs_id\",\n    store_hash: bool = True,\n    unpack_data: bool = True,\n    searchable_fields: Optional[list[str]] = None,\n    index_store_kwargs: Optional[dict] = None,\n    **kwargs,\n):\n    \"\"\"\n    Initializes an S3 Store.\n\n    Args:\n        index: a store to use to index the S3 bucket.\n        bucket: name of the bucket.\n        s3_profile: name of AWS profile containing the credentials. Alternatively\n            you can pass in a dictionary with the full credentials:\n                aws_access_key_id (string) -- AWS access key ID\n                aws_secret_access_key (string) -- AWS secret access key\n                aws_session_token (string) -- AWS temporary session token\n                region_name (string) -- Default region when creating new connections\n        compress: compress files inserted into the store.\n        endpoint_url: this allows the interface with minio service; ignored if\n            `ssh_tunnel` is provided, in which case it is inferred.\n        sub_dir: subdirectory of the S3 bucket to store the data.\n        s3_workers: number of concurrent S3 puts to run.\n        s3_resource_kwargs: additional kwargs to pass to the boto3 session resource.\n        ssh_tunnel: optional SSH tunnel to use for the S3 connection.\n        key: main key to index on.\n        store_hash: store the SHA1 hash right before insertion to the database.\n        unpack_data: whether to decompress and unpack byte data when querying from\n            the bucket.\n        searchable_fields: fields to keep in the index store.\n        index_store_kwargs: kwargs to pass to the index store. Allows the user to\n            use kwargs here to update the index store.\n    \"\"\"\n    if boto3 is None:\n        raise RuntimeError(\"boto3 and botocore are required for S3Store\")\n    self.index_store_kwargs = index_store_kwargs or {}\n    if index_store_kwargs:\n        d_ = index.as_dict()\n        d_.update(index_store_kwargs)\n        self.index = index.__class__.from_dict(d_)\n    else:\n        self.index = index\n    self.bucket = bucket\n    self.s3_profile = s3_profile\n    self.compress = compress\n    self.endpoint_url = endpoint_url\n    self.sub_dir = sub_dir.strip(\"/\") + \"/\" if sub_dir else \"\"\n    self.s3: Any = None\n    self.s3_bucket: Any = None\n    self.s3_workers = s3_workers\n    self.s3_resource_kwargs = s3_resource_kwargs if s3_resource_kwargs is not None else {}\n    self.ssh_tunnel = ssh_tunnel\n    self.unpack_data = unpack_data\n    self.searchable_fields = searchable_fields if searchable_fields is not None else []\n    self.store_hash = store_hash\n\n    # Force the key to be the same as the index\n    assert isinstance(index.key, str), \"Since we are using the key as a file name in S3, they key must be a string\"\n    if key != index.key:\n        warnings.warn(\n            f'The desired S3Store key \"{key}\" does not match the index key \"{index.key},\"'\n            \"the index key will be used\",\n            UserWarning,\n        )\n    kwargs[\"key\"] = str(index.key)\n\n    self._thread_local = threading.local()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.close","title":"<code>close()</code>","text":"<p>Closes any connections.</p> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>def close(self):\n    \"\"\"Closes any connections.\"\"\"\n    self.index.close()\n\n    self.s3.meta.client.close()\n    self.s3 = None\n    self.s3_bucket = None\n\n    if self.ssh_tunnel is not None:\n        self.ssh_tunnel.stop()\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.connect","title":"<code>connect(force_reset=False)</code>","text":"<p>Connect to the source data.</p> <p>Parameters:</p> Name Type Description Default <code>force_reset</code> <code>bool</code> <p>whether to force a reset of the connection</p> <code>False</code> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>def connect(self, force_reset: bool = False):  # lgtm[py/conflicting-attributes]\n    \"\"\"Connect to the source data.\n\n    Args:\n        force_reset: whether to force a reset of the connection\n    \"\"\"\n    if self.s3 is None or force_reset:\n        self.s3, self.s3_bucket = self._get_resource_and_bucket()\n    self.index.connect(force_reset=force_reset)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.count","title":"<code>count(criteria=None)</code>","text":"<p>Counts the number of documents matching the query criteria.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to count in.</p> <code>None</code> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>def count(self, criteria: Optional[dict] = None) -&gt; int:\n    \"\"\"\n    Counts the number of documents matching the query criteria.\n\n    Args:\n        criteria: PyMongo filter for documents to count in.\n    \"\"\"\n    return self.index.count(criteria)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.distinct","title":"<code>distinct(field, criteria=None, all_exist=False)</code>","text":"<p>Get all distinct values for a field.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>the field(s) to get distinct values for.</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in.</p> <code>None</code> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>def distinct(self, field: str, criteria: Optional[dict] = None, all_exist: bool = False) -&gt; list:\n    \"\"\"\n    Get all distinct values for a field.\n\n    Args:\n        field: the field(s) to get distinct values for.\n        criteria: PyMongo filter for documents to search in.\n    \"\"\"\n    # Index is a store so it should have its own distinct function\n    return self.index.distinct(field, criteria=criteria)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.ensure_index","title":"<code>ensure_index(key, unique=False)</code>","text":"<p>Tries to create an index and return true if it succeeded.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>single key to index.</p> required <code>unique</code> <code>bool</code> <p>whether this index contains only unique keys.</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>bool indicating if the index exists/was created.</p> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>def ensure_index(self, key: str, unique: bool = False) -&gt; bool:\n    \"\"\"\n    Tries to create an index and return true if it succeeded.\n\n    Args:\n        key: single key to index.\n        unique: whether this index contains only unique keys.\n\n    Returns:\n        bool indicating if the index exists/was created.\n    \"\"\"\n    return self.index.ensure_index(key, unique=unique)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.groupby","title":"<code>groupby(keys, criteria=None, properties=None, sort=None, skip=0, limit=0)</code>","text":"<p>Simple grouping function that will group documents by keys.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[list[str], str]</code> <p>fields to group documents.</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in.</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents.</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip.</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned.</p> <code>0</code> <p>Returns:</p> Type Description <code>Iterator[tuple[dict, list[dict]]]</code> <p>generator returning tuples of (dict, list of docs)</p> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>def groupby(\n    self,\n    keys: Union[list[str], str],\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[tuple[dict, list[dict]]]:\n    \"\"\"\n    Simple grouping function that will group documents by keys.\n\n    Args:\n        keys: fields to group documents.\n        criteria: PyMongo filter for documents to search in.\n        properties: properties to return in grouped documents.\n        sort: Dictionary of sort order for fields. Keys are field names and values\n        are 1 for ascending or -1 for descending.\n        skip: number documents to skip.\n        limit: limit on total number of documents returned.\n\n    Returns:\n        generator returning tuples of (dict, list of docs)\n    \"\"\"\n    return self.index.groupby(\n        keys=keys,\n        criteria=criteria,\n        properties=properties,\n        sort=sort,\n        skip=skip,\n        limit=limit,\n    )\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.newer_in","title":"<code>newer_in(target, criteria=None, exhaustive=False)</code>","text":"<p>Returns the keys of documents that are newer in the target Store than this Store.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Store</code> <p>target Store.</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in.</p> <code>None</code> <code>exhaustive</code> <code>bool</code> <p>triggers an item-by-item check vs. checking the last_updated of the target Store and using that to filter out new items in.</p> <code>False</code> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>def newer_in(self, target: Store, criteria: Optional[dict] = None, exhaustive: bool = False) -&gt; list[str]:\n    \"\"\"\n    Returns the keys of documents that are newer in the target Store than this Store.\n\n    Args:\n        target: target Store.\n        criteria: PyMongo filter for documents to search in.\n        exhaustive: triggers an item-by-item check vs. checking the last_updated of\n            the target Store and using that to filter out new items in.\n    \"\"\"\n    if hasattr(target, \"index\"):\n        return self.index.newer_in(target=target.index, criteria=criteria, exhaustive=exhaustive)\n    return self.index.newer_in(target=target, criteria=criteria, exhaustive=exhaustive)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.query","title":"<code>query(criteria=None, properties=None, sort=None, skip=0, limit=0)</code>","text":"<p>Queries the Store for a set of documents.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in.</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents.</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip.</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned.</p> <code>0</code> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>def query(\n    self,\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[dict]:\n    \"\"\"\n    Queries the Store for a set of documents.\n\n    Args:\n        criteria: PyMongo filter for documents to search in.\n        properties: properties to return in grouped documents.\n        sort: Dictionary of sort order for fields. Keys are field names and values\n            are 1 for ascending or -1 for descending.\n        skip: number documents to skip.\n        limit: limit on total number of documents returned.\n\n    \"\"\"\n    prop_keys = set()\n    if isinstance(properties, dict):\n        prop_keys = set(properties.keys())\n    elif isinstance(properties, list):\n        prop_keys = set(properties)\n\n    for doc in self.index.query(criteria=criteria, sort=sort, limit=limit, skip=skip):\n        if properties is not None and prop_keys.issubset(set(doc.keys())):\n            yield {p: doc[p] for p in properties if p in doc}\n        else:\n            try:\n                # TODO: This is ugly and unsafe, do some real checking before pulling data\n                data = self.s3_bucket.Object(self._get_full_key_path(doc[self.key])).get()[\"Body\"].read()\n            except botocore.exceptions.ClientError as e:\n                # If a client error is thrown, then check that it was a NoSuchKey or NoSuchBucket error.\n                # If it was a NoSuchKey error, then the object does not exist.\n                error_code = e.response[\"Error\"][\"Code\"]\n                if error_code in [\"NoSuchKey\", \"NoSuchBucket\"]:\n                    error_message = e.response[\"Error\"][\"Message\"]\n                    self.logger.error(\n                        f\"S3 returned '{error_message}' while querying '{self.bucket}' for '{doc[self.key]}'\"\n                    )\n                    continue\n                else:\n                    raise e\n\n            if self.unpack_data:\n                data = self._read_data(data=data, compress_header=doc.get(\"compression\", \"\"))\n\n                if self.last_updated_field in doc:\n                    data[self.last_updated_field] = doc[self.last_updated_field]\n\n            yield data\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.rebuild_index_from_s3_data","title":"<code>rebuild_index_from_s3_data(**kwargs)</code>","text":"<p>Rebuilds the index Store from the data in S3.</p> <p>Relies on the index document being stores as the metadata for the file. This can help recover lost databases.</p> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>def rebuild_index_from_s3_data(self, **kwargs):\n    \"\"\"\n    Rebuilds the index Store from the data in S3.\n\n    Relies on the index document being stores as the metadata for the file. This can\n    help recover lost databases.\n    \"\"\"\n    bucket = self.s3_bucket\n    objects = bucket.objects.filter(Prefix=self.sub_dir)\n    for obj in objects:\n        key_ = self._get_full_key_path(obj.key)\n        data = self.s3_bucket.Object(key_).get()[\"Body\"].read()\n\n        if self.compress:\n            data = self._get_decompression_function()(data)\n        unpacked_data = msgpack.unpackb(data, raw=False)\n        self.update(unpacked_data, **kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.rebuild_metadata_from_index","title":"<code>rebuild_metadata_from_index(index_query=None)</code>","text":"<p>Read data from the index store and populate the metadata of the S3 bucket. Force all the keys to be lower case to be Minio compatible.</p> <p>Parameters:</p> Name Type Description Default <code>index_query</code> <code>Optional[dict]</code> <p>query on the index store.</p> <code>None</code> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>def rebuild_metadata_from_index(self, index_query: Optional[dict] = None):\n    \"\"\"\n    Read data from the index store and populate the metadata of the S3 bucket.\n    Force all the keys to be lower case to be Minio compatible.\n\n    Args:\n        index_query: query on the index store.\n    \"\"\"\n    qq = {} if index_query is None else index_query\n    for index_doc in self.index.query(qq):\n        key_ = self._get_full_key_path(index_doc[self.key])\n        s3_object = self.s3_bucket.Object(key_)\n        new_meta = {self._sanitize_key(k): v for k, v in s3_object.metadata.items()}\n        for k, v in index_doc.items():\n            new_meta[str(k).lower()] = v\n        new_meta.pop(\"_id\")\n        if self.last_updated_field in new_meta:\n            new_meta[self.last_updated_field] = str(to_isoformat_ceil_ms(new_meta[self.last_updated_field]))\n        # s3_object.metadata.update(new_meta)\n        s3_object.copy_from(\n            CopySource={\"Bucket\": self.s3_bucket.name, \"Key\": key_},\n            Metadata=new_meta,\n            MetadataDirective=\"REPLACE\",\n        )\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.remove_docs","title":"<code>remove_docs(criteria, remove_s3_object=False)</code>","text":"<p>Remove docs matching the query dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>dict</code> <p>query dictionary to match.</p> required <code>remove_s3_object</code> <code>bool</code> <p>whether to remove the actual S3 object or not.</p> <code>False</code> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>def remove_docs(self, criteria: dict, remove_s3_object: bool = False):\n    \"\"\"\n    Remove docs matching the query dictionary.\n\n    Args:\n        criteria: query dictionary to match.\n        remove_s3_object: whether to remove the actual S3 object or not.\n    \"\"\"\n    if not remove_s3_object:\n        self.index.remove_docs(criteria=criteria)\n    else:\n        to_remove = self.index.distinct(self.key, criteria=criteria)\n        self.index.remove_docs(criteria=criteria)\n\n        # Can remove up to 1000 items at a time via boto\n        to_remove_chunks = list(grouper(to_remove, n=1000))\n        for chunk_to_remove in to_remove_chunks:\n            objlist = [{\"Key\": self._get_full_key_path(obj)} for obj in chunk_to_remove]\n            self.s3_bucket.delete_objects(Delete={\"Objects\": objlist})\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.update","title":"<code>update(docs, key=None, additional_metadata=None)</code>","text":"<p>Update documents into the Store.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[list[dict], dict]</code> <p>the document or list of documents to update.</p> required <code>key</code> <code>Union[list, str, None]</code> <p>field name(s) to determine uniqueness for a document, can be a list of multiple fields, a single field, or None if the Store's key field is to be used.</p> <code>None</code> <code>additional_metadata</code> <code>Union[str, list[str], None]</code> <p>field(s) to include in the S3 store's metadata.</p> <code>None</code> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>def update(\n    self,\n    docs: Union[list[dict], dict],\n    key: Union[list, str, None] = None,\n    additional_metadata: Union[str, list[str], None] = None,\n):\n    \"\"\"\n    Update documents into the Store.\n\n    Args:\n        docs: the document or list of documents to update.\n        key: field name(s) to determine uniqueness for a document, can be a list of\n            multiple fields, a single field, or None if the Store's key field is to\n            be used.\n        additional_metadata: field(s) to include in the S3 store's metadata.\n    \"\"\"\n    if not isinstance(docs, list):\n        docs = [docs]\n\n    if isinstance(key, str):\n        key = [key]\n    elif not key:\n        key = [self.key]\n\n    if additional_metadata is None:\n        additional_metadata = []\n    elif isinstance(additional_metadata, str):\n        additional_metadata = [additional_metadata]\n    else:\n        additional_metadata = list(additional_metadata)\n\n    self._write_to_s3_and_index(docs, key + additional_metadata + self.searchable_fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.aws.S3Store.write_doc_to_s3","title":"<code>write_doc_to_s3(doc, search_keys)</code>","text":"<p>Write the data to s3 and return the metadata to be inserted into the index db.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>dict</code> <p>the document.</p> required <code>search_keys</code> <code>list[str]</code> <p>list of keys to pull from the docs and be inserted into the index db.</p> required <p>Returns:</p> Name Type Description <code>Dict</code> <code>dict</code> <p>The metadata to be inserted into the index db</p> Source code in <code>src/maggma/stores/aws.py</code> <pre><code>def write_doc_to_s3(self, doc: dict, search_keys: list[str]) -&gt; dict:\n    \"\"\"\n    Write the data to s3 and return the metadata to be inserted into the index db.\n\n    Args:\n        doc: the document.\n        search_keys: list of keys to pull from the docs and be inserted into the\n            index db.\n\n    Returns:\n        Dict: The metadata to be inserted into the index db\n    \"\"\"\n    s3_bucket = self._get_bucket()\n\n    search_doc = {k: doc[k] for k in search_keys}\n    search_doc[self.key] = doc[self.key]  # Ensure key is in metadata\n    if self.sub_dir != \"\":\n        search_doc[\"sub_dir\"] = self.sub_dir\n\n    # Remove MongoDB _id from search\n    if \"_id\" in search_doc:\n        del search_doc[\"_id\"]\n\n    # to make hashing more meaningful, make sure last updated field is removed\n    lu_info = doc.pop(self.last_updated_field, None)\n    data = msgpack.packb(doc, default=monty_default)\n\n    if self.compress:\n        # Compress with zlib if chosen\n        search_doc[\"compression\"] = \"zlib\"\n        data = self._get_compression_function()(data)\n\n    # keep a record of original keys, in case these are important for the individual researcher\n    # it is not expected that this information will be used except in disaster recovery\n    s3_to_mongo_keys = {k: self._sanitize_key(k) for k in search_doc}\n    s3_to_mongo_keys[\"s3-to-mongo-keys\"] = \"s3-to-mongo-keys\"  # inception\n    # encode dictionary since values have to be strings\n    search_doc[\"s3-to-mongo-keys\"] = dumps(s3_to_mongo_keys)\n    s3_bucket.upload_fileobj(\n        Fileobj=BytesIO(data),\n        Key=self._get_full_key_path(str(doc[self.key])),\n        ExtraArgs={\"Metadata\": {s3_to_mongo_keys[k]: str(v) for k, v in search_doc.items()}},\n    )\n\n    if lu_info is not None:\n        search_doc[self.last_updated_field] = lu_info\n\n    if self.store_hash:\n        hasher = sha1()\n        hasher.update(data)\n        obj_hash = hasher.hexdigest()\n        search_doc[\"obj_hash\"] = obj_hash\n    return search_doc\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore","title":"<code>AzureBlobStore</code>","text":"<p>               Bases: <code>Store</code></p> <p>GridFS like storage using Azure Blob and a regular store for indexing.</p> <p>Requires azure-storage-blob and azure-identity modules to be installed.</p> Source code in <code>src/maggma/stores/azure.py</code> <pre><code>class AzureBlobStore(Store):\n    \"\"\"\n    GridFS like storage using Azure Blob and a regular store for indexing.\n\n    Requires azure-storage-blob and azure-identity modules to be installed.\n    \"\"\"\n\n    def __init__(\n        self,\n        index: Store,\n        container_name: str,\n        azure_client_info: Optional[Union[str, dict]] = None,\n        credential_type: CredentialType = \"DefaultAzureCredential\",\n        compress: bool = False,\n        sub_dir: Optional[str] = None,\n        workers: int = 1,\n        azure_resource_kwargs: Optional[dict] = None,\n        key: str = \"fs_id\",\n        store_hash: bool = True,\n        unpack_data: bool = True,\n        searchable_fields: Optional[list[str]] = None,\n        key_sanitize_dict: Optional[dict] = None,\n        create_container: bool = False,\n        **kwargs,\n    ):\n        \"\"\"\n        Initializes an AzureBlob Store.\n\n        Args:\n            index: a store to use to index the Azure blob\n            container_name: name of the container\n            azure_client_info: connection_url of the BlobServiceClient if a string.\n                Assumes that the access is passwordless in that case.\n                Otherwise, if a dictionary, options to instantiate the\n                BlobServiceClient.\n                Currently supported keywords:\n                    - connection_string: a connection string for the Azure blob\n            credential_type: the type of credential to use to authenticate with Azure.\n                Default is \"DefaultAzureCredential\".  For serializable stores, provide\n                a string representation of the credential class. Otherwises, you may\n                provide the class itself.\n            compress: compress files inserted into the store\n            sub_dir: (optional)  subdirectory of the container to store the data.\n                When defined, a final \"/\" will be added if not already present.\n            workers: number of concurrent Azure puts to run\n            store_hash: store the sha1 hash right before insertion to the database.\n            unpack_data: whether to decompress and unpack byte data when querying from\n                the container.\n            searchable_fields: fields to keep in the index store\n            key_sanitize_dict: a dictionary that allows to customize the sanitization\n                of the keys in metadata, since they should adhere to the naming rules\n                for C# identifiers. If None the AZURE_KEY_SANITIZE default will be used\n                to handle the most common cases.\n            create_container: if True the Store creates the container, in case it does\n                not exist.\n            kwargs: keywords for the base Store.\n        \"\"\"\n        if azure_blob is None:\n            raise RuntimeError(\"azure-storage-blob and azure-identity are required for AzureBlobStore\")\n\n        self.index = index\n        self.container_name = container_name\n        self.azure_client_info = azure_client_info\n        self.compress = compress\n        self.sub_dir = sub_dir.rstrip(\"/\") + \"/\" if sub_dir else \"\"\n        self.service: Optional[BlobServiceClient] = None\n        self.container: Optional[ContainerClient] = None\n        self.workers = workers\n        self.azure_resource_kwargs = azure_resource_kwargs if azure_resource_kwargs is not None else {}\n        self.unpack_data = unpack_data\n        self.searchable_fields = searchable_fields if searchable_fields is not None else []\n        self.store_hash = store_hash\n        if key_sanitize_dict is None:\n            key_sanitize_dict = AZURE_KEY_SANITIZE\n        self.key_sanitize_dict = key_sanitize_dict\n        self.create_container = create_container\n        self.credential_type = credential_type\n\n        # Force the key to be the same as the index\n        assert isinstance(\n            index.key, str\n        ), \"Since we are using the key as a file name in Azure Blob, the key must be a string\"\n        if key != index.key:\n            warnings.warn(\n                f'The desired AzureBlobStore key \"{key}\" does not match the index key \"{index.key},\"'\n                \"the index key will be used\",\n                UserWarning,\n            )\n        kwargs[\"key\"] = str(index.key)\n\n        self._thread_local = threading.local()\n        super().__init__(**kwargs)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        Returns:\n            a string representing this data source.\n        \"\"\"\n        return f\"container://{self.container_name}\"\n\n    def connect(self, *args, **kwargs):  # lgtm[py/conflicting-attributes]\n        \"\"\"\n        Connect to the source data.\n        \"\"\"\n        service_client = self._get_service_client()\n\n        if not self.service:\n            self.service = service_client\n            container = service_client.get_container_client(self.container_name)\n            if not container.exists():\n                if self.create_container:\n                    # catch the exception to avoid errors if already created\n                    try:\n                        container.create_container()\n                    except ResourceExistsError:\n                        pass\n                else:\n                    raise RuntimeError(f\"Container not present on Azure: {self.container_name}\")\n\n            self.container = container\n        self.index.connect(*args, **kwargs)\n\n    def close(self):\n        \"\"\"\n        Closes any connections.\n        \"\"\"\n        self.index.close()\n        self.service = None\n        self.container = None\n\n    @property\n    def _collection(self):\n        \"\"\"\n        Returns:\n            a handle to the pymongo collection object.\n\n        Important:\n            Not guaranteed to exist in the future\n        \"\"\"\n        # For now returns the index collection since that is what we would \"search\" on\n        return self.index._collection\n\n    def count(self, criteria: Optional[dict] = None) -&gt; int:\n        \"\"\"\n        Counts the number of documents matching the query criteria.\n\n        Args:\n            criteria: PyMongo filter for documents to count in\n        \"\"\"\n        return self.index.count(criteria)\n\n    def query(\n        self,\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[dict]:\n        \"\"\"\n        Queries the Store for a set of documents.\n\n        Args:\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n\n        \"\"\"\n        if self.container is None or self.service is None:\n            raise RuntimeError(\"The store has not been connected\")\n\n        prop_keys = set()\n        if isinstance(properties, dict):\n            prop_keys = set(properties.keys())\n        elif isinstance(properties, list):\n            prop_keys = set(properties)\n\n        for doc in self.index.query(criteria=criteria, sort=sort, limit=limit, skip=skip):\n            if properties is not None and prop_keys.issubset(set(doc.keys())):\n                yield {p: doc[p] for p in properties if p in doc}\n            else:\n                try:\n                    data = self.container.download_blob(self.sub_dir + str(doc[self.key])).readall()\n                except azure.core.exceptions.ResourceNotFoundError:\n                    self.logger.error(f\"Could not find Blob object {doc[self.key]}\")\n\n                if self.unpack_data:\n                    data = self._unpack(data=data, compressed=doc.get(\"compression\", \"\") == \"zlib\")\n\n                    if self.last_updated_field in doc:\n                        data[self.last_updated_field] = doc[self.last_updated_field]  # type: ignore\n\n                yield data  # type: ignore\n\n    @staticmethod\n    def _unpack(data: bytes, compressed: bool):\n        if compressed:\n            data = zlib.decompress(data)\n        # requires msgpack-python to be installed to fix string encoding problem\n        # https://github.com/msgpack/msgpack/issues/121\n        # During recursion\n        # msgpack.unpackb goes as deep as possible during reconstruction\n        # MontyDecoder().process_decode only goes until it finds a from_dict\n        # as such, we cannot just use msgpack.unpackb(data, object_hook=monty_object_hook, raw=False)\n        # Should just return the unpacked object then let the user run process_decoded\n        return msgpack.unpackb(data, raw=False)\n\n    def distinct(self, field: str, criteria: Optional[dict] = None, all_exist: bool = False) -&gt; list:\n        \"\"\"\n        Get all distinct values for a field.\n\n        Args:\n            field: the field(s) to get distinct values for\n            criteria: PyMongo filter for documents to search in\n        \"\"\"\n        # Index is a store so it should have its own distinct function\n        return self.index.distinct(field, criteria=criteria)\n\n    def groupby(\n        self,\n        keys: Union[list[str], str],\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[tuple[dict, list[dict]]]:\n        \"\"\"\n        Simple grouping function that will group documents\n        by keys.\n\n        Args:\n            keys: fields to group documents\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n\n        Returns:\n            generator returning tuples of (dict, list of docs)\n        \"\"\"\n        return self.index.groupby(\n            keys=keys,\n            criteria=criteria,\n            properties=properties,\n            sort=sort,\n            skip=skip,\n            limit=limit,\n        )\n\n    def ensure_index(self, key: str, unique: bool = False) -&gt; bool:\n        \"\"\"\n        Tries to create an index and return true if it succeeded.\n\n        Args:\n            key: single key to index\n            unique: Whether or not this index contains only unique keys\n\n        Returns:\n            bool indicating if the index exists/was created\n        \"\"\"\n        return self.index.ensure_index(key, unique=unique)\n\n    def update(\n        self,\n        docs: Union[list[dict], dict],\n        key: Union[list, str, None] = None,\n        additional_metadata: Union[str, list[str], None] = None,\n    ):\n        \"\"\"\n        Update documents into the Store.\n\n        Args:\n            docs: the document or list of documents to update\n            key: field name(s) to determine uniqueness for a\n                 document, can be a list of multiple fields,\n                 a single field, or None if the Store's key\n                 field is to be used\n            additional_metadata: field(s) to include in the blob store's metadata\n        \"\"\"\n        if self.container is None or self.service is None:\n            raise RuntimeError(\"The store has not been connected\")\n\n        if not isinstance(docs, list):\n            docs = [docs]\n\n        if isinstance(key, str):\n            key = [key]\n        elif not key:\n            key = [self.key]\n\n        if additional_metadata is None:\n            additional_metadata = []\n        elif isinstance(additional_metadata, str):\n            additional_metadata = [additional_metadata]\n        else:\n            additional_metadata = list(additional_metadata)\n\n        with ThreadPoolExecutor(max_workers=self.workers) as pool:\n            fs = {\n                pool.submit(\n                    self.write_doc_to_blob,\n                    doc=itr_doc,\n                    search_keys=key + additional_metadata + self.searchable_fields,\n                )\n                for itr_doc in docs\n            }\n            fs, _ = wait(fs)\n\n            search_docs = [sdoc.result() for sdoc in fs]\n\n        # Use store's update to remove key clashes\n        self.index.update(search_docs, key=self.key)\n\n    def _get_service_client(self):\n        if not hasattr(self._thread_local, \"container\"):\n            if isinstance(self.azure_client_info, str):\n                # assume it is the account_url and that the connection is passwordless\n                credentials_ = _get_azure_credential(self.credential_type)\n                return BlobServiceClient(self.azure_client_info, credential=credentials_)\n\n            if isinstance(self.azure_client_info, dict):\n                connection_string = self.azure_client_info.get(\"connection_string\")\n                if connection_string:\n                    return BlobServiceClient.from_connection_string(conn_str=connection_string)\n\n            msg = f\"Could not instantiate BlobServiceClient from azure_client_info: {self.azure_client_info}\"\n            raise RuntimeError(msg)\n        return None\n\n    def _get_container(self) -&gt; Optional[ContainerClient]:\n        \"\"\"\n        If on the main thread return the container created above, else create a new\n        container on each thread.\n        \"\"\"\n        if threading.current_thread().name == \"MainThread\":\n            return self.container\n        if not hasattr(self._thread_local, \"container\"):\n            service_client = self._get_service_client()\n            container = service_client.get_container_client(self.container_name)\n            self._thread_local.container = container\n        return self._thread_local.container\n\n    def write_doc_to_blob(self, doc: dict, search_keys: list[str]):\n        \"\"\"\n        Write the data to an Azure blob and return the metadata to be inserted into the index db.\n\n        Args:\n            doc: the document\n            search_keys: list of keys to pull from the docs and be inserted into the\n            index db\n        \"\"\"\n        container = self._get_container()\n        if container is None:\n            raise RuntimeError(\"The store has not been connected\")\n\n        search_doc = {k: doc[k] for k in search_keys}\n        search_doc[self.key] = doc[self.key]  # Ensure key is in metadata\n        if self.sub_dir != \"\":\n            search_doc[\"sub_dir\"] = self.sub_dir\n\n        # Remove MongoDB _id from search\n        if \"_id\" in search_doc:\n            del search_doc[\"_id\"]\n\n        # to make hashing more meaningful, make sure last updated field is removed\n        lu_info = doc.pop(self.last_updated_field, None)\n        data = msgpack.packb(doc, default=monty_default)\n\n        if self.compress:\n            # Compress with zlib if chosen\n            search_doc[\"compression\"] = \"zlib\"\n            data = zlib.compress(data)\n\n        if self.last_updated_field in doc:\n            # need this conversion for metadata insert\n            search_doc[self.last_updated_field] = str(to_isoformat_ceil_ms(doc[self.last_updated_field]))\n\n        # keep a record of original keys, in case these are important for the individual researcher\n        # it is not expected that this information will be used except in disaster recovery\n        blob_to_mongo_keys = {k: self._sanitize_key(k) for k in search_doc}\n        blob_to_mongo_keys[\"blob_to_mongo_keys\"] = \"blob_to_mongo_keys\"  # inception\n        # encode dictionary since values have to be strings\n        search_doc[\"blob_to_mongo_keys\"] = dumps(blob_to_mongo_keys)\n\n        container.upload_blob(\n            name=self.sub_dir + str(doc[self.key]),\n            data=data,\n            metadata={blob_to_mongo_keys[k]: str(v) for k, v in search_doc.items()},\n            overwrite=True,\n        )\n\n        if lu_info is not None:\n            search_doc[self.last_updated_field] = lu_info\n\n        if self.store_hash:\n            hasher = sha1()\n            hasher.update(data)\n            obj_hash = hasher.hexdigest()\n            search_doc[\"obj_hash\"] = obj_hash\n        return search_doc\n\n    def _sanitize_key(self, key):\n        \"\"\"\n        Sanitize keys to store metadata.\n        The metadata keys should adhere to the naming rules for C# identifiers.\n        \"\"\"\n        new_key = str(key)\n        for k, v in self.key_sanitize_dict.items():\n            new_key = new_key.replace(k, v)\n\n        return new_key\n\n    def remove_docs(self, criteria: dict, remove_blob_object: bool = False):\n        \"\"\"\n        Remove docs matching the query dictionary.\n\n        Args:\n            criteria: query dictionary to match\n            remove_blob_object: whether to remove the actual blob Object or not\n        \"\"\"\n        if self.container is None or self.service is None:\n            raise RuntimeError(\"The store has not been connected\")\n\n        if not remove_blob_object:\n            self.index.remove_docs(criteria=criteria)\n        else:\n            to_remove = self.index.distinct(self.key, criteria=criteria)\n            self.index.remove_docs(criteria=criteria)\n\n            # Can remove up to 256 items at a time\n            to_remove_chunks = list(grouper(to_remove, n=256))\n            for chunk_to_remove in to_remove_chunks:\n                objlist = [{\"name\": f\"{self.sub_dir}{obj}\"} for obj in chunk_to_remove]\n                self.container.delete_blobs(*objlist)\n\n    @property\n    def last_updated(self):\n        return self.index.last_updated\n\n    def newer_in(self, target: Store, criteria: Optional[dict] = None, exhaustive: bool = False) -&gt; list[str]:\n        \"\"\"\n        Returns the keys of documents that are newer in the target\n        Store than this Store.\n\n        Args:\n            target: target Store\n            criteria: PyMongo filter for documents to search in\n            exhaustive: triggers an item-by-item check vs. checking\n                        the last_updated of the target Store and using\n                        that to filter out new items in\n        \"\"\"\n        if hasattr(target, \"index\"):\n            return self.index.newer_in(target=target.index, criteria=criteria, exhaustive=exhaustive)\n\n        return self.index.newer_in(target=target, criteria=criteria, exhaustive=exhaustive)\n\n    def __hash__(self):\n        return hash((self.index.__hash__, self.container_name))\n\n    def rebuild_index_from_blob_data(self, **kwargs):\n        \"\"\"\n        Rebuilds the index Store from the data in Azure\n        Relies on the index document being stores as the metadata for the file\n        This can help recover lost databases.\n        \"\"\"\n        objects = self.container.list_blobs(name_starts_with=self.sub_dir)\n        for obj in objects:\n            # handle the case where there are subdirs in the chosen container\n            # but are below the level of the current subdir\n            dir_name = os.path.dirname(obj.name)\n            if dir_name != self.sub_dir:\n                continue\n\n            data = self.container.download_blob(obj.name).readall()\n\n            if self.compress:\n                data = zlib.decompress(data)\n            unpacked_data = msgpack.unpackb(data, raw=False)\n            # TODO maybe it can be avoided to reupload the data, since it is paid\n            self.update(unpacked_data, **kwargs)\n\n    def rebuild_metadata_from_index(self, index_query: Optional[dict] = None):\n        \"\"\"\n        Read data from the index store and populate the metadata of the Azure Blob.\n        Force all of the keys to be lower case to be Minio compatible\n\n        Args:\n            index_query: query on the index store.\n        \"\"\"\n        if self.container is None or self.service is None:\n            raise RuntimeError(\"The store has not been connected\")\n\n        qq = {} if index_query is None else index_query\n        for index_doc in self.index.query(qq):\n            key_ = self.sub_dir + index_doc[self.key]\n            blob = self.container.get_blob_client(key_)\n            properties = blob.get_blob_properties()\n            new_meta = {self._sanitize_key(k): v for k, v in properties.metadata.items()}\n            for k, v in index_doc.items():\n                new_meta[str(k).lower()] = v\n            new_meta.pop(\"_id\")\n            if self.last_updated_field in new_meta:\n                new_meta[self.last_updated_field] = str(to_isoformat_ceil_ms(new_meta[self.last_updated_field]))\n            blob.set_blob_metadata(new_meta)\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"\n        Check equality for AzureBlobStore\n        other: other AzureBlobStore to compare with.\n        \"\"\"\n        if not isinstance(other, AzureBlobStore):\n            return False\n\n        fields = [\"index\", \"container_name\", \"last_updated_field\"]\n        return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.name","title":"<code>name</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>a string representing this data source.</p>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality for AzureBlobStore other: other AzureBlobStore to compare with.</p> Source code in <code>src/maggma/stores/azure.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"\n    Check equality for AzureBlobStore\n    other: other AzureBlobStore to compare with.\n    \"\"\"\n    if not isinstance(other, AzureBlobStore):\n        return False\n\n    fields = [\"index\", \"container_name\", \"last_updated_field\"]\n    return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.__init__","title":"<code>__init__(index, container_name, azure_client_info=None, credential_type='DefaultAzureCredential', compress=False, sub_dir=None, workers=1, azure_resource_kwargs=None, key='fs_id', store_hash=True, unpack_data=True, searchable_fields=None, key_sanitize_dict=None, create_container=False, **kwargs)</code>","text":"<p>Initializes an AzureBlob Store.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>Store</code> <p>a store to use to index the Azure blob</p> required <code>container_name</code> <code>str</code> <p>name of the container</p> required <code>azure_client_info</code> <code>Optional[Union[str, dict]]</code> <p>connection_url of the BlobServiceClient if a string. Assumes that the access is passwordless in that case. Otherwise, if a dictionary, options to instantiate the BlobServiceClient. Currently supported keywords:     - connection_string: a connection string for the Azure blob</p> <code>None</code> <code>credential_type</code> <code>CredentialType</code> <p>the type of credential to use to authenticate with Azure. Default is \"DefaultAzureCredential\".  For serializable stores, provide a string representation of the credential class. Otherwises, you may provide the class itself.</p> <code>'DefaultAzureCredential'</code> <code>compress</code> <code>bool</code> <p>compress files inserted into the store</p> <code>False</code> <code>sub_dir</code> <code>Optional[str]</code> <p>(optional)  subdirectory of the container to store the data. When defined, a final \"/\" will be added if not already present.</p> <code>None</code> <code>workers</code> <code>int</code> <p>number of concurrent Azure puts to run</p> <code>1</code> <code>store_hash</code> <code>bool</code> <p>store the sha1 hash right before insertion to the database.</p> <code>True</code> <code>unpack_data</code> <code>bool</code> <p>whether to decompress and unpack byte data when querying from the container.</p> <code>True</code> <code>searchable_fields</code> <code>Optional[list[str]]</code> <p>fields to keep in the index store</p> <code>None</code> <code>key_sanitize_dict</code> <code>Optional[dict]</code> <p>a dictionary that allows to customize the sanitization of the keys in metadata, since they should adhere to the naming rules for C# identifiers. If None the AZURE_KEY_SANITIZE default will be used to handle the most common cases.</p> <code>None</code> <code>create_container</code> <code>bool</code> <p>if True the Store creates the container, in case it does not exist.</p> <code>False</code> <code>kwargs</code> <p>keywords for the base Store.</p> <code>{}</code> Source code in <code>src/maggma/stores/azure.py</code> <pre><code>def __init__(\n    self,\n    index: Store,\n    container_name: str,\n    azure_client_info: Optional[Union[str, dict]] = None,\n    credential_type: CredentialType = \"DefaultAzureCredential\",\n    compress: bool = False,\n    sub_dir: Optional[str] = None,\n    workers: int = 1,\n    azure_resource_kwargs: Optional[dict] = None,\n    key: str = \"fs_id\",\n    store_hash: bool = True,\n    unpack_data: bool = True,\n    searchable_fields: Optional[list[str]] = None,\n    key_sanitize_dict: Optional[dict] = None,\n    create_container: bool = False,\n    **kwargs,\n):\n    \"\"\"\n    Initializes an AzureBlob Store.\n\n    Args:\n        index: a store to use to index the Azure blob\n        container_name: name of the container\n        azure_client_info: connection_url of the BlobServiceClient if a string.\n            Assumes that the access is passwordless in that case.\n            Otherwise, if a dictionary, options to instantiate the\n            BlobServiceClient.\n            Currently supported keywords:\n                - connection_string: a connection string for the Azure blob\n        credential_type: the type of credential to use to authenticate with Azure.\n            Default is \"DefaultAzureCredential\".  For serializable stores, provide\n            a string representation of the credential class. Otherwises, you may\n            provide the class itself.\n        compress: compress files inserted into the store\n        sub_dir: (optional)  subdirectory of the container to store the data.\n            When defined, a final \"/\" will be added if not already present.\n        workers: number of concurrent Azure puts to run\n        store_hash: store the sha1 hash right before insertion to the database.\n        unpack_data: whether to decompress and unpack byte data when querying from\n            the container.\n        searchable_fields: fields to keep in the index store\n        key_sanitize_dict: a dictionary that allows to customize the sanitization\n            of the keys in metadata, since they should adhere to the naming rules\n            for C# identifiers. If None the AZURE_KEY_SANITIZE default will be used\n            to handle the most common cases.\n        create_container: if True the Store creates the container, in case it does\n            not exist.\n        kwargs: keywords for the base Store.\n    \"\"\"\n    if azure_blob is None:\n        raise RuntimeError(\"azure-storage-blob and azure-identity are required for AzureBlobStore\")\n\n    self.index = index\n    self.container_name = container_name\n    self.azure_client_info = azure_client_info\n    self.compress = compress\n    self.sub_dir = sub_dir.rstrip(\"/\") + \"/\" if sub_dir else \"\"\n    self.service: Optional[BlobServiceClient] = None\n    self.container: Optional[ContainerClient] = None\n    self.workers = workers\n    self.azure_resource_kwargs = azure_resource_kwargs if azure_resource_kwargs is not None else {}\n    self.unpack_data = unpack_data\n    self.searchable_fields = searchable_fields if searchable_fields is not None else []\n    self.store_hash = store_hash\n    if key_sanitize_dict is None:\n        key_sanitize_dict = AZURE_KEY_SANITIZE\n    self.key_sanitize_dict = key_sanitize_dict\n    self.create_container = create_container\n    self.credential_type = credential_type\n\n    # Force the key to be the same as the index\n    assert isinstance(\n        index.key, str\n    ), \"Since we are using the key as a file name in Azure Blob, the key must be a string\"\n    if key != index.key:\n        warnings.warn(\n            f'The desired AzureBlobStore key \"{key}\" does not match the index key \"{index.key},\"'\n            \"the index key will be used\",\n            UserWarning,\n        )\n    kwargs[\"key\"] = str(index.key)\n\n    self._thread_local = threading.local()\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.close","title":"<code>close()</code>","text":"<p>Closes any connections.</p> Source code in <code>src/maggma/stores/azure.py</code> <pre><code>def close(self):\n    \"\"\"\n    Closes any connections.\n    \"\"\"\n    self.index.close()\n    self.service = None\n    self.container = None\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.connect","title":"<code>connect(*args, **kwargs)</code>","text":"<p>Connect to the source data.</p> Source code in <code>src/maggma/stores/azure.py</code> <pre><code>def connect(self, *args, **kwargs):  # lgtm[py/conflicting-attributes]\n    \"\"\"\n    Connect to the source data.\n    \"\"\"\n    service_client = self._get_service_client()\n\n    if not self.service:\n        self.service = service_client\n        container = service_client.get_container_client(self.container_name)\n        if not container.exists():\n            if self.create_container:\n                # catch the exception to avoid errors if already created\n                try:\n                    container.create_container()\n                except ResourceExistsError:\n                    pass\n            else:\n                raise RuntimeError(f\"Container not present on Azure: {self.container_name}\")\n\n        self.container = container\n    self.index.connect(*args, **kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.count","title":"<code>count(criteria=None)</code>","text":"<p>Counts the number of documents matching the query criteria.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to count in</p> <code>None</code> Source code in <code>src/maggma/stores/azure.py</code> <pre><code>def count(self, criteria: Optional[dict] = None) -&gt; int:\n    \"\"\"\n    Counts the number of documents matching the query criteria.\n\n    Args:\n        criteria: PyMongo filter for documents to count in\n    \"\"\"\n    return self.index.count(criteria)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.distinct","title":"<code>distinct(field, criteria=None, all_exist=False)</code>","text":"<p>Get all distinct values for a field.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>the field(s) to get distinct values for</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> Source code in <code>src/maggma/stores/azure.py</code> <pre><code>def distinct(self, field: str, criteria: Optional[dict] = None, all_exist: bool = False) -&gt; list:\n    \"\"\"\n    Get all distinct values for a field.\n\n    Args:\n        field: the field(s) to get distinct values for\n        criteria: PyMongo filter for documents to search in\n    \"\"\"\n    # Index is a store so it should have its own distinct function\n    return self.index.distinct(field, criteria=criteria)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.ensure_index","title":"<code>ensure_index(key, unique=False)</code>","text":"<p>Tries to create an index and return true if it succeeded.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>single key to index</p> required <code>unique</code> <code>bool</code> <p>Whether or not this index contains only unique keys</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>bool indicating if the index exists/was created</p> Source code in <code>src/maggma/stores/azure.py</code> <pre><code>def ensure_index(self, key: str, unique: bool = False) -&gt; bool:\n    \"\"\"\n    Tries to create an index and return true if it succeeded.\n\n    Args:\n        key: single key to index\n        unique: Whether or not this index contains only unique keys\n\n    Returns:\n        bool indicating if the index exists/was created\n    \"\"\"\n    return self.index.ensure_index(key, unique=unique)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.groupby","title":"<code>groupby(keys, criteria=None, properties=None, sort=None, skip=0, limit=0)</code>","text":"<p>Simple grouping function that will group documents by keys.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[list[str], str]</code> <p>fields to group documents</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> <p>Returns:</p> Type Description <code>Iterator[tuple[dict, list[dict]]]</code> <p>generator returning tuples of (dict, list of docs)</p> Source code in <code>src/maggma/stores/azure.py</code> <pre><code>def groupby(\n    self,\n    keys: Union[list[str], str],\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[tuple[dict, list[dict]]]:\n    \"\"\"\n    Simple grouping function that will group documents\n    by keys.\n\n    Args:\n        keys: fields to group documents\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n\n    Returns:\n        generator returning tuples of (dict, list of docs)\n    \"\"\"\n    return self.index.groupby(\n        keys=keys,\n        criteria=criteria,\n        properties=properties,\n        sort=sort,\n        skip=skip,\n        limit=limit,\n    )\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.newer_in","title":"<code>newer_in(target, criteria=None, exhaustive=False)</code>","text":"<p>Returns the keys of documents that are newer in the target Store than this Store.</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Store</code> <p>target Store</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>exhaustive</code> <code>bool</code> <p>triggers an item-by-item check vs. checking         the last_updated of the target Store and using         that to filter out new items in</p> <code>False</code> Source code in <code>src/maggma/stores/azure.py</code> <pre><code>def newer_in(self, target: Store, criteria: Optional[dict] = None, exhaustive: bool = False) -&gt; list[str]:\n    \"\"\"\n    Returns the keys of documents that are newer in the target\n    Store than this Store.\n\n    Args:\n        target: target Store\n        criteria: PyMongo filter for documents to search in\n        exhaustive: triggers an item-by-item check vs. checking\n                    the last_updated of the target Store and using\n                    that to filter out new items in\n    \"\"\"\n    if hasattr(target, \"index\"):\n        return self.index.newer_in(target=target.index, criteria=criteria, exhaustive=exhaustive)\n\n    return self.index.newer_in(target=target, criteria=criteria, exhaustive=exhaustive)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.query","title":"<code>query(criteria=None, properties=None, sort=None, skip=0, limit=0)</code>","text":"<p>Queries the Store for a set of documents.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> Source code in <code>src/maggma/stores/azure.py</code> <pre><code>def query(\n    self,\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[dict]:\n    \"\"\"\n    Queries the Store for a set of documents.\n\n    Args:\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n\n    \"\"\"\n    if self.container is None or self.service is None:\n        raise RuntimeError(\"The store has not been connected\")\n\n    prop_keys = set()\n    if isinstance(properties, dict):\n        prop_keys = set(properties.keys())\n    elif isinstance(properties, list):\n        prop_keys = set(properties)\n\n    for doc in self.index.query(criteria=criteria, sort=sort, limit=limit, skip=skip):\n        if properties is not None and prop_keys.issubset(set(doc.keys())):\n            yield {p: doc[p] for p in properties if p in doc}\n        else:\n            try:\n                data = self.container.download_blob(self.sub_dir + str(doc[self.key])).readall()\n            except azure.core.exceptions.ResourceNotFoundError:\n                self.logger.error(f\"Could not find Blob object {doc[self.key]}\")\n\n            if self.unpack_data:\n                data = self._unpack(data=data, compressed=doc.get(\"compression\", \"\") == \"zlib\")\n\n                if self.last_updated_field in doc:\n                    data[self.last_updated_field] = doc[self.last_updated_field]  # type: ignore\n\n            yield data  # type: ignore\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.rebuild_index_from_blob_data","title":"<code>rebuild_index_from_blob_data(**kwargs)</code>","text":"<p>Rebuilds the index Store from the data in Azure Relies on the index document being stores as the metadata for the file This can help recover lost databases.</p> Source code in <code>src/maggma/stores/azure.py</code> <pre><code>def rebuild_index_from_blob_data(self, **kwargs):\n    \"\"\"\n    Rebuilds the index Store from the data in Azure\n    Relies on the index document being stores as the metadata for the file\n    This can help recover lost databases.\n    \"\"\"\n    objects = self.container.list_blobs(name_starts_with=self.sub_dir)\n    for obj in objects:\n        # handle the case where there are subdirs in the chosen container\n        # but are below the level of the current subdir\n        dir_name = os.path.dirname(obj.name)\n        if dir_name != self.sub_dir:\n            continue\n\n        data = self.container.download_blob(obj.name).readall()\n\n        if self.compress:\n            data = zlib.decompress(data)\n        unpacked_data = msgpack.unpackb(data, raw=False)\n        # TODO maybe it can be avoided to reupload the data, since it is paid\n        self.update(unpacked_data, **kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.rebuild_metadata_from_index","title":"<code>rebuild_metadata_from_index(index_query=None)</code>","text":"<p>Read data from the index store and populate the metadata of the Azure Blob. Force all of the keys to be lower case to be Minio compatible</p> <p>Parameters:</p> Name Type Description Default <code>index_query</code> <code>Optional[dict]</code> <p>query on the index store.</p> <code>None</code> Source code in <code>src/maggma/stores/azure.py</code> <pre><code>def rebuild_metadata_from_index(self, index_query: Optional[dict] = None):\n    \"\"\"\n    Read data from the index store and populate the metadata of the Azure Blob.\n    Force all of the keys to be lower case to be Minio compatible\n\n    Args:\n        index_query: query on the index store.\n    \"\"\"\n    if self.container is None or self.service is None:\n        raise RuntimeError(\"The store has not been connected\")\n\n    qq = {} if index_query is None else index_query\n    for index_doc in self.index.query(qq):\n        key_ = self.sub_dir + index_doc[self.key]\n        blob = self.container.get_blob_client(key_)\n        properties = blob.get_blob_properties()\n        new_meta = {self._sanitize_key(k): v for k, v in properties.metadata.items()}\n        for k, v in index_doc.items():\n            new_meta[str(k).lower()] = v\n        new_meta.pop(\"_id\")\n        if self.last_updated_field in new_meta:\n            new_meta[self.last_updated_field] = str(to_isoformat_ceil_ms(new_meta[self.last_updated_field]))\n        blob.set_blob_metadata(new_meta)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.remove_docs","title":"<code>remove_docs(criteria, remove_blob_object=False)</code>","text":"<p>Remove docs matching the query dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>dict</code> <p>query dictionary to match</p> required <code>remove_blob_object</code> <code>bool</code> <p>whether to remove the actual blob Object or not</p> <code>False</code> Source code in <code>src/maggma/stores/azure.py</code> <pre><code>def remove_docs(self, criteria: dict, remove_blob_object: bool = False):\n    \"\"\"\n    Remove docs matching the query dictionary.\n\n    Args:\n        criteria: query dictionary to match\n        remove_blob_object: whether to remove the actual blob Object or not\n    \"\"\"\n    if self.container is None or self.service is None:\n        raise RuntimeError(\"The store has not been connected\")\n\n    if not remove_blob_object:\n        self.index.remove_docs(criteria=criteria)\n    else:\n        to_remove = self.index.distinct(self.key, criteria=criteria)\n        self.index.remove_docs(criteria=criteria)\n\n        # Can remove up to 256 items at a time\n        to_remove_chunks = list(grouper(to_remove, n=256))\n        for chunk_to_remove in to_remove_chunks:\n            objlist = [{\"name\": f\"{self.sub_dir}{obj}\"} for obj in chunk_to_remove]\n            self.container.delete_blobs(*objlist)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.update","title":"<code>update(docs, key=None, additional_metadata=None)</code>","text":"<p>Update documents into the Store.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[list[dict], dict]</code> <p>the document or list of documents to update</p> required <code>key</code> <code>Union[list, str, None]</code> <p>field name(s) to determine uniqueness for a  document, can be a list of multiple fields,  a single field, or None if the Store's key  field is to be used</p> <code>None</code> <code>additional_metadata</code> <code>Union[str, list[str], None]</code> <p>field(s) to include in the blob store's metadata</p> <code>None</code> Source code in <code>src/maggma/stores/azure.py</code> <pre><code>def update(\n    self,\n    docs: Union[list[dict], dict],\n    key: Union[list, str, None] = None,\n    additional_metadata: Union[str, list[str], None] = None,\n):\n    \"\"\"\n    Update documents into the Store.\n\n    Args:\n        docs: the document or list of documents to update\n        key: field name(s) to determine uniqueness for a\n             document, can be a list of multiple fields,\n             a single field, or None if the Store's key\n             field is to be used\n        additional_metadata: field(s) to include in the blob store's metadata\n    \"\"\"\n    if self.container is None or self.service is None:\n        raise RuntimeError(\"The store has not been connected\")\n\n    if not isinstance(docs, list):\n        docs = [docs]\n\n    if isinstance(key, str):\n        key = [key]\n    elif not key:\n        key = [self.key]\n\n    if additional_metadata is None:\n        additional_metadata = []\n    elif isinstance(additional_metadata, str):\n        additional_metadata = [additional_metadata]\n    else:\n        additional_metadata = list(additional_metadata)\n\n    with ThreadPoolExecutor(max_workers=self.workers) as pool:\n        fs = {\n            pool.submit(\n                self.write_doc_to_blob,\n                doc=itr_doc,\n                search_keys=key + additional_metadata + self.searchable_fields,\n            )\n            for itr_doc in docs\n        }\n        fs, _ = wait(fs)\n\n        search_docs = [sdoc.result() for sdoc in fs]\n\n    # Use store's update to remove key clashes\n    self.index.update(search_docs, key=self.key)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.azure.AzureBlobStore.write_doc_to_blob","title":"<code>write_doc_to_blob(doc, search_keys)</code>","text":"<p>Write the data to an Azure blob and return the metadata to be inserted into the index db.</p> <p>Parameters:</p> Name Type Description Default <code>doc</code> <code>dict</code> <p>the document</p> required <code>search_keys</code> <code>list[str]</code> <p>list of keys to pull from the docs and be inserted into the</p> required Source code in <code>src/maggma/stores/azure.py</code> <pre><code>def write_doc_to_blob(self, doc: dict, search_keys: list[str]):\n    \"\"\"\n    Write the data to an Azure blob and return the metadata to be inserted into the index db.\n\n    Args:\n        doc: the document\n        search_keys: list of keys to pull from the docs and be inserted into the\n        index db\n    \"\"\"\n    container = self._get_container()\n    if container is None:\n        raise RuntimeError(\"The store has not been connected\")\n\n    search_doc = {k: doc[k] for k in search_keys}\n    search_doc[self.key] = doc[self.key]  # Ensure key is in metadata\n    if self.sub_dir != \"\":\n        search_doc[\"sub_dir\"] = self.sub_dir\n\n    # Remove MongoDB _id from search\n    if \"_id\" in search_doc:\n        del search_doc[\"_id\"]\n\n    # to make hashing more meaningful, make sure last updated field is removed\n    lu_info = doc.pop(self.last_updated_field, None)\n    data = msgpack.packb(doc, default=monty_default)\n\n    if self.compress:\n        # Compress with zlib if chosen\n        search_doc[\"compression\"] = \"zlib\"\n        data = zlib.compress(data)\n\n    if self.last_updated_field in doc:\n        # need this conversion for metadata insert\n        search_doc[self.last_updated_field] = str(to_isoformat_ceil_ms(doc[self.last_updated_field]))\n\n    # keep a record of original keys, in case these are important for the individual researcher\n    # it is not expected that this information will be used except in disaster recovery\n    blob_to_mongo_keys = {k: self._sanitize_key(k) for k in search_doc}\n    blob_to_mongo_keys[\"blob_to_mongo_keys\"] = \"blob_to_mongo_keys\"  # inception\n    # encode dictionary since values have to be strings\n    search_doc[\"blob_to_mongo_keys\"] = dumps(blob_to_mongo_keys)\n\n    container.upload_blob(\n        name=self.sub_dir + str(doc[self.key]),\n        data=data,\n        metadata={blob_to_mongo_keys[k]: str(v) for k, v in search_doc.items()},\n        overwrite=True,\n    )\n\n    if lu_info is not None:\n        search_doc[self.last_updated_field] = lu_info\n\n    if self.store_hash:\n        hasher = sha1()\n        hasher.update(data)\n        obj_hash = hasher.hexdigest()\n        search_doc[\"obj_hash\"] = obj_hash\n    return search_doc\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore","title":"<code>AliasingStore</code>","text":"<p>               Bases: <code>Store</code></p> <p>Special Store that aliases for the primary accessors.</p> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>class AliasingStore(Store):\n    \"\"\"\n    Special Store that aliases for the primary accessors.\n    \"\"\"\n\n    def __init__(self, store: Store, aliases: dict, **kwargs):\n        \"\"\"\n        Args:\n            store: the store to wrap around\n            aliases: dict of aliases of the form external key: internal key.\n        \"\"\"\n        self.store = store\n        # Given an external key tells what the internal key is\n        self.aliases = aliases\n        # Given the internal key tells us what the external key is\n        self.reverse_aliases = {v: k for k, v in aliases.items()}\n        self.kwargs = kwargs\n\n        kwargs.update(\n            {\n                \"last_updated_field\": store.last_updated_field,\n                \"last_updated_type\": store.last_updated_type,\n            }\n        )\n        super().__init__(**kwargs)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        Return a string representing this data source.\n        \"\"\"\n        return self.store.name\n\n    def count(self, criteria: Optional[dict] = None) -&gt; int:\n        \"\"\"\n        Counts the number of documents matching the query criteria.\n\n        Args:\n            criteria: PyMongo filter for documents to count in\n        \"\"\"\n        criteria = criteria if criteria else {}\n        lazy_substitute(criteria, self.reverse_aliases)\n        return self.store.count(criteria)\n\n    def query(\n        self,\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[dict]:\n        \"\"\"\n        Queries the Store for a set of documents.\n\n        Args:\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n        \"\"\"\n        criteria = criteria if criteria else {}\n\n        if properties is not None:\n            if isinstance(properties, list):\n                properties = {p: 1 for p in properties}\n            substitute(properties, self.reverse_aliases)\n\n        lazy_substitute(criteria, self.reverse_aliases)\n        for d in self.store.query(properties=properties, criteria=criteria, sort=sort, limit=limit, skip=skip):\n            substitute(d, self.aliases)\n            yield d\n\n    def distinct(self, field: str, criteria: Optional[dict] = None, all_exist: bool = False) -&gt; list:\n        \"\"\"\n        Get all distinct values for a field.\n\n        Args:\n            field: the field(s) to get distinct values for\n            criteria: PyMongo filter for documents to search in\n        \"\"\"\n        criteria = criteria if criteria else {}\n        lazy_substitute(criteria, self.reverse_aliases)\n\n        # substitute forward\n        return self.store.distinct(self.aliases[field], criteria=criteria)\n\n    def groupby(\n        self,\n        keys: Union[list[str], str],\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[tuple[dict, list[dict]]]:\n        \"\"\"\n        Simple grouping function that will group documents\n        by keys.\n\n        Args:\n            keys: fields to group documents\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n\n        Returns:\n            generator returning tuples of (dict, list of docs)\n        \"\"\"\n        # Convert to a list\n        keys = keys if isinstance(keys, list) else [keys]\n\n        # Make the aliasing transformations on keys\n        keys = [self.aliases.get(k, k) for k in keys]\n\n        # Update criteria and properties based on aliases\n        criteria = criteria if criteria else {}\n\n        if properties is not None:\n            if isinstance(properties, list):\n                properties = {p: 1 for p in properties}\n            substitute(properties, self.reverse_aliases)\n\n        lazy_substitute(criteria, self.reverse_aliases)\n\n        return self.store.groupby(keys=keys, properties=properties, criteria=criteria, skip=skip, limit=limit)\n\n    def update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n        \"\"\"\n        Update documents into the Store.\n\n        Args:\n            docs: the document or list of documents to update\n            key: field name(s) to determine uniqueness for a\n                 document, can be a list of multiple fields,\n                 a single field, or None if the Store's key\n                 field is to be used\n        \"\"\"\n        key = key if key else self.key\n\n        for d in docs:\n            substitute(d, self.reverse_aliases)\n\n        if key in self.aliases:\n            key = self.aliases[key]\n\n        self.store.update(docs, key=key)\n\n    def remove_docs(self, criteria: dict):\n        \"\"\"\n        Remove docs matching the query dictionary.\n\n        Args:\n            criteria: query dictionary to match\n        \"\"\"\n        # Update criteria and properties based on aliases\n        lazy_substitute(criteria, self.reverse_aliases)\n        self.store.remove_docs(criteria)\n\n    def ensure_index(self, key, unique=False, **kwargs):\n        if key in self.aliases:\n            key = self.aliases\n        return self.store.ensure_index(key, unique, **kwargs)\n\n    def close(self):\n        self.store.close()\n\n    @property\n    def _collection(self):\n        return self.store._collection\n\n    def connect(self, force_reset=False):\n        self.store.connect(force_reset=force_reset)\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"\n        Check equality for AliasingStore.\n\n        Args:\n            other: other AliasingStore to compare with\n        \"\"\"\n        if not isinstance(other, AliasingStore):\n            return False\n\n        fields = [\"store\", \"aliases\", \"last_updated_field\"]\n        return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return a string representing this data source.</p>"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality for AliasingStore.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>object</code> <p>other AliasingStore to compare with</p> required Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"\n    Check equality for AliasingStore.\n\n    Args:\n        other: other AliasingStore to compare with\n    \"\"\"\n    if not isinstance(other, AliasingStore):\n        return False\n\n    fields = [\"store\", \"aliases\", \"last_updated_field\"]\n    return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.__init__","title":"<code>__init__(store, aliases, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>store</code> <code>Store</code> <p>the store to wrap around</p> required <code>aliases</code> <code>dict</code> <p>dict of aliases of the form external key: internal key.</p> required Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def __init__(self, store: Store, aliases: dict, **kwargs):\n    \"\"\"\n    Args:\n        store: the store to wrap around\n        aliases: dict of aliases of the form external key: internal key.\n    \"\"\"\n    self.store = store\n    # Given an external key tells what the internal key is\n    self.aliases = aliases\n    # Given the internal key tells us what the external key is\n    self.reverse_aliases = {v: k for k, v in aliases.items()}\n    self.kwargs = kwargs\n\n    kwargs.update(\n        {\n            \"last_updated_field\": store.last_updated_field,\n            \"last_updated_type\": store.last_updated_type,\n        }\n    )\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.count","title":"<code>count(criteria=None)</code>","text":"<p>Counts the number of documents matching the query criteria.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to count in</p> <code>None</code> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def count(self, criteria: Optional[dict] = None) -&gt; int:\n    \"\"\"\n    Counts the number of documents matching the query criteria.\n\n    Args:\n        criteria: PyMongo filter for documents to count in\n    \"\"\"\n    criteria = criteria if criteria else {}\n    lazy_substitute(criteria, self.reverse_aliases)\n    return self.store.count(criteria)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.distinct","title":"<code>distinct(field, criteria=None, all_exist=False)</code>","text":"<p>Get all distinct values for a field.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>the field(s) to get distinct values for</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def distinct(self, field: str, criteria: Optional[dict] = None, all_exist: bool = False) -&gt; list:\n    \"\"\"\n    Get all distinct values for a field.\n\n    Args:\n        field: the field(s) to get distinct values for\n        criteria: PyMongo filter for documents to search in\n    \"\"\"\n    criteria = criteria if criteria else {}\n    lazy_substitute(criteria, self.reverse_aliases)\n\n    # substitute forward\n    return self.store.distinct(self.aliases[field], criteria=criteria)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.groupby","title":"<code>groupby(keys, criteria=None, properties=None, sort=None, skip=0, limit=0)</code>","text":"<p>Simple grouping function that will group documents by keys.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[list[str], str]</code> <p>fields to group documents</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> <p>Returns:</p> Type Description <code>Iterator[tuple[dict, list[dict]]]</code> <p>generator returning tuples of (dict, list of docs)</p> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def groupby(\n    self,\n    keys: Union[list[str], str],\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[tuple[dict, list[dict]]]:\n    \"\"\"\n    Simple grouping function that will group documents\n    by keys.\n\n    Args:\n        keys: fields to group documents\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n\n    Returns:\n        generator returning tuples of (dict, list of docs)\n    \"\"\"\n    # Convert to a list\n    keys = keys if isinstance(keys, list) else [keys]\n\n    # Make the aliasing transformations on keys\n    keys = [self.aliases.get(k, k) for k in keys]\n\n    # Update criteria and properties based on aliases\n    criteria = criteria if criteria else {}\n\n    if properties is not None:\n        if isinstance(properties, list):\n            properties = {p: 1 for p in properties}\n        substitute(properties, self.reverse_aliases)\n\n    lazy_substitute(criteria, self.reverse_aliases)\n\n    return self.store.groupby(keys=keys, properties=properties, criteria=criteria, skip=skip, limit=limit)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.query","title":"<code>query(criteria=None, properties=None, sort=None, skip=0, limit=0)</code>","text":"<p>Queries the Store for a set of documents.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def query(\n    self,\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[dict]:\n    \"\"\"\n    Queries the Store for a set of documents.\n\n    Args:\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n    \"\"\"\n    criteria = criteria if criteria else {}\n\n    if properties is not None:\n        if isinstance(properties, list):\n            properties = {p: 1 for p in properties}\n        substitute(properties, self.reverse_aliases)\n\n    lazy_substitute(criteria, self.reverse_aliases)\n    for d in self.store.query(properties=properties, criteria=criteria, sort=sort, limit=limit, skip=skip):\n        substitute(d, self.aliases)\n        yield d\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.remove_docs","title":"<code>remove_docs(criteria)</code>","text":"<p>Remove docs matching the query dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>dict</code> <p>query dictionary to match</p> required Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def remove_docs(self, criteria: dict):\n    \"\"\"\n    Remove docs matching the query dictionary.\n\n    Args:\n        criteria: query dictionary to match\n    \"\"\"\n    # Update criteria and properties based on aliases\n    lazy_substitute(criteria, self.reverse_aliases)\n    self.store.remove_docs(criteria)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.AliasingStore.update","title":"<code>update(docs, key=None)</code>","text":"<p>Update documents into the Store.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[list[dict], dict]</code> <p>the document or list of documents to update</p> required <code>key</code> <code>Union[list, str, None]</code> <p>field name(s) to determine uniqueness for a  document, can be a list of multiple fields,  a single field, or None if the Store's key  field is to be used</p> <code>None</code> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n    \"\"\"\n    Update documents into the Store.\n\n    Args:\n        docs: the document or list of documents to update\n        key: field name(s) to determine uniqueness for a\n             document, can be a list of multiple fields,\n             a single field, or None if the Store's key\n             field is to be used\n    \"\"\"\n    key = key if key else self.key\n\n    for d in docs:\n        substitute(d, self.reverse_aliases)\n\n    if key in self.aliases:\n        key = self.aliases[key]\n\n    self.store.update(docs, key=key)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore","title":"<code>MongograntStore</code>","text":"<p>               Bases: <code>MongoStore</code></p> <p>Initialize a Store with a mongogrant \"<code>&lt;role&gt;</code>:<code>&lt;host&gt;</code>/<code>&lt;db&gt;</code>.\" spec.</p> <p>Some class methods of MongoStore, e.g. from_db_file and from_collection, are not supported.</p> <p>mongogrant documentation: https://github.com/materialsproject/mongogrant</p> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>@deprecated(MongoStore)\nclass MongograntStore(MongoStore):\n    \"\"\"Initialize a Store with a mongogrant \"`&lt;role&gt;`:`&lt;host&gt;`/`&lt;db&gt;`.\" spec.\n\n    Some class methods of MongoStore, e.g. from_db_file and from_collection,\n    are not supported.\n\n    mongogrant documentation: https://github.com/materialsproject/mongogrant\n    \"\"\"\n\n    @requires(\n        Client is not None,\n        \"mongogrant is required to use MongoGrantStore. Please run `pip install maggma[mongogrant]\",\n    )\n    def __init__(\n        self,\n        mongogrant_spec: str,\n        collection_name: str,\n        mgclient_config_path: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            mongogrant_spec: of the form `&lt;role&gt;`:`&lt;host&gt;`/`&lt;db&gt;`, where\n                role is one of {\"read\", \"readWrite\"} or aliases {\"ro\", \"rw\"};\n                host is a db host (w/ optional port) or alias; and db is a db\n                on that host, or alias. See mongogrant documentation.\n            collection_name: name of mongo collection\n            mgclient_config_path: Path to mongogrant client config file,\n               or None if default path (`mongogrant.client.path`).\n        \"\"\"\n        self.mongogrant_spec = mongogrant_spec\n        self.collection_name = collection_name\n        self.mgclient_config_path = mgclient_config_path\n        self._coll = None\n\n        if self.mgclient_config_path:\n            config = Config(check=check, path=self.mgclient_config_path)\n            client = Client(config)\n        else:\n            client = Client()\n\n        if {\"username\", \"password\", \"database\", \"host\"} &amp; set(kwargs):\n            raise StoreError(\n                \"MongograntStore does not accept \"\n                \"username, password, database, or host \"\n                \"arguments. Use `mongogrant_spec`.\"\n            )\n\n        self.kwargs = kwargs\n        _auth_info = client.get_db_auth_from_spec(self.mongogrant_spec)\n        super().__init__(\n            host=_auth_info[\"host\"],\n            database=_auth_info[\"authSource\"],\n            username=_auth_info[\"username\"],\n            password=_auth_info[\"password\"],\n            collection_name=self.collection_name,\n            **kwargs,\n        )\n\n    @property\n    def name(self):\n        return f\"mgrant://{self.mongogrant_spec}/{self.collection_name}\"\n\n    def __hash__(self):\n        return hash((self.mongogrant_spec, self.collection_name, self.last_updated_field))\n\n    @classmethod\n    def from_db_file(cls, file):\n        \"\"\"\n        Raises ValueError since MongograntStores can't be initialized from a file.\n        \"\"\"\n        raise ValueError(\"MongograntStore doesn't implement from_db_file\")\n\n    @classmethod\n    def from_collection(cls, collection):\n        \"\"\"\n        Raises ValueError since MongograntStores can't be initialized from a PyMongo collection.\n        \"\"\"\n        raise ValueError(\"MongograntStore doesn't implement from_collection\")\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"\n        Check equality for MongograntStore.\n\n        Args:\n            other: other MongograntStore to compare with\n        \"\"\"\n        if not isinstance(other, MongograntStore):\n            return False\n\n        fields = [\n            \"mongogrant_spec\",\n            \"collection_name\",\n            \"mgclient_config_path\",\n            \"last_updated_field\",\n        ]\n        return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality for MongograntStore.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>object</code> <p>other MongograntStore to compare with</p> required Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"\n    Check equality for MongograntStore.\n\n    Args:\n        other: other MongograntStore to compare with\n    \"\"\"\n    if not isinstance(other, MongograntStore):\n        return False\n\n    fields = [\n        \"mongogrant_spec\",\n        \"collection_name\",\n        \"mgclient_config_path\",\n        \"last_updated_field\",\n    ]\n    return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.__init__","title":"<code>__init__(mongogrant_spec, collection_name, mgclient_config_path=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>mongogrant_spec</code> <code>str</code> <p>of the form <code>&lt;role&gt;</code>:<code>&lt;host&gt;</code>/<code>&lt;db&gt;</code>, where role is one of {\"read\", \"readWrite\"} or aliases {\"ro\", \"rw\"}; host is a db host (w/ optional port) or alias; and db is a db on that host, or alias. See mongogrant documentation.</p> required <code>collection_name</code> <code>str</code> <p>name of mongo collection</p> required <code>mgclient_config_path</code> <code>Optional[str]</code> <p>Path to mongogrant client config file, or None if default path (<code>mongogrant.client.path</code>).</p> <code>None</code> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>@requires(\n    Client is not None,\n    \"mongogrant is required to use MongoGrantStore. Please run `pip install maggma[mongogrant]\",\n)\ndef __init__(\n    self,\n    mongogrant_spec: str,\n    collection_name: str,\n    mgclient_config_path: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        mongogrant_spec: of the form `&lt;role&gt;`:`&lt;host&gt;`/`&lt;db&gt;`, where\n            role is one of {\"read\", \"readWrite\"} or aliases {\"ro\", \"rw\"};\n            host is a db host (w/ optional port) or alias; and db is a db\n            on that host, or alias. See mongogrant documentation.\n        collection_name: name of mongo collection\n        mgclient_config_path: Path to mongogrant client config file,\n           or None if default path (`mongogrant.client.path`).\n    \"\"\"\n    self.mongogrant_spec = mongogrant_spec\n    self.collection_name = collection_name\n    self.mgclient_config_path = mgclient_config_path\n    self._coll = None\n\n    if self.mgclient_config_path:\n        config = Config(check=check, path=self.mgclient_config_path)\n        client = Client(config)\n    else:\n        client = Client()\n\n    if {\"username\", \"password\", \"database\", \"host\"} &amp; set(kwargs):\n        raise StoreError(\n            \"MongograntStore does not accept \"\n            \"username, password, database, or host \"\n            \"arguments. Use `mongogrant_spec`.\"\n        )\n\n    self.kwargs = kwargs\n    _auth_info = client.get_db_auth_from_spec(self.mongogrant_spec)\n    super().__init__(\n        host=_auth_info[\"host\"],\n        database=_auth_info[\"authSource\"],\n        username=_auth_info[\"username\"],\n        password=_auth_info[\"password\"],\n        collection_name=self.collection_name,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.from_collection","title":"<code>from_collection(collection)</code>  <code>classmethod</code>","text":"<p>Raises ValueError since MongograntStores can't be initialized from a PyMongo collection.</p> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>@classmethod\ndef from_collection(cls, collection):\n    \"\"\"\n    Raises ValueError since MongograntStores can't be initialized from a PyMongo collection.\n    \"\"\"\n    raise ValueError(\"MongograntStore doesn't implement from_collection\")\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.MongograntStore.from_db_file","title":"<code>from_db_file(file)</code>  <code>classmethod</code>","text":"<p>Raises ValueError since MongograntStores can't be initialized from a file.</p> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>@classmethod\ndef from_db_file(cls, file):\n    \"\"\"\n    Raises ValueError since MongograntStores can't be initialized from a file.\n    \"\"\"\n    raise ValueError(\"MongograntStore doesn't implement from_db_file\")\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore","title":"<code>SandboxStore</code>","text":"<p>               Bases: <code>Store</code></p> <p>Provides a sandboxed view to another store.</p> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>class SandboxStore(Store):\n    \"\"\"\n    Provides a sandboxed view to another store.\n    \"\"\"\n\n    def __init__(self, store: Store, sandbox: str, exclusive: bool = False):\n        \"\"\"\n        Args:\n            store: store to wrap sandboxing around\n            sandbox: the corresponding sandbox\n            exclusive: whether to be exclusively in this sandbox or include global items.\n        \"\"\"\n        self.store = store\n        self.sandbox = sandbox\n        self.exclusive = exclusive\n        super().__init__(\n            key=self.store.key,\n            last_updated_field=self.store.last_updated_field,\n            last_updated_type=self.store.last_updated_type,\n            validator=self.store.validator,\n        )\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        Returns:\n            a string representing this data source.\n        \"\"\"\n        return f\"Sandbox[{self.store.name}][{self.sandbox}]\"\n\n    @property\n    def sbx_criteria(self) -&gt; dict:\n        \"\"\"\n        Returns:\n            the sandbox criteria dict used to filter the source store.\n        \"\"\"\n        if self.exclusive:\n            return {\"sbxn\": self.sandbox}\n        return {\"$or\": [{\"sbxn\": {\"$in\": [self.sandbox]}}, {\"sbxn\": {\"$exists\": False}}]}\n\n    def count(self, criteria: Optional[dict] = None) -&gt; int:\n        \"\"\"\n        Counts the number of documents matching the query criteria.\n\n        Args:\n            criteria: PyMongo filter for documents to count in\n        \"\"\"\n        criteria = dict(**criteria, **self.sbx_criteria) if criteria else self.sbx_criteria\n        return self.store.count(criteria=criteria)\n\n    def query(\n        self,\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[dict]:\n        \"\"\"\n        Queries the Store for a set of documents.\n\n        Args:\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n        \"\"\"\n        criteria = dict(**criteria, **self.sbx_criteria) if criteria else self.sbx_criteria\n        return self.store.query(properties=properties, criteria=criteria, sort=sort, limit=limit, skip=skip)\n\n    def groupby(\n        self,\n        keys: Union[list[str], str],\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[tuple[dict, list[dict]]]:\n        \"\"\"\n        Simple grouping function that will group documents\n        by keys.\n\n        Args:\n            keys: fields to group documents\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n\n        Returns:\n            generator returning tuples of (dict, list of docs)\n        \"\"\"\n        criteria = dict(**criteria, **self.sbx_criteria) if criteria else self.sbx_criteria\n\n        return self.store.groupby(keys=keys, properties=properties, criteria=criteria, skip=skip, limit=limit)\n\n    def update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n        \"\"\"\n        Update documents into the Store.\n\n        Args:\n            docs: the document or list of documents to update\n            key: field name(s) to determine uniqueness for a\n                 document, can be a list of multiple fields,\n                 a single field, or None if the Store's key\n                 field is to be used\n        \"\"\"\n        for d in docs:\n            if \"sbxn\" in d:\n                d[\"sbxn\"] = list(set(d[\"sbxn\"] + [self.sandbox]))\n            else:\n                d[\"sbxn\"] = [self.sandbox]\n\n        self.store.update(docs, key=key)\n\n    def remove_docs(self, criteria: dict):\n        \"\"\"\n        Remove docs matching the query dictionary.\n\n        Args:\n            criteria: query dictionary to match\n        \"\"\"\n        # Update criteria and properties based on aliases\n        criteria = dict(**criteria, **self.sbx_criteria) if criteria else self.sbx_criteria\n        self.store.remove_docs(criteria)\n\n    def ensure_index(self, key, unique=False, **kwargs):\n        return self.store.ensure_index(key, unique, **kwargs)\n\n    def close(self):\n        self.store.close()\n\n    @property\n    def _collection(self):\n        return self.store._collection\n\n    def connect(self, force_reset=False):\n        self.store.connect(force_reset=force_reset)\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"\n        Check equality for SandboxStore.\n\n        Args:\n            other: other SandboxStore to compare with\n        \"\"\"\n        if not isinstance(other, SandboxStore):\n            return False\n\n        fields = [\"store\", \"sandbox\", \"last_updated_field\"]\n        return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.name","title":"<code>name</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>str</code> <p>a string representing this data source.</p>"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.sbx_criteria","title":"<code>sbx_criteria</code>  <code>property</code>","text":"<p>Returns:</p> Type Description <code>dict</code> <p>the sandbox criteria dict used to filter the source store.</p>"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality for SandboxStore.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>object</code> <p>other SandboxStore to compare with</p> required Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"\n    Check equality for SandboxStore.\n\n    Args:\n        other: other SandboxStore to compare with\n    \"\"\"\n    if not isinstance(other, SandboxStore):\n        return False\n\n    fields = [\"store\", \"sandbox\", \"last_updated_field\"]\n    return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.__init__","title":"<code>__init__(store, sandbox, exclusive=False)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>store</code> <code>Store</code> <p>store to wrap sandboxing around</p> required <code>sandbox</code> <code>str</code> <p>the corresponding sandbox</p> required <code>exclusive</code> <code>bool</code> <p>whether to be exclusively in this sandbox or include global items.</p> <code>False</code> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def __init__(self, store: Store, sandbox: str, exclusive: bool = False):\n    \"\"\"\n    Args:\n        store: store to wrap sandboxing around\n        sandbox: the corresponding sandbox\n        exclusive: whether to be exclusively in this sandbox or include global items.\n    \"\"\"\n    self.store = store\n    self.sandbox = sandbox\n    self.exclusive = exclusive\n    super().__init__(\n        key=self.store.key,\n        last_updated_field=self.store.last_updated_field,\n        last_updated_type=self.store.last_updated_type,\n        validator=self.store.validator,\n    )\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.count","title":"<code>count(criteria=None)</code>","text":"<p>Counts the number of documents matching the query criteria.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to count in</p> <code>None</code> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def count(self, criteria: Optional[dict] = None) -&gt; int:\n    \"\"\"\n    Counts the number of documents matching the query criteria.\n\n    Args:\n        criteria: PyMongo filter for documents to count in\n    \"\"\"\n    criteria = dict(**criteria, **self.sbx_criteria) if criteria else self.sbx_criteria\n    return self.store.count(criteria=criteria)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.groupby","title":"<code>groupby(keys, criteria=None, properties=None, sort=None, skip=0, limit=0)</code>","text":"<p>Simple grouping function that will group documents by keys.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[list[str], str]</code> <p>fields to group documents</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> <p>Returns:</p> Type Description <code>Iterator[tuple[dict, list[dict]]]</code> <p>generator returning tuples of (dict, list of docs)</p> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def groupby(\n    self,\n    keys: Union[list[str], str],\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[tuple[dict, list[dict]]]:\n    \"\"\"\n    Simple grouping function that will group documents\n    by keys.\n\n    Args:\n        keys: fields to group documents\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n\n    Returns:\n        generator returning tuples of (dict, list of docs)\n    \"\"\"\n    criteria = dict(**criteria, **self.sbx_criteria) if criteria else self.sbx_criteria\n\n    return self.store.groupby(keys=keys, properties=properties, criteria=criteria, skip=skip, limit=limit)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.query","title":"<code>query(criteria=None, properties=None, sort=None, skip=0, limit=0)</code>","text":"<p>Queries the Store for a set of documents.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def query(\n    self,\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[dict]:\n    \"\"\"\n    Queries the Store for a set of documents.\n\n    Args:\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n    \"\"\"\n    criteria = dict(**criteria, **self.sbx_criteria) if criteria else self.sbx_criteria\n    return self.store.query(properties=properties, criteria=criteria, sort=sort, limit=limit, skip=skip)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.remove_docs","title":"<code>remove_docs(criteria)</code>","text":"<p>Remove docs matching the query dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>dict</code> <p>query dictionary to match</p> required Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def remove_docs(self, criteria: dict):\n    \"\"\"\n    Remove docs matching the query dictionary.\n\n    Args:\n        criteria: query dictionary to match\n    \"\"\"\n    # Update criteria and properties based on aliases\n    criteria = dict(**criteria, **self.sbx_criteria) if criteria else self.sbx_criteria\n    self.store.remove_docs(criteria)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.SandboxStore.update","title":"<code>update(docs, key=None)</code>","text":"<p>Update documents into the Store.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[list[dict], dict]</code> <p>the document or list of documents to update</p> required <code>key</code> <code>Union[list, str, None]</code> <p>field name(s) to determine uniqueness for a  document, can be a list of multiple fields,  a single field, or None if the Store's key  field is to be used</p> <code>None</code> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n    \"\"\"\n    Update documents into the Store.\n\n    Args:\n        docs: the document or list of documents to update\n        key: field name(s) to determine uniqueness for a\n             document, can be a list of multiple fields,\n             a single field, or None if the Store's key\n             field is to be used\n    \"\"\"\n    for d in docs:\n        if \"sbxn\" in d:\n            d[\"sbxn\"] = list(set(d[\"sbxn\"] + [self.sandbox]))\n        else:\n            d[\"sbxn\"] = [self.sandbox]\n\n    self.store.update(docs, key=key)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.VaultStore","title":"<code>VaultStore</code>","text":"<p>               Bases: <code>MongoStore</code></p> <p>Extends MongoStore to read credentials out of Vault server and uses these values to initialize MongoStore instance.</p> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>class VaultStore(MongoStore):\n    \"\"\"\n    Extends MongoStore to read credentials out of Vault server\n    and uses these values to initialize MongoStore instance.\n    \"\"\"\n\n    @requires(hvac is not None, \"hvac is required to use VaultStore\")\n    def __init__(self, collection_name: str, vault_secret_path: str):\n        \"\"\"\n        Args:\n            collection_name: name of mongo collection\n            vault_secret_path: path on vault server with mongo creds object.\n\n        Important:\n            Environment variables that must be set prior to invocation\n            VAULT_ADDR - URL of vault server (eg. https://matgen8.lbl.gov:8200)\n            VAULT_TOKEN or GITHUB_TOKEN - token used to authenticate to vault\n        \"\"\"\n        self.collection_name = collection_name\n        self.vault_secret_path = vault_secret_path\n\n        # TODO: Switch this over to Pydantic ConfigSettings\n        vault_addr = os.getenv(\"VAULT_ADDR\")\n\n        if not vault_addr:\n            raise RuntimeError(\"VAULT_ADDR not set\")\n\n        client = hvac.Client(vault_addr)\n\n        # If we have a vault token use this\n        token = os.getenv(\"VAULT_TOKEN\")\n\n        # Look for a github token instead\n        if not token:\n            github_token = os.getenv(\"GITHUB_TOKEN\")\n\n            if github_token:\n                client.auth_github(github_token)\n            else:\n                raise RuntimeError(\"VAULT_TOKEN or GITHUB_TOKEN not set\")\n        else:\n            client.token = token\n            if not client.is_authenticated():\n                raise RuntimeError(\"Bad token\")\n\n        # Read the vault secret\n        json_db_creds = client.read(vault_secret_path)\n        db_creds = json.loads(json_db_creds[\"data\"][\"value\"])\n\n        database = db_creds.get(\"db\")\n        host = db_creds.get(\"host\", \"localhost\")\n        port = db_creds.get(\"port\", 27017)\n        username = db_creds.get(\"username\", \"\")\n        password = db_creds.get(\"password\", \"\")\n\n        super().__init__(database, collection_name, host, port, username, password)\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"\n        Check equality for VaultStore.\n\n        Args:\n            other: other VaultStore to compare with\n        \"\"\"\n        if not isinstance(other, VaultStore):\n            return False\n\n        fields = [\"vault_secret_path\", \"collection_name\", \"last_updated_field\"]\n        return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.VaultStore.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality for VaultStore.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>object</code> <p>other VaultStore to compare with</p> required Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"\n    Check equality for VaultStore.\n\n    Args:\n        other: other VaultStore to compare with\n    \"\"\"\n    if not isinstance(other, VaultStore):\n        return False\n\n    fields = [\"vault_secret_path\", \"collection_name\", \"last_updated_field\"]\n    return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.advanced_stores.VaultStore.__init__","title":"<code>__init__(collection_name, vault_secret_path)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>collection_name</code> <code>str</code> <p>name of mongo collection</p> required <code>vault_secret_path</code> <code>str</code> <p>path on vault server with mongo creds object.</p> required Important <p>Environment variables that must be set prior to invocation VAULT_ADDR - URL of vault server (eg. https://matgen8.lbl.gov:8200) VAULT_TOKEN or GITHUB_TOKEN - token used to authenticate to vault</p> Source code in <code>src/maggma/stores/advanced_stores.py</code> <pre><code>@requires(hvac is not None, \"hvac is required to use VaultStore\")\ndef __init__(self, collection_name: str, vault_secret_path: str):\n    \"\"\"\n    Args:\n        collection_name: name of mongo collection\n        vault_secret_path: path on vault server with mongo creds object.\n\n    Important:\n        Environment variables that must be set prior to invocation\n        VAULT_ADDR - URL of vault server (eg. https://matgen8.lbl.gov:8200)\n        VAULT_TOKEN or GITHUB_TOKEN - token used to authenticate to vault\n    \"\"\"\n    self.collection_name = collection_name\n    self.vault_secret_path = vault_secret_path\n\n    # TODO: Switch this over to Pydantic ConfigSettings\n    vault_addr = os.getenv(\"VAULT_ADDR\")\n\n    if not vault_addr:\n        raise RuntimeError(\"VAULT_ADDR not set\")\n\n    client = hvac.Client(vault_addr)\n\n    # If we have a vault token use this\n    token = os.getenv(\"VAULT_TOKEN\")\n\n    # Look for a github token instead\n    if not token:\n        github_token = os.getenv(\"GITHUB_TOKEN\")\n\n        if github_token:\n            client.auth_github(github_token)\n        else:\n            raise RuntimeError(\"VAULT_TOKEN or GITHUB_TOKEN not set\")\n    else:\n        client.token = token\n        if not client.is_authenticated():\n            raise RuntimeError(\"Bad token\")\n\n    # Read the vault secret\n    json_db_creds = client.read(vault_secret_path)\n    db_creds = json.loads(json_db_creds[\"data\"][\"value\"])\n\n    database = db_creds.get(\"db\")\n    host = db_creds.get(\"host\", \"localhost\")\n    port = db_creds.get(\"port\", 27017)\n    username = db_creds.get(\"username\", \"\")\n    password = db_creds.get(\"password\", \"\")\n\n    super().__init__(database, collection_name, host, port, username, password)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore","title":"<code>ConcatStore</code>","text":"<p>               Bases: <code>Store</code></p> <p>Store concatting multiple stores.</p> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>class ConcatStore(Store):\n    \"\"\"Store concatting multiple stores.\"\"\"\n\n    def __init__(self, stores: list[Store], **kwargs):\n        \"\"\"\n        Initialize a ConcatStore that concatenates multiple stores together\n        to appear as one store.\n\n        Args:\n            stores: list of stores to concatenate together\n        \"\"\"\n        self.stores = stores\n        self.kwargs = kwargs\n        super().__init__(**kwargs)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        A string representing this data source.\n        \"\"\"\n        compound_name = \",\".join([store.name for store in self.stores])\n        return f\"Concat[{compound_name}]\"\n\n    def connect(self, force_reset: bool = False):\n        \"\"\"\n        Connect all stores in this ConcatStore.\n\n        Args:\n            force_reset: Whether to forcibly reset the connection for all stores\n        \"\"\"\n        for store in self.stores:\n            store.connect(force_reset)\n\n    def close(self):\n        \"\"\"\n        Close all connections in this ConcatStore.\n        \"\"\"\n        for store in self.stores:\n            store.close()\n\n    @property\n    def _collection(self):\n        raise NotImplementedError(\"No collection property for ConcatStore\")\n\n    @property\n    def last_updated(self) -&gt; datetime:\n        \"\"\"\n        Finds the most recent last_updated across all the stores.\n        This might not be the most useful way to do this for this type of Store\n        since it could very easily over-estimate the last_updated based on what stores\n        are used.\n        \"\"\"\n        lus = []\n        for store in self.stores:\n            lu = store.last_updated\n            lus.append(lu)\n        return max(lus)\n\n    def update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n        \"\"\"\n        Update documents into the Store\n        Not implemented in ConcatStore.\n\n        Args:\n            docs: the document or list of documents to update\n            key: field name(s) to determine uniqueness for a\n                 document, can be a list of multiple fields,\n                 a single field, or None if the Store's key\n                 field is to be used\n        \"\"\"\n        raise NotImplementedError(\"No update method for ConcatStore\")\n\n    def distinct(self, field: str, criteria: Optional[dict] = None, all_exist: bool = False) -&gt; list:\n        \"\"\"\n        Get all distinct values for a field.\n\n        Args:\n            field: the field(s) to get distinct values for\n            criteria: PyMongo filter for documents to search in\n        \"\"\"\n        distincts = []\n        for store in self.stores:\n            distincts.extend(store.distinct(field=field, criteria=criteria))\n\n        return list(set(distincts))\n\n    def ensure_index(self, key: str, unique: bool = False) -&gt; bool:\n        \"\"\"\n        Ensure an index is properly set. Returns whether all stores support this index or not.\n\n        Args:\n            key: single key to index\n            unique: Whether or not this index contains only unique keys\n\n        Returns:\n            bool indicating if the index exists/was created on all stores\n        \"\"\"\n        return all(store.ensure_index(key, unique) for store in self.stores)\n\n    def count(self, criteria: Optional[dict] = None) -&gt; int:\n        \"\"\"\n        Counts the number of documents matching the query criteria.\n\n        Args:\n            criteria: PyMongo filter for documents to count in\n        \"\"\"\n        counts = [store.count(criteria) for store in self.stores]\n\n        return sum(counts)\n\n    def query(\n        self,\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[dict]:\n        \"\"\"\n        Queries across all Store for a set of documents.\n\n        Args:\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n        \"\"\"\n        # TODO: skip, sort and limit are broken. implement properly\n        for store in self.stores:\n            yield from store.query(criteria=criteria, properties=properties)\n\n    def groupby(\n        self,\n        keys: Union[list[str], str],\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[tuple[dict, list[dict]]]:\n        \"\"\"\n        Simple grouping function that will group documents\n        by keys.\n\n        Args:\n            keys: fields to group documents\n            criteria: PyMongo filter for documents to search in\n            properties: properties to return in grouped documents\n            sort: Dictionary of sort order for fields. Keys are field names and\n                values are 1 for ascending or -1 for descending.\n            skip: number documents to skip\n            limit: limit on total number of documents returned\n\n        Returns:\n            generator returning tuples of (dict, list of docs)\n        \"\"\"\n        if isinstance(keys, str):\n            keys = [keys]\n\n        docs = []\n        for store in self.stores:\n            temp_docs = list(\n                store.groupby(\n                    keys=keys,\n                    criteria=criteria,\n                    properties=properties,\n                    sort=sort,\n                    skip=skip,\n                    limit=limit,\n                )\n            )\n            for _key, group in temp_docs:\n                docs.extend(group)\n\n        def key_set(d: dict) -&gt; tuple:\n            \"\"\"Index function based on passed in keys.\"\"\"\n            return tuple(d.get(k) for k in keys)\n\n        sorted_docs = sorted(docs, key=key_set)\n        for vals, group_iter in groupby(sorted_docs, key=key_set):\n            id_dict = dict(zip(keys, vals))\n            yield id_dict, list(group_iter)\n\n    def remove_docs(self, criteria: dict):\n        \"\"\"\n        Remove docs matching the query dictionary.\n\n        Args:\n            criteria: query dictionary to match\n        \"\"\"\n        raise NotImplementedError(\"No remove_docs method for JointStore\")\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"\n        Check equality for ConcatStore.\n\n        Args:\n            other: other JointStore to compare with\n        \"\"\"\n        if not isinstance(other, ConcatStore):\n            return False\n\n        fields = [\"stores\"]\n        return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.last_updated","title":"<code>last_updated</code>  <code>property</code>","text":"<p>Finds the most recent last_updated across all the stores. This might not be the most useful way to do this for this type of Store since it could very easily over-estimate the last_updated based on what stores are used.</p>"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.name","title":"<code>name</code>  <code>property</code>","text":"<p>A string representing this data source.</p>"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality for ConcatStore.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>object</code> <p>other JointStore to compare with</p> required Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"\n    Check equality for ConcatStore.\n\n    Args:\n        other: other JointStore to compare with\n    \"\"\"\n    if not isinstance(other, ConcatStore):\n        return False\n\n    fields = [\"stores\"]\n    return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.__init__","title":"<code>__init__(stores, **kwargs)</code>","text":"<p>Initialize a ConcatStore that concatenates multiple stores together to appear as one store.</p> <p>Parameters:</p> Name Type Description Default <code>stores</code> <code>list[Store]</code> <p>list of stores to concatenate together</p> required Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def __init__(self, stores: list[Store], **kwargs):\n    \"\"\"\n    Initialize a ConcatStore that concatenates multiple stores together\n    to appear as one store.\n\n    Args:\n        stores: list of stores to concatenate together\n    \"\"\"\n    self.stores = stores\n    self.kwargs = kwargs\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.close","title":"<code>close()</code>","text":"<p>Close all connections in this ConcatStore.</p> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def close(self):\n    \"\"\"\n    Close all connections in this ConcatStore.\n    \"\"\"\n    for store in self.stores:\n        store.close()\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.connect","title":"<code>connect(force_reset=False)</code>","text":"<p>Connect all stores in this ConcatStore.</p> <p>Parameters:</p> Name Type Description Default <code>force_reset</code> <code>bool</code> <p>Whether to forcibly reset the connection for all stores</p> <code>False</code> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def connect(self, force_reset: bool = False):\n    \"\"\"\n    Connect all stores in this ConcatStore.\n\n    Args:\n        force_reset: Whether to forcibly reset the connection for all stores\n    \"\"\"\n    for store in self.stores:\n        store.connect(force_reset)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.count","title":"<code>count(criteria=None)</code>","text":"<p>Counts the number of documents matching the query criteria.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to count in</p> <code>None</code> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def count(self, criteria: Optional[dict] = None) -&gt; int:\n    \"\"\"\n    Counts the number of documents matching the query criteria.\n\n    Args:\n        criteria: PyMongo filter for documents to count in\n    \"\"\"\n    counts = [store.count(criteria) for store in self.stores]\n\n    return sum(counts)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.distinct","title":"<code>distinct(field, criteria=None, all_exist=False)</code>","text":"<p>Get all distinct values for a field.</p> <p>Parameters:</p> Name Type Description Default <code>field</code> <code>str</code> <p>the field(s) to get distinct values for</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def distinct(self, field: str, criteria: Optional[dict] = None, all_exist: bool = False) -&gt; list:\n    \"\"\"\n    Get all distinct values for a field.\n\n    Args:\n        field: the field(s) to get distinct values for\n        criteria: PyMongo filter for documents to search in\n    \"\"\"\n    distincts = []\n    for store in self.stores:\n        distincts.extend(store.distinct(field=field, criteria=criteria))\n\n    return list(set(distincts))\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.ensure_index","title":"<code>ensure_index(key, unique=False)</code>","text":"<p>Ensure an index is properly set. Returns whether all stores support this index or not.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>single key to index</p> required <code>unique</code> <code>bool</code> <p>Whether or not this index contains only unique keys</p> <code>False</code> <p>Returns:</p> Type Description <code>bool</code> <p>bool indicating if the index exists/was created on all stores</p> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def ensure_index(self, key: str, unique: bool = False) -&gt; bool:\n    \"\"\"\n    Ensure an index is properly set. Returns whether all stores support this index or not.\n\n    Args:\n        key: single key to index\n        unique: Whether or not this index contains only unique keys\n\n    Returns:\n        bool indicating if the index exists/was created on all stores\n    \"\"\"\n    return all(store.ensure_index(key, unique) for store in self.stores)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.groupby","title":"<code>groupby(keys, criteria=None, properties=None, sort=None, skip=0, limit=0)</code>","text":"<p>Simple grouping function that will group documents by keys.</p> <p>Parameters:</p> Name Type Description Default <code>keys</code> <code>Union[list[str], str]</code> <p>fields to group documents</p> required <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> <p>Returns:</p> Type Description <code>Iterator[tuple[dict, list[dict]]]</code> <p>generator returning tuples of (dict, list of docs)</p> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def groupby(\n    self,\n    keys: Union[list[str], str],\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[tuple[dict, list[dict]]]:\n    \"\"\"\n    Simple grouping function that will group documents\n    by keys.\n\n    Args:\n        keys: fields to group documents\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n\n    Returns:\n        generator returning tuples of (dict, list of docs)\n    \"\"\"\n    if isinstance(keys, str):\n        keys = [keys]\n\n    docs = []\n    for store in self.stores:\n        temp_docs = list(\n            store.groupby(\n                keys=keys,\n                criteria=criteria,\n                properties=properties,\n                sort=sort,\n                skip=skip,\n                limit=limit,\n            )\n        )\n        for _key, group in temp_docs:\n            docs.extend(group)\n\n    def key_set(d: dict) -&gt; tuple:\n        \"\"\"Index function based on passed in keys.\"\"\"\n        return tuple(d.get(k) for k in keys)\n\n    sorted_docs = sorted(docs, key=key_set)\n    for vals, group_iter in groupby(sorted_docs, key=key_set):\n        id_dict = dict(zip(keys, vals))\n        yield id_dict, list(group_iter)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.query","title":"<code>query(criteria=None, properties=None, sort=None, skip=0, limit=0)</code>","text":"<p>Queries across all Store for a set of documents.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to search in</p> <code>None</code> <code>properties</code> <code>Union[dict, list, None]</code> <p>properties to return in grouped documents</p> <code>None</code> <code>sort</code> <code>Optional[dict[str, Union[Sort, int]]]</code> <p>Dictionary of sort order for fields. Keys are field names and values are 1 for ascending or -1 for descending.</p> <code>None</code> <code>skip</code> <code>int</code> <p>number documents to skip</p> <code>0</code> <code>limit</code> <code>int</code> <p>limit on total number of documents returned</p> <code>0</code> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def query(\n    self,\n    criteria: Optional[dict] = None,\n    properties: Union[dict, list, None] = None,\n    sort: Optional[dict[str, Union[Sort, int]]] = None,\n    skip: int = 0,\n    limit: int = 0,\n) -&gt; Iterator[dict]:\n    \"\"\"\n    Queries across all Store for a set of documents.\n\n    Args:\n        criteria: PyMongo filter for documents to search in\n        properties: properties to return in grouped documents\n        sort: Dictionary of sort order for fields. Keys are field names and\n            values are 1 for ascending or -1 for descending.\n        skip: number documents to skip\n        limit: limit on total number of documents returned\n    \"\"\"\n    # TODO: skip, sort and limit are broken. implement properly\n    for store in self.stores:\n        yield from store.query(criteria=criteria, properties=properties)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.remove_docs","title":"<code>remove_docs(criteria)</code>","text":"<p>Remove docs matching the query dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>dict</code> <p>query dictionary to match</p> required Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def remove_docs(self, criteria: dict):\n    \"\"\"\n    Remove docs matching the query dictionary.\n\n    Args:\n        criteria: query dictionary to match\n    \"\"\"\n    raise NotImplementedError(\"No remove_docs method for JointStore\")\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.ConcatStore.update","title":"<code>update(docs, key=None)</code>","text":"<p>Update documents into the Store Not implemented in ConcatStore.</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>Union[list[dict], dict]</code> <p>the document or list of documents to update</p> required <code>key</code> <code>Union[list, str, None]</code> <p>field name(s) to determine uniqueness for a  document, can be a list of multiple fields,  a single field, or None if the Store's key  field is to be used</p> <code>None</code> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def update(self, docs: Union[list[dict], dict], key: Union[list, str, None] = None):\n    \"\"\"\n    Update documents into the Store\n    Not implemented in ConcatStore.\n\n    Args:\n        docs: the document or list of documents to update\n        key: field name(s) to determine uniqueness for a\n             document, can be a list of multiple fields,\n             a single field, or None if the Store's key\n             field is to be used\n    \"\"\"\n    raise NotImplementedError(\"No update method for ConcatStore\")\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore","title":"<code>JointStore</code>","text":"<p>               Bases: <code>Store</code></p> <p>Store that implements a on-the-fly join across multiple collections all in the same MongoDB database. This is a Read-Only Store designed to combine data from multiple collections.</p> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>class JointStore(Store):\n    \"\"\"\n    Store that implements a on-the-fly join across multiple collections all in the same MongoDB database.\n    This is a Read-Only Store designed to combine data from multiple collections.\n    \"\"\"\n\n    def __init__(\n        self,\n        database: str,\n        collection_names: list[str],\n        host: str = \"localhost\",\n        port: int = 27017,\n        username: str = \"\",\n        password: str = \"\",\n        main: Optional[str] = None,\n        merge_at_root: bool = False,\n        mongoclient_kwargs: Optional[dict] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            database: The database name\n            collection_names: list of all collections\n                to join\n            host: Hostname for the database\n            port: TCP port to connect to\n            username: Username for the collection\n            password: Password to connect with\n            main: name for the main collection\n                if not specified this defaults to the first\n                in collection_names list.\n        \"\"\"\n        self.database = database\n        self.collection_names = collection_names\n        self.host = host\n        self.port = port\n        self.username = username\n        self.password = password\n        self._coll = None  # type: Any\n        self.main = main or collection_names[0]\n        self.merge_at_root = merge_at_root\n        self.mongoclient_kwargs = mongoclient_kwargs or {}\n        self.kwargs = kwargs\n\n        super().__init__(**kwargs)\n\n    @property\n    def name(self) -&gt; str:\n        \"\"\"\n        Return a string representing this data source.\n        \"\"\"\n        compound_name = \",\".join(self.collection_names)\n        return f\"Compound[{self.host}/{self.database}][{compound_name}]\"\n\n    def connect(self, force_reset: bool = False):\n        \"\"\"\n        Connects the underlying Mongo database and all collection connections.\n\n        Args:\n            force_reset: whether to reset the connection or not when the Store is\n                already connected.\n        \"\"\"\n        if not self._coll or force_reset:\n            conn: MongoClient = (\n                MongoClient(\n                    host=self.host,\n                    port=self.port,\n                    username=self.username,\n                    password=self.password,\n                    **self.mongoclient_kwargs,\n                )\n                if self.username != \"\"\n                else MongoClient(self.host, self.port, **self.mongoclient_kwargs)\n            )\n            db = conn[self.database]\n            self._coll = db[self.main]\n            self._has_merge_objects = self._collection.database.client.server_info()[\"version\"] &gt; \"3.6\"\n\n    def close(self):\n        \"\"\"\n        Closes underlying database connections.\n        \"\"\"\n        self._collection.database.client.close()\n\n    @property\n    def _collection(self):\n        \"\"\"Property referring to the root pymongo collection.\"\"\"\n        if self._coll is None:\n            raise StoreError(\"Must connect Mongo-like store before attempting to use it\")\n        return self._coll\n\n    @property\n    def nonmain_names(self) -&gt; list:\n        \"\"\"\n        all non-main collection names.\n        \"\"\"\n        return list(set(self.collection_names) - {self.main})\n\n    @property\n    def last_updated(self) -&gt; datetime:\n        \"\"\"\n        Special last_updated for this JointStore\n        that checks all underlying collections.\n        \"\"\"\n        lus = []\n        for cname in self.collection_names:\n            store = MongoStore.from_collection(self._collection.database[cname])\n            store.last_updated_field = self.last_updated_field\n            lu = store.last_updated\n            lus.append(lu)\n        return max(lus)\n\n    # TODO: implement update?\n    def update(self, docs, update_lu=True, key=None, **kwargs):\n        \"\"\"\n        Update documents into the underlying collections\n        Not Implemented for JointStore.\n        \"\"\"\n        raise NotImplementedError(\"JointStore is a read-only store\")\n\n    def _get_store_by_name(self, name) -&gt; MongoStore:\n        \"\"\"\n        Gets an underlying collection as a mongoStore.\n        \"\"\"\n        if name not in self.collection_names:\n            raise ValueError(\"Asking for collection not referenced in this Store\")\n        return MongoStore.from_collection(self._collection.database[name])\n\n    def ensure_index(self, key, unique=False, **kwargs):\n        \"\"\"\n        Can't ensure index for JointStore.\n        \"\"\"\n        raise NotImplementedError(\"No ensure_index method for JointStore\")\n\n    def _get_pipeline(self, criteria=None, properties=None, skip=0, limit=0):\n        \"\"\"\n        Gets the aggregation pipeline for query and query_one.\n\n        Args:\n            properties: properties to be returned\n            criteria: criteria to filter by\n            skip: docs to skip\n            limit: limit results to N docs\n        Returns:\n            list of aggregation operators\n        \"\"\"\n        pipeline = []\n        collection_names = list(set(self.collection_names) - set(self.main))\n        for cname in collection_names:\n            pipeline.append(\n                {\n                    \"$lookup\": {\n                        \"from\": cname,\n                        \"localField\": self.key,\n                        \"foreignField\": self.key,\n                        \"as\": cname,\n                    }\n                }\n            )\n\n            if self.merge_at_root:\n                if not self._has_merge_objects:\n                    raise Exception(\"MongoDB server version too low to use $mergeObjects.\")\n\n                pipeline.append(\n                    {\n                        \"$replaceRoot\": {\n                            \"newRoot\": {\n                                \"$mergeObjects\": [\n                                    {\"$arrayElemAt\": [f\"${cname}\", 0]},\n                                    \"$$ROOT\",\n                                ]\n                            }\n                        }\n                    }\n                )\n            else:\n                pipeline.append(\n                    {\n                        \"$unwind\": {\n                            \"path\": f\"${cname}\",\n                            \"preserveNullAndEmptyArrays\": True,\n                        }\n                    }\n                )\n\n        # Do projection for max last_updated\n        lu_max_fields = [f\"${self.last_updated_field}\"]\n        lu_max_fields.extend([f\"${cname}.{self.last_updated_field}\" for cname in self.collection_names])\n        lu_proj = {self.last_updated_field: {\"$max\": lu_max_fields}}\n        pipeline.append({\"$addFields\": lu_proj})\n\n        if criteria:\n            pipeline.append({\"$match\": criteria})\n        if isinstance(properties, list):\n            properties = {k: 1 for k in properties}\n        if properties:\n            pipeline.append({\"$project\": properties})\n\n        if skip &gt; 0:\n            pipeline.append({\"$skip\": skip})\n\n        if limit &gt; 0:\n            pipeline.append({\"$limit\": limit})\n        return pipeline\n\n    def count(self, criteria: Optional[dict] = None) -&gt; int:\n        \"\"\"\n        Counts the number of documents matching the query criteria.\n\n        Args:\n            criteria: PyMongo filter for documents to count in\n        \"\"\"\n        pipeline = self._get_pipeline(criteria=criteria)\n        pipeline.append({\"$count\": \"count\"})\n        agg = list(self._collection.aggregate(pipeline))\n        return agg[0].get(\"count\", 0) if len(agg) &gt; 0 else 0\n\n    def query(\n        self,\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[dict]:\n        pipeline = self._get_pipeline(criteria=criteria, properties=properties, skip=skip, limit=limit)\n        agg = self._collection.aggregate(pipeline)\n        yield from agg\n\n    def groupby(\n        self,\n        keys: Union[list[str], str],\n        criteria: Optional[dict] = None,\n        properties: Union[dict, list, None] = None,\n        sort: Optional[dict[str, Union[Sort, int]]] = None,\n        skip: int = 0,\n        limit: int = 0,\n    ) -&gt; Iterator[tuple[dict, list[dict]]]:\n        pipeline = self._get_pipeline(criteria=criteria, properties=properties, skip=skip, limit=limit)\n        if not isinstance(keys, list):\n            keys = [keys]\n        group_id = {}  # type: Dict[str,Any]\n        for key in keys:\n            set_(group_id, key, f\"${key}\")\n        pipeline.append({\"$group\": {\"_id\": group_id, \"docs\": {\"$push\": \"$$ROOT\"}}})\n\n        agg = self._collection.aggregate(pipeline)\n\n        for d in agg:\n            yield d[\"_id\"], d[\"docs\"]\n\n    def query_one(self, criteria=None, properties=None, **kwargs):\n        \"\"\"\n        Get one document.\n\n        Args:\n            properties: properties to return in query\n            criteria: filter for matching\n            kwargs: kwargs for collection.aggregate\n\n        Returns:\n            single document\n        \"\"\"\n        # TODO: maybe adding explicit limit in agg pipeline is better as below?\n        # pipeline = self._get_pipeline(properties, criteria)\n        # pipeline.append({\"$limit\": 1})\n        query = self.query(criteria=criteria, properties=properties, **kwargs)\n        try:\n            return next(query)\n        except StopIteration:\n            return None\n\n    def remove_docs(self, criteria: dict):\n        \"\"\"\n        Remove docs matching the query dictionary.\n\n        Args:\n            criteria: query dictionary to match\n        \"\"\"\n        raise NotImplementedError(\"No remove_docs method for JointStore\")\n\n    def __eq__(self, other: object) -&gt; bool:\n        \"\"\"\n        Check equality for JointStore\n\n        Args:\n            other: other JointStore to compare with.\n        \"\"\"\n        if not isinstance(other, JointStore):\n            return False\n\n        fields = [\n            \"database\",\n            \"collection_names\",\n            \"host\",\n            \"port\",\n            \"main\",\n            \"merge_at_root\",\n        ]\n        return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.last_updated","title":"<code>last_updated</code>  <code>property</code>","text":"<p>Special last_updated for this JointStore that checks all underlying collections.</p>"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.name","title":"<code>name</code>  <code>property</code>","text":"<p>Return a string representing this data source.</p>"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.nonmain_names","title":"<code>nonmain_names</code>  <code>property</code>","text":"<p>all non-main collection names.</p>"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.__eq__","title":"<code>__eq__(other)</code>","text":"<p>Check equality for JointStore</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>object</code> <p>other JointStore to compare with.</p> required Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def __eq__(self, other: object) -&gt; bool:\n    \"\"\"\n    Check equality for JointStore\n\n    Args:\n        other: other JointStore to compare with.\n    \"\"\"\n    if not isinstance(other, JointStore):\n        return False\n\n    fields = [\n        \"database\",\n        \"collection_names\",\n        \"host\",\n        \"port\",\n        \"main\",\n        \"merge_at_root\",\n    ]\n    return all(getattr(self, f) == getattr(other, f) for f in fields)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.__init__","title":"<code>__init__(database, collection_names, host='localhost', port=27017, username='', password='', main=None, merge_at_root=False, mongoclient_kwargs=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>database</code> <code>str</code> <p>The database name</p> required <code>collection_names</code> <code>list[str]</code> <p>list of all collections to join</p> required <code>host</code> <code>str</code> <p>Hostname for the database</p> <code>'localhost'</code> <code>port</code> <code>int</code> <p>TCP port to connect to</p> <code>27017</code> <code>username</code> <code>str</code> <p>Username for the collection</p> <code>''</code> <code>password</code> <code>str</code> <p>Password to connect with</p> <code>''</code> <code>main</code> <code>Optional[str]</code> <p>name for the main collection if not specified this defaults to the first in collection_names list.</p> <code>None</code> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def __init__(\n    self,\n    database: str,\n    collection_names: list[str],\n    host: str = \"localhost\",\n    port: int = 27017,\n    username: str = \"\",\n    password: str = \"\",\n    main: Optional[str] = None,\n    merge_at_root: bool = False,\n    mongoclient_kwargs: Optional[dict] = None,\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        database: The database name\n        collection_names: list of all collections\n            to join\n        host: Hostname for the database\n        port: TCP port to connect to\n        username: Username for the collection\n        password: Password to connect with\n        main: name for the main collection\n            if not specified this defaults to the first\n            in collection_names list.\n    \"\"\"\n    self.database = database\n    self.collection_names = collection_names\n    self.host = host\n    self.port = port\n    self.username = username\n    self.password = password\n    self._coll = None  # type: Any\n    self.main = main or collection_names[0]\n    self.merge_at_root = merge_at_root\n    self.mongoclient_kwargs = mongoclient_kwargs or {}\n    self.kwargs = kwargs\n\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.close","title":"<code>close()</code>","text":"<p>Closes underlying database connections.</p> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def close(self):\n    \"\"\"\n    Closes underlying database connections.\n    \"\"\"\n    self._collection.database.client.close()\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.connect","title":"<code>connect(force_reset=False)</code>","text":"<p>Connects the underlying Mongo database and all collection connections.</p> <p>Parameters:</p> Name Type Description Default <code>force_reset</code> <code>bool</code> <p>whether to reset the connection or not when the Store is already connected.</p> <code>False</code> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def connect(self, force_reset: bool = False):\n    \"\"\"\n    Connects the underlying Mongo database and all collection connections.\n\n    Args:\n        force_reset: whether to reset the connection or not when the Store is\n            already connected.\n    \"\"\"\n    if not self._coll or force_reset:\n        conn: MongoClient = (\n            MongoClient(\n                host=self.host,\n                port=self.port,\n                username=self.username,\n                password=self.password,\n                **self.mongoclient_kwargs,\n            )\n            if self.username != \"\"\n            else MongoClient(self.host, self.port, **self.mongoclient_kwargs)\n        )\n        db = conn[self.database]\n        self._coll = db[self.main]\n        self._has_merge_objects = self._collection.database.client.server_info()[\"version\"] &gt; \"3.6\"\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.count","title":"<code>count(criteria=None)</code>","text":"<p>Counts the number of documents matching the query criteria.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>Optional[dict]</code> <p>PyMongo filter for documents to count in</p> <code>None</code> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def count(self, criteria: Optional[dict] = None) -&gt; int:\n    \"\"\"\n    Counts the number of documents matching the query criteria.\n\n    Args:\n        criteria: PyMongo filter for documents to count in\n    \"\"\"\n    pipeline = self._get_pipeline(criteria=criteria)\n    pipeline.append({\"$count\": \"count\"})\n    agg = list(self._collection.aggregate(pipeline))\n    return agg[0].get(\"count\", 0) if len(agg) &gt; 0 else 0\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.ensure_index","title":"<code>ensure_index(key, unique=False, **kwargs)</code>","text":"<p>Can't ensure index for JointStore.</p> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def ensure_index(self, key, unique=False, **kwargs):\n    \"\"\"\n    Can't ensure index for JointStore.\n    \"\"\"\n    raise NotImplementedError(\"No ensure_index method for JointStore\")\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.query_one","title":"<code>query_one(criteria=None, properties=None, **kwargs)</code>","text":"<p>Get one document.</p> <p>Parameters:</p> Name Type Description Default <code>properties</code> <p>properties to return in query</p> <code>None</code> <code>criteria</code> <p>filter for matching</p> <code>None</code> <code>kwargs</code> <p>kwargs for collection.aggregate</p> <code>{}</code> <p>Returns:</p> Type Description <p>single document</p> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def query_one(self, criteria=None, properties=None, **kwargs):\n    \"\"\"\n    Get one document.\n\n    Args:\n        properties: properties to return in query\n        criteria: filter for matching\n        kwargs: kwargs for collection.aggregate\n\n    Returns:\n        single document\n    \"\"\"\n    # TODO: maybe adding explicit limit in agg pipeline is better as below?\n    # pipeline = self._get_pipeline(properties, criteria)\n    # pipeline.append({\"$limit\": 1})\n    query = self.query(criteria=criteria, properties=properties, **kwargs)\n    try:\n        return next(query)\n    except StopIteration:\n        return None\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.remove_docs","title":"<code>remove_docs(criteria)</code>","text":"<p>Remove docs matching the query dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>criteria</code> <code>dict</code> <p>query dictionary to match</p> required Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def remove_docs(self, criteria: dict):\n    \"\"\"\n    Remove docs matching the query dictionary.\n\n    Args:\n        criteria: query dictionary to match\n    \"\"\"\n    raise NotImplementedError(\"No remove_docs method for JointStore\")\n</code></pre>"},{"location":"reference/stores/#maggma.stores.compound_stores.JointStore.update","title":"<code>update(docs, update_lu=True, key=None, **kwargs)</code>","text":"<p>Update documents into the underlying collections Not Implemented for JointStore.</p> Source code in <code>src/maggma/stores/compound_stores.py</code> <pre><code>def update(self, docs, update_lu=True, key=None, **kwargs):\n    \"\"\"\n    Update documents into the underlying collections\n    Not Implemented for JointStore.\n    \"\"\"\n    raise NotImplementedError(\"JointStore is a read-only store\")\n</code></pre>"},{"location":"reference/stores/#maggma.stores.ssh_tunnel.SSHTunnel","title":"<code>SSHTunnel</code>","text":"<p>               Bases: <code>MSONable</code></p> <p>SSH tunnel to remote server.</p> Source code in <code>src/maggma/stores/ssh_tunnel.py</code> <pre><code>class SSHTunnel(MSONable):\n    \"\"\"SSH tunnel to remote server.\"\"\"\n\n    __TUNNELS: dict[str, SSHTunnelForwarder] = {}\n\n    def __init__(\n        self,\n        tunnel_server_address: str,\n        remote_server_address: str,\n        local_port: Optional[int] = None,\n        username: Optional[str] = None,\n        password: Optional[str] = None,\n        private_key: Optional[str] = None,\n        **kwargs,\n    ):\n        \"\"\"\n        Args:\n            tunnel_server_address: string address with port for the SSH tunnel server\n            remote_server_address: string address with port for the server to connect to\n            local_port: optional port to use for the local address (127.0.0.1);\n                if `None`, a random open port will be automatically selected\n            username: optional username for the ssh tunnel server\n            password: optional password for the ssh tunnel server; If a private_key is\n                supplied this password is assumed to be the private key password\n            private_key: ssh private key to authenticate to the tunnel server\n            kwargs: any extra args passed to the SSHTunnelForwarder.\n        \"\"\"\n        self.tunnel_server_address = tunnel_server_address\n        self.remote_server_address = remote_server_address\n        self.local_port = local_port\n        self.username = username\n        self.password = password\n        self.private_key = private_key\n        self.kwargs = kwargs\n\n        if remote_server_address in SSHTunnel.__TUNNELS:\n            self.tunnel = SSHTunnel.__TUNNELS[remote_server_address]\n        else:\n            if local_port is None:\n                local_port = _find_free_port(\"127.0.0.1\")\n            local_bind_address = (\"127.0.0.1\", local_port)\n\n            ssh_address, ssh_port = tunnel_server_address.split(\":\")\n            ssh_port = int(ssh_port)  # type: ignore\n\n            remote_bind_address, remote_bind_port = remote_server_address.split(\":\")\n            remote_bind_port = int(remote_bind_port)  # type: ignore\n\n            if private_key is not None:\n                ssh_password = None\n                ssh_private_key_password = password\n            else:\n                ssh_password = password\n                ssh_private_key_password = None\n\n            self.tunnel = SSHTunnelForwarder(\n                ssh_address_or_host=(ssh_address, ssh_port),\n                local_bind_address=local_bind_address,\n                remote_bind_address=(remote_bind_address, remote_bind_port),\n                ssh_username=username,\n                ssh_password=ssh_password,\n                ssh_private_key_password=ssh_private_key_password,\n                ssh_pkey=private_key,\n                **kwargs,\n            )\n\n    def start(self):\n        if not self.tunnel.is_active:\n            self.tunnel.start()\n\n    def stop(self):\n        if self.tunnel.tunnel_is_up:\n            self.tunnel.stop()\n\n    @property\n    def local_address(self) -&gt; tuple[str, int]:\n        return self.tunnel.local_bind_address\n</code></pre>"},{"location":"reference/stores/#maggma.stores.ssh_tunnel.SSHTunnel.__init__","title":"<code>__init__(tunnel_server_address, remote_server_address, local_port=None, username=None, password=None, private_key=None, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>tunnel_server_address</code> <code>str</code> <p>string address with port for the SSH tunnel server</p> required <code>remote_server_address</code> <code>str</code> <p>string address with port for the server to connect to</p> required <code>local_port</code> <code>Optional[int]</code> <p>optional port to use for the local address (127.0.0.1); if <code>None</code>, a random open port will be automatically selected</p> <code>None</code> <code>username</code> <code>Optional[str]</code> <p>optional username for the ssh tunnel server</p> <code>None</code> <code>password</code> <code>Optional[str]</code> <p>optional password for the ssh tunnel server; If a private_key is supplied this password is assumed to be the private key password</p> <code>None</code> <code>private_key</code> <code>Optional[str]</code> <p>ssh private key to authenticate to the tunnel server</p> <code>None</code> <code>kwargs</code> <p>any extra args passed to the SSHTunnelForwarder.</p> <code>{}</code> Source code in <code>src/maggma/stores/ssh_tunnel.py</code> <pre><code>def __init__(\n    self,\n    tunnel_server_address: str,\n    remote_server_address: str,\n    local_port: Optional[int] = None,\n    username: Optional[str] = None,\n    password: Optional[str] = None,\n    private_key: Optional[str] = None,\n    **kwargs,\n):\n    \"\"\"\n    Args:\n        tunnel_server_address: string address with port for the SSH tunnel server\n        remote_server_address: string address with port for the server to connect to\n        local_port: optional port to use for the local address (127.0.0.1);\n            if `None`, a random open port will be automatically selected\n        username: optional username for the ssh tunnel server\n        password: optional password for the ssh tunnel server; If a private_key is\n            supplied this password is assumed to be the private key password\n        private_key: ssh private key to authenticate to the tunnel server\n        kwargs: any extra args passed to the SSHTunnelForwarder.\n    \"\"\"\n    self.tunnel_server_address = tunnel_server_address\n    self.remote_server_address = remote_server_address\n    self.local_port = local_port\n    self.username = username\n    self.password = password\n    self.private_key = private_key\n    self.kwargs = kwargs\n\n    if remote_server_address in SSHTunnel.__TUNNELS:\n        self.tunnel = SSHTunnel.__TUNNELS[remote_server_address]\n    else:\n        if local_port is None:\n            local_port = _find_free_port(\"127.0.0.1\")\n        local_bind_address = (\"127.0.0.1\", local_port)\n\n        ssh_address, ssh_port = tunnel_server_address.split(\":\")\n        ssh_port = int(ssh_port)  # type: ignore\n\n        remote_bind_address, remote_bind_port = remote_server_address.split(\":\")\n        remote_bind_port = int(remote_bind_port)  # type: ignore\n\n        if private_key is not None:\n            ssh_password = None\n            ssh_private_key_password = password\n        else:\n            ssh_password = password\n            ssh_private_key_password = None\n\n        self.tunnel = SSHTunnelForwarder(\n            ssh_address_or_host=(ssh_address, ssh_port),\n            local_bind_address=local_bind_address,\n            remote_bind_address=(remote_bind_address, remote_bind_port),\n            ssh_username=username,\n            ssh_password=ssh_password,\n            ssh_private_key_password=ssh_private_key_password,\n            ssh_pkey=private_key,\n            **kwargs,\n        )\n</code></pre>"}]}